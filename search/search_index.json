{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TINYAUTON: Microcontroller-oriented Distributed Intelligence Enabling Framework","text":""},{"location":"#about-this-project","title":"ABOUT THIS PROJECT","text":"<p>This project dedicates to the development of a library for tiny agent related computing running on MCU devices to serve the multi-agent system\uff0ccovering mathematical operations, digital signal processing, and TinyML. </p> <p>About the Name</p> <p>The name \"TinyAuton\" is a combination of \"Tiny\" and \"Auton\". \"Tiny\" means the agent is designed to run on MCU devices, and \"Auton\" is short for \"Autonomous Agent\".</p>"},{"location":"#target-hardware","title":"TARGET HARDWARE","text":"<ul> <li>MCU devices (currently targeting ESP32 as the main platform)</li> </ul>"},{"location":"#scope","title":"SCOPE","text":"<ul> <li>Platform adaptation and various tools (time, communication, etc.)</li> <li>Basic Math Operations</li> <li>Digital Signal Processing</li> <li>TinyML / Edge AI</li> </ul>"},{"location":"#host-devkits","title":"HOST DEVKITS","text":"<p>Tip</p> <p>The following hardwares are for demonstration purposes only. This project is not limited to these and can be ported to other types of hardwares.</p> <ul> <li>DNESP32S3M from Alientek (ESP32-S3)</li> </ul> <p></p> <p></p> <ul> <li> <p> NexNode</p> <p>  Repo </p> <p>  Online Doc </p> </li> </ul>"},{"location":"#project-architecture","title":"PROJECT ARCHITECTURE","text":"<pre><code>+------------------------------+\n| APPLICATION                  |\n+------------------------------+\n|   - TinyAI                   | &lt;-- AI Functions\n|   - TinyDSP                  | &lt;-- DSP Functions\n|   - TinyMath                 | &lt;-- Common Math Functions\n|   - TinyToolbox              | &lt;-- Platform-specific Low-level Optimization + Various Utilities\n| MIDDLEWARE                   |\n+------------------------------+\n| DRIVERS                      |\n+------------------------------+\n| HARDWARE                     |\n+------------------------------+\n</code></pre>"},{"location":"AI/ai/","title":"ARTIFICIAL INTELLIGENCE","text":""},{"location":"ARCHITECTURE/architecture/","title":"ARCHITECTURE","text":""},{"location":"ARCHITECTURE/architecture/#layered-architecture","title":"LAYERED ARCHITECTURE","text":"<pre><code>+------------------------------+\n| AI                           | &lt;-- AI/ML Functions for Edge Devices based on Low Level Functions\n+------------------------------+\n| DSP                          | &lt;-- Digital Signal Processing Functions\n+------------------------------+\n| Math Operations              | &lt;-- Commonly Used Math Functions for Various Applications\n+------------------------------+\n| Adaptation/Toolbox Layer     | &lt;-- To Replace Functions in Standard C with Platform Optimized/Specific Functions\n+------------------------------+\n</code></pre>"},{"location":"DSP/dsp/","title":"DIGITAL SIGNAL PROCESSING","text":"<p>Note</p> <p>This component provides a set of functions designed for signal processing on edge devices, with a focus on lightweight and efficient implementations of commonly used signal processing algorithms.</p> <p>Note</p> <p>This component is a wrapper and extension of the official ESP32 digital signal processing library ESP-DSP, providing higher-level API interfaces. In simple terms, the TinyMath library corresponds to the Math, Matrix, and DotProduct modules in ESP-DSP, while the other modules in ESP-DSP correspond to the TinyDSP library. Additionally, TinyDSP provides some functionalities not available in ESP-DSP, focusing on scenarios such as structural health monitoring.</p>"},{"location":"DSP/dsp/#component-dependencies","title":"COMPONENT DEPENDENCIES","text":"<pre><code>set(src_dirs\n    .\n    signal\n    filter\n    transform\n    support\n)\n\nset(include_dirs\n    .\n    include\n    signal\n    filter\n    transform\n    support\n)\n\nset(requires\n    tiny_math\n)\n\nidf_component_register(SRC_DIRS ${src_dirs} INCLUDE_DIRS ${include_dirs} REQUIRES ${requires})\n</code></pre>"},{"location":"DSP/dsp/#architecture-and-directory","title":"ARCHITECTURE AND DIRECTORY","text":""},{"location":"DSP/dsp/#dependency-diagram","title":"Dependency Diagram","text":""},{"location":"DSP/dsp/#code-tree","title":"Code Tree","text":"<pre><code>tiny_dsp/\n\u251c\u2500\u2500 include/                     \n\u2502   \u251c\u2500\u2500 tiny_dsp.h               # entrance header file\n\u2502   \u2514\u2500\u2500 tiny_dsp_config.h        # dsp module configuration file\n\u2502\n\u251c\u2500\u2500 signal/\n\u2502   \u251c\u2500\u2500 tiny_conv.h              # convolution - header file\n\u2502   \u251c\u2500\u2500 tiny_conv.c              # convolution - source file\n|   \u251c\u2500\u2500 tiny_conv_test.h         # convolution - test header file\n|   \u251c\u2500\u2500 tiny_conv_test.c         # convolution - test source file\n\u2502   \u251c\u2500\u2500 tiny_corr.h              # correlation - header file\n\u2502   \u251c\u2500\u2500 tiny_corr.c              # correlation - source file\n|   \u251c\u2500\u2500 tiny_corr_test.h         # correlation - test header file\n|   \u251c\u2500\u2500 tiny_corr_test.c         # correlation - test source file\n|   \u251c\u2500\u2500 tiny_resample.h          # resampling - header file\n|   \u251c\u2500\u2500 tiny_resample.c          # resampling - source file\n|   \u251c\u2500\u2500 tiny_resample_test.h     # resampling - test header file\n|   \u2514\u2500\u2500 tiny_resample_test.c     # resampling - test source file\n\u2502\n\u251c\u2500\u2500 filter/\n\u2502\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 tiny_dwt.h               # discrete wavelet transform - header file\n\u2502   \u251c\u2500\u2500 tiny_dwt.c               # discrete wavelet transform - source file\n\u2502   \u251c\u2500\u2500 tiny_dwt_test.h          # discrete wavelet transform - test header file\n\u2502   \u2514\u2500\u2500 tiny_dwt_test.c          # discrete wavelet transform - test source\n\u2502\n\u2514\u2500\u2500 support/\n</code></pre>"},{"location":"DSP/HEADER-FILE/tiny_dsp/","title":"TinyDSP HEADER FILE","text":"<p>Info</p> <p>This is the main header file of the TinyDSP library. It includes all necessary header files and provides a unified interface to use the functions of the library. After completing the porting of this library in the project, you can insert this header file where you want to use the relevant functions to use all functions in the library. The documentation update speed is slow and may not be consistent with the actual code, please refer to the actual code.</p> <pre><code>/**\n * @file tiny_dsp.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_dsp | entrance file\n * @version 1.0\n * @date 2025-04-28\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n\n// tiny_dsp configuration file\n#include \"tiny_dsp_config.h\"\n\n// [signal]\n\n// signal - convolution\n#include \"tiny_conv.h\"\n#include \"tiny_conv_test.h\"\n\n// signal - correlation &amp; crosss correlation\n#include \"tiny_corr.h\"\n#include \"tiny_corr_test.h\"\n\n// signal - resample &amp; decimate\n#include \"tiny_resample.h\"\n#include \"tiny_resample_test.h\"\n\n// [filter]\n\n// [transform]\n\n// transform - discrete wavelet transform\n#include \"tiny_dwt.h\"\n#include \"tiny_dwt_test.h\"\n\n// [support]\n\n\n\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/HEADER-FILE/tiny_dsp_config/","title":"TinyDSP CONFIGURATION","text":"<p>Info</p> <p>This header file configures the entire TinyDSP module, and each submodule includes this header file. It defines the configuration options and macros for TinyDSP, allowing users to customize settings as needed. By modifying the configuration options in this header file, users can easily adjust the behavior and functionality of TinyDSP to meet specific requirements. The documentation update speed is slow and may not be consistent with the actual code, please refer to the actual code.</p> <p>Tip</p> <p>For platform acceleration options, please set them in the TinyMath configuration file.</p> <pre><code>/**\n * @file tiny_dsp_config.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_dsp middleware.\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* DEPENDENCIES */\n#include \"tiny_math.h\"\n\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/CONVOLUTION/code/","title":"CODE","text":""},{"location":"DSP/SIGNAL/CONVOLUTION/code/#tiny_convh","title":"tiny_conv.h","text":"<pre><code>/**\n * @file tiny_conv.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_conv | code | header \n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// tiny_dsp configuration file\n#include \"tiny_dsp_config.h\"\n\n// ESP32 DSP Library for Acceleration\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32 // ESP32 DSP library\n\n#include \"dsps_conv.h\"\n#include \"dspi_conv.h\"\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/**\n * @name: tiny_conv_f32\n * @brief Convolution function\n * \n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n * \n * @return tiny_error_t \n */\ntiny_error_t tiny_conv_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *convout);\n\ntypedef enum\n{\n    TINY_PADDING_ZERO = 0,      // Zero padding\n    TINY_PADDING_SYMMETRIC = 1, // Symmetric reflection\n    TINY_PADDING_PERIODIC = 2   // Periodic extension\n} tiny_padding_mode_t;\n\ntypedef enum\n{\n    TINY_CONV_FULL = 0,   // Full convolution (len = siglen + kernlen - 1)\n    TINY_CONV_HEAD = 1,   // Head mode (first lkern points)\n    TINY_CONV_CENTER = 2, // Centered mode (output siglen points)\n    TINY_CONV_TAIL = 3    // Tail mode (last lkern points)\n} tiny_conv_mode_t;\n\n/**\n * @name: tiny_conv_ex_f32\n * @brief Extended convolution function with padding and mode options\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n * @param padding_mode Padding mode (zero, symmetric, periodic)\n * @param conv_mode Convolution mode (full, head, center, tail)\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_ex_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *convout, tiny_padding_mode_t padding_mode, tiny_conv_mode_t conv_mode);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/CONVOLUTION/code/#tiny_convc","title":"tiny_conv.c","text":"<pre><code>/**\n * @file tiny_conv.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_conv | code | source\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_conv.h\"\n\n/**\n * @name: tiny_conv_f32\n * @brief Convolution function\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *convout)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // ESP32 DSP library\n    dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n#else\n    float *sig = (float *)Signal;\n    float *kern = (float *)Kernel;\n    int lsig = siglen;\n    int lkern = kernlen;\n\n    // stage I\n    for (int n = 0; n &lt; lkern; n++)\n    {\n        size_t k;\n\n        convout[n] = 0;\n\n        for (k = 0; k &lt;= n; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage II\n    for (int n = lkern; n &lt; lsig; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = n;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage III\n    for (int n = lsig; n &lt; lsig + lkern - 1; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = lsig - 1;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n#endif\n\n    return TINY_OK;\n}\n\n/**\n * @name: tiny_conv_ex_f32\n * @brief Extended convolution function with padding and mode options\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n * @param padding_mode Padding mode (zero, symmetric, periodic)\n * @param conv_mode Convolution mode (full, head, center, tail)\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_ex_f32(const float *Signal, const int siglen,\n                              const float *Kernel, const int kernlen,\n                              float *convout,\n                              tiny_padding_mode_t padding_mode,\n                              tiny_conv_mode_t conv_mode)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padding_mode == TINY_PADDING_ZERO &amp;&amp; conv_mode == TINY_CONV_FULL)\n    {\n        dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n        return TINY_OK;\n    }\n#endif\n\n    int pad_len = kernlen - 1;\n    int padded_len = siglen + 2 * pad_len;\n    float *padded_signal = (float *)calloc(padded_len, sizeof(float));\n    if (padded_signal == NULL)\n    {\n        return TINY_ERR_DSP_MEMORY_ALLOC;\n    }\n\n    // Fill padded signal\n    switch (padding_mode)\n    {\n    case TINY_PADDING_ZERO:\n        // Middle copy only, left and right are zeros (calloc already zeroed)\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen);\n        break;\n\n    case TINY_PADDING_SYMMETRIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[i];                   // Mirror left\n            padded_signal[pad_len + siglen + i] = Signal[siglen - 1 - i]; // Mirror right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    case TINY_PADDING_PERIODIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[(siglen - pad_len + i) % siglen]; // Wrap left\n            padded_signal[pad_len + siglen + i] = Signal[i % siglen];                 // Wrap right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    default:\n        free(padded_signal);\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n    // Full convolution\n    int convlen_full = siglen + kernlen - 1;\n    for (int n = 0; n &lt; convlen_full; n++)\n    {\n        float sum = 0.0f;\n        for (int k = 0; k &lt; kernlen; k++)\n        {\n            sum += padded_signal[n + k] * Kernel[kernlen - 1 - k]; // Convolution is flip+slide\n        }\n        convout[n] = sum;\n    }\n\n    free(padded_signal);\n\n    // Handle output mode\n    if (conv_mode == TINY_CONV_FULL)\n    {\n        return TINY_OK;\n    }\n    else\n    {\n        int start_idx = 0;\n        int out_len = 0;\n\n        switch (conv_mode)\n        {\n        case TINY_CONV_HEAD:\n            start_idx = 0;\n            out_len = kernlen;\n            break;\n        case TINY_CONV_CENTER:\n            start_idx = (kernlen - 1) / 2;\n            out_len = siglen;\n            break;\n        case TINY_CONV_TAIL:\n            start_idx = siglen - 1;\n            out_len = kernlen;\n            break;\n        default:\n            return TINY_ERR_DSP_INVALID_MODE;\n        }\n\n        // Copy the selected part to the beginning\n        for (int i = 0; i &lt; out_len; i++)\n        {\n            convout[i] = convout[start_idx + i];\n        }\n    }\n\n    return TINY_OK;\n}\n</code></pre>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/","title":"NOTES","text":""},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#mathematical-principle-of-convolution","title":"Mathematical Principle of Convolution","text":"<p>Convolution is an important operation in signal processing, which is used to describe the relationship between two signals. It can be regarded as the weighted average of one signal and another signal. The mathematical definition of convolution is as follows:</p> \\[y(t) = \\int_{-\\infty}^{\\infty} x(\\tau) h(t - \\tau) d\\tau\\] <p>Where \\(x(t)\\) is the input signal, \\(h(t)\\) is the impulse response of the system, and \\(y(t)\\) is the output signal. The result of convolution is a new signal that contains all the information between the input signal and the impulse response of the system.</p> <p></p> <ul> <li> <p> Physical Meaning of Convolution \uff08Chinese\uff09</p> <p>  Portal </p> </li> </ul>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#programming-philosophy","title":"Programming Philosophy","text":"<p>The convolution operation in this library actually reverses the direction of the convolution kernel and then multiplies it point by point with the input signal and sums it.</p>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_f32","title":"tiny_conv_f32","text":"<pre><code>/**\n * @name: tiny_conv_f32\n * @brief Convolution function\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *convout)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // ESP32 DSP library\n    dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n#else\n    float *sig = (float *)Signal;\n    float *kern = (float *)Kernel;\n    int lsig = siglen;\n    int lkern = kernlen;\n\n    // stage I\n    for (int n = 0; n &lt; lkern; n++)\n    {\n        size_t k;\n\n        convout[n] = 0;\n\n        for (k = 0; k &lt;= n; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage II\n    for (int n = lkern; n &lt; lsig; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = n;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage III\n    for (int n = lsig; n &lt; lsig + lkern - 1; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = lsig - 1;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n#endif\n\n    return TINY_OK;\n}\n</code></pre> <p>Description: This function performs the convolution operation between the input signal and the kernel. It first checks whether the input parameters are <code>NULL</code>, and then selects either the ESP32 DSP library or a standard C implementation for the convolution calculation based on the platform. The function returns the convolution result.</p> <p>Features:</p> <ul> <li>Supports acceleration with the ESP32 DSP library.</li> <li>Supports swapping the signal and kernel to ensure the signal length is greater than the kernel length.</li> </ul> <p>Parameters:</p> <ul> <li><code>Signal</code>: Input signal array.</li> <li><code>siglen</code>: Length of the input signal array.</li> <li><code>Kernel</code>: Input kernel array.</li> <li><code>kernlen</code>: Length of the input kernel array.</li> <li><code>convout</code>: Output array to store the convolution result.</li> </ul> <p>Return Value:</p> <ul> <li><code>TINY_OK</code>: Convolution completed successfully.</li> <li><code>TINY_ERR_DSP_NULL_POINTER</code>: One or more input parameters are <code>NULL</code>.</li> </ul>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_ex_f32","title":"tiny_conv_ex_f32","text":"<pre><code>/**\n * @name: tiny_conv_ex_f32\n * @brief Extended convolution function with padding and mode options\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n * @param padding_mode Padding mode (zero, symmetric, periodic)\n * @param conv_mode Convolution mode (full, head, center, tail)\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_ex_f32(const float *Signal, const int siglen,\n                              const float *Kernel, const int kernlen,\n                              float *convout,\n                              tiny_padding_mode_t padding_mode,\n                              tiny_conv_mode_t conv_mode)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padding_mode == TINY_PADDING_ZERO &amp;&amp; conv_mode == TINY_CONV_FULL)\n    {\n        dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n        return TINY_OK;\n    }\n#endif\n\n    int pad_len = kernlen - 1;\n    int padded_len = siglen + 2 * pad_len;\n    float *padded_signal = (float *)calloc(padded_len, sizeof(float));\n    if (padded_signal == NULL)\n    {\n        return TINY_ERR_DSP_MEMORY_ALLOC;\n    }\n\n    // Fill padded signal\n    switch (padding_mode)\n    {\n    case TINY_PADDING_ZERO:\n        // Middle copy only, left and right are zeros (calloc already zeroed)\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen);\n        break;\n\n    case TINY_PADDING_SYMMETRIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[i];                   // Mirror left\n            padded_signal[pad_len + siglen + i] = Signal[siglen - 1 - i]; // Mirror right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    case TINY_PADDING_PERIODIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[(siglen - pad_len + i) % siglen]; // Wrap left\n            padded_signal[pad_len + siglen + i] = Signal[i % siglen];                 // Wrap right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    default:\n        free(padded_signal);\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n    // Full convolution\n    int convlen_full = siglen + kernlen - 1;\n    for (int n = 0; n &lt; convlen_full; n++)\n    {\n        float sum = 0.0f;\n        for (int k = 0; k &lt; kernlen; k++)\n        {\n            sum += padded_signal[n + k] * Kernel[kernlen - 1 - k]; // Convolution is flip+slide\n        }\n        convout[n] = sum;\n    }\n\n    free(padded_signal);\n\n    // Handle output mode\n    if (conv_mode == TINY_CONV_FULL)\n    {\n        return TINY_OK;\n    }\n    else\n    {\n        int start_idx = 0;\n        int out_len = 0;\n\n        switch (conv_mode)\n        {\n        case TINY_CONV_HEAD:\n            start_idx = 0;\n            out_len = kernlen;\n            break;\n        case TINY_CONV_CENTER:\n            start_idx = (kernlen - 1) / 2;\n            out_len = siglen;\n            break;\n        case TINY_CONV_TAIL:\n            start_idx = siglen - 1;\n            out_len = kernlen;\n            break;\n        default:\n            return TINY_ERR_DSP_INVALID_MODE;\n        }\n\n        // Copy the selected part to the beginning\n        for (int i = 0; i &lt; out_len; i++)\n        {\n            convout[i] = convout[start_idx + i];\n        }\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>Description:</p> <p>This function performs an extended convolution operation with options for padding and output mode. It first checks the input parameters and allocates memory for the padded signal. Depending on the selected padding mode, it fills the padded signal accordingly. The convolution is then performed, and the result is returned based on the specified output mode.</p> <p>Features:</p> <ul> <li> <p>Supports different padding modes: zero, symmetric, periodic.</p> </li> <li> <p>Supports different convolution modes: full, head, center, tail.</p> </li> <li> <p>Handles memory allocation and deallocation for the padded signal.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>Signal</code>: Input signal array.</p> </li> <li> <p><code>siglen</code>: Length of the input signal array.</p> </li> <li> <p><code>Kernel</code>: Input kernel array.</p> </li> <li> <p><code>kernlen</code>: Length of the input kernel array.</p> </li> <li> <p><code>convout</code>: Output array for the convolution result.</p> </li> <li> <p><code>padding_mode</code>: Padding mode (zero, symmetric, periodic).</p> </li> <li> <p><code>conv_mode</code>: Convolution mode (full, head, center, tail).</p> </li> </ul> <p>Return Value:</p> <ul> <li> <p><code>TINY_OK</code>: Convolution completed successfully.</p> </li> <li> <p><code>TINY_ERR_DSP_NULL_POINTER</code>: One or more input parameters are <code>NULL</code>.</p> </li> <li> <p><code>TINY_ERR_DSP_INVALID_PARAM</code>: Invalid parameters provided.</p> </li> <li> <p><code>TINY_ERR_DSP_MEMORY_ALLOC</code>: Memory allocation failed.</p> </li> </ul>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#function-comparison","title":"Function Comparison","text":"<p>To help readers choose the appropriate function for their needs, here is a comparison between <code>tiny_conv_f32</code> and <code>tiny_conv_ex_f32</code>:</p> Feature <code>tiny_conv_f32</code> <code>tiny_conv_ex_f32</code> Padding Mode Zero padding only (implicit) Zero, symmetric, or periodic padding (explicit) Output Mode Full convolution only Full, head, center, or tail modes Output Length <code>siglen + kernlen - 1</code> Configurable based on <code>conv_mode</code> Memory Usage No dynamic allocation Requires dynamic memory allocation for padded signal Performance Optimized (ESP32 hardware acceleration available) Optimized only when using zero padding + full mode on ESP32 Use Cases Simple full convolution with zero padding Advanced convolution with custom padding and output modes"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#when-to-use-tiny_conv_f32","title":"When to Use <code>tiny_conv_f32</code>","text":"<p>Use <code>tiny_conv_f32</code> when:</p> <ul> <li> <p>You need a simple full convolution result</p> </li> <li> <p>Zero padding is acceptable for boundary handling</p> </li> <li> <p>You want maximum performance (especially on ESP32 with hardware acceleration)</p> </li> <li> <p>You want to avoid dynamic memory allocation</p> </li> <li> <p>The output length <code>siglen + kernlen - 1</code> is acceptable</p> </li> </ul> <p>Example scenarios:</p> <ul> <li> <p>Basic signal filtering</p> </li> <li> <p>Simple correlation operations</p> </li> <li> <p>Real-time processing where memory allocation should be avoided</p> </li> </ul>"},{"location":"DSP/SIGNAL/CONVOLUTION/notes/#when-to-use-tiny_conv_ex_f32","title":"When to Use <code>tiny_conv_ex_f32</code>","text":"<p>Use <code>tiny_conv_ex_f32</code> when: - You need different padding strategies (symmetric or periodic) to handle signal boundaries</p> <ul> <li> <p>You want to extract specific parts of the convolution result (head, center, or tail)</p> </li> <li> <p>You need output length equal to input signal length (center mode)</p> </li> <li> <p>You are processing signals where boundary effects matter (e.g., image processing, periodic signals)</p> </li> <li> <p>You can tolerate dynamic memory allocation</p> </li> </ul> <p>Example scenarios:</p> <ul> <li> <p>Image filtering with symmetric padding to reduce boundary artifacts</p> </li> <li> <p>Processing periodic signals with periodic padding</p> </li> <li> <p>Extracting only the valid convolution region (center mode) when output length should match input</p> </li> <li> <p>Advanced signal processing where boundary handling is critical</p> </li> </ul>"},{"location":"DSP/SIGNAL/CONVOLUTION/test/","title":"TESTS","text":""},{"location":"DSP/SIGNAL/CONVOLUTION/test/#tiny_conv_testh","title":"tiny_conv_test.h","text":"<pre><code>/**\n * @file tiny_conv_test.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_conv | test | header\n * @version 1.0\n * @date 2025-04-28\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// tiny_conv\n#include \"tiny_conv.h\"\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\nvoid tiny_signal_conv_test(void);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/CONVOLUTION/test/#tiny_conv_testc","title":"tiny_conv_test.c","text":"<pre><code>/**\n * @file tiny_conv_test.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_conv | test | header\n * @version 1.0\n * @date 2025-04-28\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n// tiny_conv\n#include \"tiny_conv_test.h\"\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n\n#define EPSILON 1e-5  // Tolerance for floating-point comparison\n\n// Helper function to compare two float arrays\nint compare_float_arrays(const float *a, const float *b, int len, float tol)\n{\n    for (int i = 0; i &lt; len; i++)\n    {\n        if (fabs(a[i] - b[i]) &gt; tol)\n        {\n            return 0; // Not equal\n        }\n    }\n    return 1; // Equal\n}\n\n// Test function\nvoid tiny_signal_conv_test(void)\n{\n    printf(\"===== tiny_conv_f32 and tiny_conv_ex_f32 Test Start =====\\n\");\n\n    // Define sample signal and kernel\n    float signal[] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f, 11.0f};\n    float kernel[] = {0.2f, 0.5f, 0.3f};\n    int siglen = sizeof(signal) / sizeof(signal[0]);\n    int kernlen = sizeof(kernel) / sizeof(kernel[0]);\n\n    int convlen_full = siglen + kernlen - 1;\n    float convout_ref[convlen_full];\n    float convout_test[convlen_full];\n\n    // show signal and kernel \n    printf(\"Signal: \");\n    for (int i = 0; i &lt; siglen; i++)\n    {\n        printf(\"%.2f \", signal[i]);\n    }\n    printf(\"\\nKernel: \");\n    for (int i = 0; i &lt; kernlen; i++)\n    {\n        printf(\"%.2f \", kernel[i]);\n    }\n    printf(\"\\n\");\n\n    // 1. Test basic tiny_conv_f32\n    printf(\"\\n[Test] tiny_conv_f32 basic full convolution...\\n\");\n    memset(convout_ref, 0, sizeof(convout_ref));\n    if (tiny_conv_f32(signal, siglen, kernel, kernlen, convout_ref) != TINY_OK)\n    {\n        printf(\"[FAIL] tiny_conv_f32 error.\\n\");\n        return;\n    }\n    printf(\"[PASS] tiny_conv_f32 full convolution completed.\\n\");\n\n    // 2. Test tiny_conv_ex_f32 with padding = zero, conv_mode = full\n    printf(\"\\n[Test] tiny_conv_ex_f32 padding=zero, mode=full...\\n\");\n    memset(convout_test, 0, sizeof(convout_test));\n    if (tiny_conv_ex_f32(signal, siglen, kernel, kernlen, convout_test, TINY_PADDING_ZERO, TINY_CONV_FULL) != TINY_OK)\n    {\n        printf(\"[FAIL] tiny_conv_ex_f32 error.\\n\");\n        return;\n    }\n\n    if (compare_float_arrays(convout_ref, convout_test, convlen_full, EPSILON))\n    {\n        printf(\"[PASS] tiny_conv_ex_f32 matches tiny_conv_f32 (zero padding, full mode).\\n\");\n    }\n    else\n    {\n        printf(\"[FAIL] tiny_conv_ex_f32 result mismatch!\\n\");\n        return;\n    }\n\n    // 3. Test tiny_conv_ex_f32 with different padding modes\n    tiny_padding_mode_t paddings[] = {TINY_PADDING_ZERO, TINY_PADDING_SYMMETRIC, TINY_PADDING_PERIODIC};\n    const char *padding_names[] = {\"ZERO\", \"SYMMETRIC\", \"PERIODIC\"};\n\n    // 4. Test different convolution modes\n    tiny_conv_mode_t modes[] = {TINY_CONV_FULL, TINY_CONV_HEAD, TINY_CONV_CENTER, TINY_CONV_TAIL};\n    const char *mode_names[] = {\"FULL\", \"HEAD\", \"CENTER\", \"TAIL\"};\n\n    for (int p = 0; p &lt; 3; p++)\n    {\n        for (int m = 0; m &lt; 4; m++)\n        {\n            printf(\"\\n[Test] tiny_conv_ex_f32 padding=%s, mode=%s...\\n\", padding_names[p], mode_names[m]);\n            memset(convout_test, 0, sizeof(convout_test));\n\n            if (tiny_conv_ex_f32(signal, siglen, kernel, kernlen, convout_test, paddings[p], modes[m]) != TINY_OK)\n            {\n                printf(\"[FAIL] tiny_conv_ex_f32 error at padding=%s, mode=%s.\\n\", padding_names[p], mode_names[m]);\n                continue;\n            }\n\n            printf(\"[PASS] tiny_conv_ex_f32 completed (padding=%s, mode=%s).\\n\", padding_names[p], mode_names[m]);\n            printf(\"Output:\\n\");\n\n            int out_len = 0;\n            switch (modes[m])\n            {\n            case TINY_CONV_FULL:\n                out_len = siglen + kernlen - 1;\n                break;\n            case TINY_CONV_HEAD:\n                out_len = kernlen;\n                break;\n            case TINY_CONV_CENTER:\n                out_len = siglen;\n                break;\n            case TINY_CONV_TAIL:\n                out_len = kernlen;\n                break;\n            default:\n                break;\n            }\n\n            for (int i = 0; i &lt; out_len; i++)\n            {\n                printf(\"%.5f \", convout_test[i]);\n            }\n            printf(\"\\n\");\n        }\n    }\n\n    printf(\"\\n===== tiny_conv_f32 and tiny_conv_ex_f32 Test End =====\\n\");\n}\n</code></pre>"},{"location":"DSP/SIGNAL/CONVOLUTION/test/#test-results","title":"test results","text":"<pre><code>===== tiny_conv_f32 and tiny_conv_ex_f32 Test Start =====\nSignal: 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11.00 \nKernel: 0.20 0.50 0.30 \n\n[Test] tiny_conv_f32 basic full convolution...\n[PASS] tiny_conv_f32 full convolution completed.\n\n[Test] tiny_conv_ex_f32 padding=zero, mode=full...\n[PASS] tiny_conv_ex_f32 matches tiny_conv_f32 (zero padding, full mode).\n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=FULL).\nOutput:\n0.20000 0.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.50000 3.30000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=HEAD).\nOutput:\n0.20000 0.90000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=CENTER).\nOutput:\n0.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.50000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=TAIL).\nOutput:\n9.90000 8.50000 3.30000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=FULL).\nOutput:\n1.30000 1.20000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 10.70000 10.80000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=HEAD).\nOutput:\n1.30000 1.20000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=CENTER).\nOutput:\n1.20000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 10.70000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=TAIL).\nOutput:\n9.90000 10.70000 10.80000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=FULL).\nOutput:\n8.50000 3.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.70000 4.20000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=HEAD).\nOutput:\n8.50000 3.90000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=CENTER).\nOutput:\n3.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.70000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=TAIL).\nOutput:\n9.90000 8.70000 4.20000 \n\n===== tiny_conv_f32 and tiny_conv_ex_f32 Test End =====\n</code></pre>"},{"location":"DSP/SIGNAL/CORRELATION/code/","title":"CODE","text":""},{"location":"DSP/SIGNAL/CORRELATION/code/#tiny_corrh","title":"tiny_corr.h","text":"<pre><code>/**\n * @file tiny_corr.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_corr | code | header\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// tiny_dsp configuration file\n#include \"tiny_dsp_config.h\"\n\n// ESP32 DSP Library for Acceleration\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32 // ESP32 DSP library\n\n#include \"dsps_corr.h\"\n#include \"dsps_ccorr.h\"\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /**\n     * @name: tiny_corr_f32\n     * @brief Correlation function\n     *\n     * @param Signal: input signal array\n     * @param siglen: length of the signal array\n     * @param Pattern: input pattern array\n     * @param patlen: length of the pattern array\n     * @param dest: output array for the correlation result\n     *\n     * @return tiny_error_t\n     */\n    tiny_error_t tiny_corr_f32(const float *Signal, const int siglen, const float *Pattern, const int patlen, float *dest);\n\n    /**\n     * @name: tiny_ccorr_f32\n     * @brief Cross-correlation function\n     *\n     * @param Signal: input signal array\n     * @param siglen: length of the signal array\n     * @param Kernel: input kernel array\n     * @param kernlen: length of the kernel array\n     * @param corrvout: output array for the cross-correlation result\n     *\n     * @return tiny_error_t\n     */\n    tiny_error_t tiny_ccorr_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *corrvout);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/CORRELATION/code/#tiny_corrc","title":"tiny_corr.c","text":"<pre><code>/**\n * @file tiny_corr.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_corr | code | source\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n/* DEPENDENCIES */\n#include \"tiny_corr.h\"\n\n/**\n * @name: tiny_corr_f32\n * @brief Correlation function\n *\n * @param Signal: input signal array\n * @param siglen: length of the signal array\n * @param Pattern: input pattern array\n * @param patlen: length of the pattern array\n * @param dest: output array for the correlation result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_corr_f32(const float *Signal, const int siglen, const float *Pattern, const int patlen, float *dest)\n{\n    if (NULL == Signal || NULL == Pattern || NULL == dest)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n\n    if (siglen &lt; patlen) // signal length shoudl be greater than pattern length\n    {\n        return TINY_ERR_DSP_MISMATCH;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_corr_f32(Signal, siglen, Pattern, patlen, dest);\n#else\n\n    for (size_t n = 0; n &lt;= (siglen - patlen); n++)\n    {\n        float k_corr = 0;\n        for (size_t m = 0; m &lt; patlen; m++)\n        {\n            k_corr += Signal[n + m] * Pattern[m];\n        }\n        dest[n] = k_corr;\n    }\n\n#endif\n\n    return TINY_OK;\n}\n\n/**\n * @name: tiny_ccorr_f32\n * @brief Cross-correlation function\n *\n * @param Signal: input signal array\n * @param siglen: length of the signal array\n * @param Kernel: input kernel array\n * @param kernlen: length of the kernel array\n * @param corrvout: output array for the cross-correlation result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_ccorr_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *corrvout)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == corrvout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n\n    float *sig = (float *)Signal;\n    float *kern = (float *)Kernel;\n    int lsig = siglen;\n    int lkern = kernlen;\n\n    // swap signal and kernel if needed\n    if (siglen &lt; kernlen)\n    {\n        sig = (float *)Kernel;\n        kern = (float *)Signal;\n        lsig = kernlen;\n        lkern = siglen;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_ccorr_f32(Signal, siglen, Kernel, kernlen, corrvout);\n#else\n    // stage I\n    for (int n = 0; n &lt; lkern; n++)\n    {\n        size_t k;\n        size_t kmin = lkern - 1 - n;\n        corrvout[n] = 0;\n\n        for (k = 0; k &lt;= n; k++)\n        {\n            corrvout[n] += sig[k] * kern[kmin + k];\n        }\n    }\n\n    // stage II\n    for (int n = lkern; n &lt; lsig; n++)\n    {\n        size_t kmin, kmax, k;\n\n        corrvout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = n;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            corrvout[n] += sig[k] * kern[k - kmin];\n        }\n    }\n\n    // stage III\n    for (int n = lsig; n &lt; lsig + lkern - 1; n++)\n    {\n        size_t kmin, kmax, k;\n\n        corrvout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = lsig - 1;\n\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            corrvout[n] += sig[k] * kern[k - kmin];\n        }\n    }\n#endif\n    return TINY_OK;\n}\n</code></pre>"},{"location":"DSP/SIGNAL/CORRELATION/notes/","title":"NOTES","text":"<p>Note</p> <p>Correlation is an important concept in signal processing, often used to analyze similarities or dependencies between signals. It is useful in many applications, such as pattern recognition, time series analysis, and signal detection.</p>"},{"location":"DSP/SIGNAL/CORRELATION/notes/#correlation-function","title":"CORRELATION FUNCTION","text":""},{"location":"DSP/SIGNAL/CORRELATION/notes/#mathematical-principle","title":"Mathematical Principle","text":"<p>The correlation is computed as:</p> \\[ \\text{Correlation}[n] = \\sum_{m=0}^{L_p - 1} S[n + m] \\cdot P[m] \\] <p>Where:</p> <ul> <li> <p>\\( S \\) is the input signal of length \\( L_s \\)</p> </li> <li> <p>\\( P \\) is the pattern of length \\( L_p \\)</p> </li> <li> <p>\\( n \\in [0, L_s - L_p] \\)</p> </li> </ul> <p>Output Length:</p> \\[ L_{\\text{out}} = L_s - L_p + 1 \\]"},{"location":"DSP/SIGNAL/CORRELATION/notes/#tiny_corr_f32","title":"tiny_corr_f32","text":"<pre><code>/**\n * @name: tiny_corr_f32\n * @brief Correlation function\n *\n * @param Signal: input signal array\n * @param siglen: length of the signal array\n * @param Pattern: input pattern array\n * @param patlen: length of the pattern array\n * @param dest: output array for the correlation result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_corr_f32(const float *Signal, const int siglen, const float *Pattern, const int patlen, float *dest)\n{\n    if (NULL == Signal || NULL == Pattern || NULL == dest)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n\n    if (siglen &lt; patlen) // signal length shoudl be greater than pattern length\n    {\n        return TINY_ERR_DSP_MISMATCH;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_corr_f32(Signal, siglen, Pattern, patlen, dest);\n#else\n\n    for (size_t n = 0; n &lt;= (siglen - patlen); n++)\n    {\n        float k_corr = 0;\n        for (size_t m = 0; m &lt; patlen; m++)\n        {\n            k_corr += Signal[n + m] * Pattern[m];\n        }\n        dest[n] = k_corr;\n    }\n\n#endif\n\n    return TINY_OK;\n}\n</code></pre> <p>Description: </p> <p>Computes the correlation between a signal and a pattern.</p> <p>Features:</p> <ul> <li>Platform-specific optimization enabled.</li> </ul> <p>Parameters:</p> <ul> <li> <p><code>Signal</code>: Pointer to the input signal array.</p> </li> <li> <p><code>siglen</code>: Length of the signal array.</p> </li> <li> <p><code>Pattern</code>: Pointer to the input pattern array.</p> </li> <li> <p><code>patlen</code>: Length of the pattern array.</p> </li> <li> <p><code>dest</code>: Pointer to the output array for the correlation result.</p> </li> </ul> <p>Return Value: Returns success or error code.</p>"},{"location":"DSP/SIGNAL/CORRELATION/notes/#cross-correlation-function","title":"CROSS-CORRELATION FUNCTION","text":""},{"location":"DSP/SIGNAL/CORRELATION/notes/#mathematical-principle_1","title":"Mathematical Principle","text":"<p>The cross-correlation is computed as:</p> \\[ R_{xy}[n] = \\sum_{k} x[k] \\cdot y[k + n] \\] <p>Where:</p> <ul> <li> <p>\\( x \\) is the signal of length \\( L_x \\)</p> </li> <li> <p>\\( y \\) is the kernel of length \\( L_y \\)</p> </li> <li> <p>\\( n \\in [0, L_x + L_y - 2] \\)</p> </li> </ul> <p>Output Length:</p> \\[ L_{\\text{out}} = L_x + L_y - 1 \\]"},{"location":"DSP/SIGNAL/CORRELATION/test/","title":"TESTS","text":""},{"location":"DSP/SIGNAL/CORRELATION/test/#tiny_corr_testh","title":"tiny_corr_test.h","text":"<pre><code>/**\n * @file tiny_corr_test.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_corr | test | header\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_corr.h\"\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\nvoid tiny_signal_corr_ccorr_test(void);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/CORRELATION/test/#tiny_corr_testc","title":"tiny_corr_test.c","text":"<pre><code>/**\n * @file tiny_corr_test.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_corr | test | source\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_corr_test.h\"\n\n#define EPSILON 1e-3f // Tolerance for floating-point comparison\n\n// Helper function to print float arrays\nstatic void print_float_array(const char *label, const float *array, int length) {\n    printf(\"%s: [\", label);\n    for (int i = 0; i &lt; length; i++) {\n        printf(\"%.2f\", array[i]);\n        if (i != length -1) {\n            printf(\", \");\n        }\n    }\n    printf(\"]\\n\");\n}\n\nvoid tiny_signal_corr_ccorr_test(void)\n{\n    printf(\"\\n========== Correlation &amp; Cross-Correlation Test ==========\\n\");\n\n    /*** Test 1: Correlation Function - tiny_corr_f32 ***/\n    const float signal[]  = {1.0f, 2.0f, 3.0f, 4.0f, 2.0f, 1.0f};\n    const float pattern[] = {2.0f, 1.0f, 0.0f};\n    const int siglen  = sizeof(signal) / sizeof(signal[0]);\n    const int patlen  = sizeof(pattern) / sizeof(pattern[0]);\n    const int corr_len = siglen - patlen + 1;\n\n    float corr_output[corr_len];\n    const float expected_corr[] = {4.0f, 7.0f, 10.0f, 10.0f};\n\n    printf(\"\\n--- Test 1: tiny_corr_f32 ---\\n\");\n    print_float_array(\"Input Signal \", signal, siglen);\n    print_float_array(\"Pattern      \", pattern, patlen);\n\n    tiny_error_t status_corr = tiny_corr_f32(signal, siglen, pattern, patlen, corr_output);\n\n    if (status_corr != TINY_OK) {\n        printf(\"[tiny_corr_f32 Test] Failed with error code: %d\\n\", status_corr);\n    } else {\n        printf(\"Output vs Expected:\\n\");\n        int pass = 1;\n        for (int i = 0; i &lt; corr_len; i++) {\n            printf(\"  [%d] Output = %.3f | Expected = %.3f\\n\", i, corr_output[i], expected_corr[i]);\n            if (fabs(corr_output[i] - expected_corr[i]) &gt; EPSILON) {\n                pass = 0;\n            }\n        }\n        printf(\"[tiny_corr_f32 Test] [%s]\\n\", pass ? \"PASS\" : \"FAIL\");\n    }\n\n    /*** Test 2: Cross-Correlation Function - tiny_ccorr_f32 ***/\n    const float x[] = {1.0f, 3.0f, 2.0f, 0.0f, 1.0f, 2.0f};\n    const float y[] = {2.0f, 1.0f, 0.0f, -1.0f};\n    const int len_x = sizeof(x) / sizeof(x[0]);\n    const int len_y = sizeof(y) / sizeof(y[0]);\n    const int ccorr_len = len_x + len_y - 1;\n\n    float ccorr_output[ccorr_len];\n    const float expected_ccorr[] = {-1.0f, -3.0f, -1.0f, 5.0f, 7.0f, 2.0f, 1.0f, 4.0f, 4.0f};\n\n    printf(\"\\n--- Test 2: tiny_ccorr_f32 ---\\n\");\n    print_float_array(\"Input Signal X\", x, len_x);\n    print_float_array(\"Input Signal Y\", y, len_y);\n\n    tiny_error_t status_ccorr = tiny_ccorr_f32(x, len_x, y, len_y, ccorr_output);\n\n    if (status_ccorr != TINY_OK) {\n        printf(\"[tiny_ccorr_f32 Test] Failed with error code: %d\\n\", status_ccorr);\n    } else {\n        printf(\"Output vs Expected:\\n\");\n        int pass = 1;\n        for (int i = 0; i &lt; ccorr_len; i++) {\n            printf(\"  [%d] Output = %.3f | Expected = %.3f\\n\", i, ccorr_output[i], expected_ccorr[i]);\n            if (fabs(ccorr_output[i] - expected_ccorr[i]) &gt; EPSILON) {\n                pass = 0;\n            }\n        }\n        printf(\"[tiny_ccorr_f32 Test] [%s]\\n\", pass ? \"PASS\" : \"FAIL\");\n    }\n\n    printf(\"==========================================================\\n\");\n}\n</code></pre>"},{"location":"DSP/SIGNAL/CORRELATION/test/#test-results","title":"TEST RESULTS","text":"<pre><code>========== Correlation &amp; Cross-Correlation Test ==========\n\n--- Test 1: tiny_corr_f32 ---\nInput Signal : [1.00, 2.00, 3.00, 4.00, 2.00, 1.00]\nPattern      : [2.00, 1.00, 0.00]\nOutput vs Expected:\n  [0] Output = 4.000 | Expected = 4.000\n  [1] Output = 7.000 | Expected = 7.000\n  [2] Output = 10.000 | Expected = 10.000\n  [3] Output = 10.000 | Expected = 10.000\n[tiny_corr_f32 Test] [PASS]\n\n--- Test 2: tiny_ccorr_f32 ---\nInput Signal X: [1.00, 3.00, 2.00, 0.00, 1.00, 2.00]\nInput Signal Y: [2.00, 1.00, 0.00, -1.00]\nOutput vs Expected:\n  [0] Output = -1.000 | Expected = -1.000\n  [1] Output = -3.000 | Expected = -3.000\n  [2] Output = -1.000 | Expected = -1.000\n  [3] Output = 5.000 | Expected = 5.000\n  [4] Output = 7.000 | Expected = 7.000\n  [5] Output = 2.000 | Expected = 2.000\n  [6] Output = 1.000 | Expected = 1.000\n  [7] Output = 4.000 | Expected = 4.000\n  [8] Output = 4.000 | Expected = 4.000\n[tiny_ccorr_f32 Test] [PASS]\n==========================================================\n</code></pre>"},{"location":"DSP/SIGNAL/RESAMPLE/code/","title":"CODE","text":""},{"location":"DSP/SIGNAL/RESAMPLE/code/#tiny_resampleh","title":"tiny_resample.h","text":"<pre><code>/**\n * @file tiny_resample.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_resample | code | header\n * @version 1.0\n * @date 2025-04-29\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// tiny_dsp configuration file\n#include \"tiny_dsp_config.h\"\n\n// ESP32 DSP Library for Acceleration\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32 // ESP32 DSP library\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /**\n     * @name tiny_downsample_skip_f32\n     * @brief Downsample a signal by a given factor using skipping\n     *\n     * @param input pointer to the input signal array\n     * @param input_len length of the input signal array\n     * @param output pointer to the output signal array\n     * @param output_len pointer to the length of the output signal array\n     * @param keep number of samples to keep\n     * @param skip number of samples to skip\n     *\n     * @return tiny_error_t\n     */\n    tiny_error_t tiny_downsample_skip_f32(const float *input, int input_len, float *output, int *output_len, int keep, int skip);\n\n    /**\n     * @name tiny_upsample_zero_f32\n     * @brief Upsample a signal using zero-insertion between samples\n     *\n     * @param input pointer to the input signal array\n     * @param input_len length of the input signal array\n     * @param output pointer to the output signal array\n     * @param target_len target length for the output signal array\n     * @return tiny_error_t\n     */\n    tiny_error_t tiny_upsample_zero_f32(const float *input, int input_len, float *output, int target_len);\n\n    /**\n     * @name: tiny_resample_f32\n     * @brief Resample a signal to a target length\n     *\n     * @param input pointer to the input signal array\n     * @param input_len length of the input signal array\n     * @param output pointer to the output signal array\n     * @param target_len target length for the output signal array\n     * @return tiny_error_t\n     */\n    tiny_error_t tiny_resample_f32(const float *input,\n                                   int input_len,\n                                   float *output,\n                                   int target_len);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/RESAMPLE/code/#tiny_resamplec","title":"tiny_resample.c","text":"<pre><code>/**\n * @file tiny_resample.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_resample | code | source\n * @version 1.0\n * @date 2025-04-29\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_resample.h\" // tiny_resample header\n\n/**\n * @name tiny_downsample_skip_f32\n * @brief Downsample a signal by a given factor using skipping\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param output_len pointer to the length of the output signal array\n * @param keep number of samples to keep\n * @param skip number of samples to skip\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_downsample_skip_f32(const float *input, int input_len, float *output, int *output_len, int keep, int skip)\n{\n    if (!input || !output || !output_len)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || keep &lt;= 0 || skip &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int out_len = input_len / skip;\n    *output_len = out_len;\n\n    for (int i = 0; i &lt; out_len; i++)\n    {\n        output[i] = input[i * skip];\n    }\n\n    return TINY_OK;\n}\n\n/**\n * @name tiny_upsample_zero_f32\n * @brief Upsample a signal using zero-insertion between samples\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_upsample_zero_f32(const float *input, int input_len, float *output, int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int factor = target_len / input_len;\n    if (factor &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        output[i] = (i % factor == 0) ? input[i / factor] : 0.0f;\n    }\n\n    return TINY_OK;\n}\n\n\n/**\n * @name: tiny_resample_f32\n * @brief Resample a signal to a target length\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_resample_f32(const float *input,\n                               int input_len,\n                               float *output,\n                               int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    float ratio = (float)(target_len) / (float)(input_len);\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        float pos = i / ratio;\n        int index = (int)floorf(pos);\n        float frac = pos - index;\n\n        if (index &gt;= input_len - 1)\n            output[i] = input[input_len - 1]; // Clamp at end\n        else\n            output[i] = input[index] * (1.0f - frac) + input[index + 1] * frac;\n    }\n\n    return TINY_OK;\n}\n</code></pre>"},{"location":"DSP/SIGNAL/RESAMPLE/notes/","title":"NOTES","text":"<p>Note</p> <p>Resampling is an important step in signal processing, typically used to change the sampling rate of a signal. It can be used in audio, video, and other types of signal processing.</p>"},{"location":"DSP/SIGNAL/RESAMPLE/notes/#signal-downsampling-skip","title":"SIGNAL DOWNSAMPLING - SKIP","text":"<pre><code>Signal downsampling by skipping samples is a method of selecting samples from the original signal at regular intervals. It is typically used to reduce the sampling rate of a signal. Note that this is different from decimation, which involves filtering before downsampling.\n</code></pre> <pre><code>/**\n * @name tiny_downsample_skip_f32\n * @brief Downsample a signal by a given factor using skipping\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param output_len pointer to the length of the output signal array\n * @param keep number of samples to keep\n * @param skip number of samples to skip\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_downsample_skip_f32(const float *input, int input_len, float *output, int *output_len, int keep, int skip)\n{\n    if (!input || !output || !output_len)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || keep &lt;= 0 || skip &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int out_len = input_len / skip;\n    *output_len = out_len;\n\n    for (int i = 0; i &lt; out_len; i++)\n    {\n        output[i] = input[i * skip];\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>***Description**:</p> <p>Signal downsampling function that uses two integer parameters <code>keep</code> and <code>skip</code> to control the downsampling process. <code>keep</code> indicates the number of samples to keep, while <code>skip</code> indicates the number of samples to skip.</p> <p>Features:</p> <ul> <li> <p>Integer Factor Downsampling</p> </li> <li> <p>Skip Downsampling</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input signal array</p> </li> <li> <p><code>input_len</code>: Length of the input signal array</p> </li> <li> <p><code>output</code>: Pointer to the output signal array</p> </li> <li> <p><code>output_len</code>: Pointer to the length of the output signal array</p> </li> <li> <p><code>keep</code>: Number of samples to keep</p> </li> <li> <p><code>skip</code>: Number of samples to skip</p> </li> </ul>"},{"location":"DSP/SIGNAL/RESAMPLE/notes/#signal-upsampling-0-insertion","title":"SIGNAL UPSAMPLING - 0 INSERTION","text":"<pre><code>Signal upsampling by inserting zeros is a method of increasing the sampling rate of a signal by inserting zeros between the original samples. This is typically used to prepare a signal for further processing, such as filtering.\n</code></pre> <pre><code>/**\n * @name tiny_upsample_zero_f32\n * @brief Upsample a signal using zero-insertion between samples\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_upsample_zero_f32(const float *input, int input_len, float *output, int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int factor = target_len / input_len;\n    if (factor &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        output[i] = (i % factor == 0) ? input[i / factor] : 0.0f;\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>***Description**:</p> <p>Signal upsampling is the process of increasing the sampling rate of a signal by inserting zeros between the original samples. It is typically used to increase the sampling rate of a signal. Note that this is different from interpolation, which involves filling in gaps between samples.</p> <p>Features:</p> <ul> <li> <p>Integer Factor Upsampling</p> </li> <li> <p>Zero Insertion</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input signal array</p> </li> <li> <p><code>input_len</code>: Length of the input signal array</p> </li> <li> <p><code>output</code>: Pointer to the output signal array</p> </li> <li> <p><code>target_len</code>: Target length for the output signal array</p> </li> </ul>"},{"location":"DSP/SIGNAL/RESAMPLE/notes/#signal-resampling-any-factor-linear-interpolation","title":"SIGNAL RESAMPLING - ANY FACTOR - LINEAR INTERPOLATION","text":"<pre><code>Signal resampling is the process of changing the sampling rate of a signal. We need to first calculate the proportional factor and then use interpolation to fill in the gaps.\n</code></pre> <pre><code>/**\n * @name: tiny_resample_f32\n * @brief Resample a signal to a target length\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_resample_f32(const float *input,\n                               int input_len,\n                               float *output,\n                               int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    float ratio = (float)(target_len) / (float)(input_len);\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        float pos = i / ratio;\n        int index = (int)floorf(pos);\n        float frac = pos - index;\n\n        if (index &gt;= input_len - 1)\n            output[i] = input[input_len - 1]; // Clamp at end\n        else\n            output[i] = input[index] * (1.0f - frac) + input[index + 1] * frac;\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>***Description**:</p> <p>Signal resampling is the process of changing the sampling rate of a signal. Here, we use the most straightforward linear interpolation method. First, we calculate the approximate position of the new signal in the old signal, and then balance the left and right values to generate the new signal.</p> <p>Features:</p> <ul> <li> <p>Non-integer Factor</p> </li> <li> <p>Linear Interpolation</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input signal array</p> </li> <li> <p><code>input_len</code>: Length of the input signal array</p> </li> <li> <p><code>output</code>: Pointer to the output signal array</p> </li> <li> <p><code>target_len</code>: Target length for the output signal array</p> </li> </ul>"},{"location":"DSP/SIGNAL/RESAMPLE/test/","title":"TESTS","text":""},{"location":"DSP/SIGNAL/RESAMPLE/test/#tiny_resampleh","title":"tiny_resample.h","text":"<pre><code>/**\n * @file tiny_resample_test.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_resample | test | header\n * @version 1.0\n * @date 2025-04-29\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_resample.h\"\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\nvoid tiny_resample_test(void);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"DSP/SIGNAL/RESAMPLE/test/#tiny_resamplec","title":"tiny_resample.c","text":"<pre><code>/**\n * @file tiny_resample_test.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_resample | test | source\n * @version 1.0\n * @date 2025-04-29\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_resample_test.h\" // tiny_resample test header\n\n\nvoid tiny_resample_test(void)\n{\n    printf(\"========== TinyResample Test ==========\\n\");\n\n    const float input[] = {1, 2, 3, 4, 5, 6, 7, 8};\n    const int input_len = sizeof(input) / sizeof(input[0]);\n\n    // Downsample\n    float downsampled[8];\n    int down_len = 0;\n    tiny_downsample_skip_f32(input, input_len, downsampled, &amp;down_len, 1, 2);\n    printf(\"Downsampled (skip 2): \");\n    for (int i = 0; i &lt; down_len; i++) printf(\" %.2f\", downsampled[i]);\n    printf(\"\\n\");\n\n    // Upsample\n    float upsampled[16];\n    tiny_upsample_zero_f32(downsampled, down_len, upsampled, 16);\n    printf(\"Zero-Upsampled:     \");\n    for (int i = 0; i &lt; 16; i++) printf(\" %.2f\", upsampled[i]);\n    printf(\"\\n\");\n\n    // Resample\n    float resampled[12];\n    tiny_resample_f32(input, input_len, resampled, 12);\n    printf(\"Interpolated:       \");\n    for (int i = 0; i &lt; 12; i++) printf(\" %.2f\", resampled[i]);\n    printf(\"\\n\");\n\n    // Validate linear interpolation at midpoints\n    printf(\"Resample validation at midpoints (expected: 1.5, 2.5, ..., 7.5):\\n\");\n    for (int i = 1; i &lt; 12; i += 2) {\n        printf(\"  midpoint[%d] = %.2f\\n\", i, resampled[i]);\n    }\n\n    printf(\"========================================\\n\");\n}\n</code></pre>"},{"location":"DSP/SIGNAL/RESAMPLE/test/#test-output","title":"Test Output","text":"<pre><code>========== TinyResample Test ==========\nDownsampled (skip 2):  1.00 3.00 5.00 7.00\nZero-Upsampled:      1.00 0.00 0.00 0.00 3.00 0.00 0.00 0.00 5.00 0.00 0.00 0.00 7.00 0.00 0.00 0.00\nInterpolated:        1.00 1.67 2.33 3.00 3.67 4.33 5.00 5.67 6.33 7.00 7.67 8.00\nResample validation at midpoints (expected: 1.5, 2.5, ..., 7.5):\n  midpoint[1] = 1.67\n  midpoint[3] = 3.00\n  midpoint[5] = 4.33\n  midpoint[7] = 5.67\n  midpoint[9] = 7.00\n  midpoint[11] = 8.00\n========================================\n</code></pre>"},{"location":"DSP/TRANSFORM/DWT/code/","title":"CODE","text":""},{"location":"DSP/TRANSFORM/DWT/notes/","title":"NOTES","text":""},{"location":"DSP/TRANSFORM/DWT/test/","title":"TESTS","text":""},{"location":"DSP/USAGE/usage/","title":"USAGE INSTRUCTIONS","text":"<p>Usage Instructions</p> <p>This document provides usage instructions for the <code>tiny_dsp</code> module. </p>"},{"location":"DSP/USAGE/usage/#import-tinymath-as-a-whole","title":"Import TinyMath as a Whole","text":"<p>Info</p> <p>Suitable for C projects or projects with a simple structure in C++.</p> <pre><code>#include \"tiny_dsp.h\"\n</code></pre>"},{"location":"DSP/USAGE/usage/#import-tinymath-by-module","title":"Import TinyMath by Module","text":"<p>Info</p> <p>Suitable for projects that require precise control over module imports or complex C++ projects.</p> <pre><code>#include \"tiny_conv.h\" // import convolution module\n#include \"tiny_corr.h\" // import correlation module\n...\n</code></pre> <p>Tip</p> <p>For specific usage methods, please refer to the test code.</p>"},{"location":"MATH/math/","title":"MATH OPERATIONS","text":"<p>Note</p> <p>This component is designed for mathematical operations. It is a lightweight library that provides basic mathematical functions to facilitate onboard computation and AI model inference. The library is designed to be lightweight and efficient, making it suitable for edge computing applications.</p> <p>Note</p> <p>This component is a wrapper and extension of the official ESP32 digital signal processing library ESP-DSP, providing higher-level API interfaces. In simple terms, the TinyMath library corresponds to the Math, Matrix, and DotProduct modules in ESP-DSP, while the other modules in ESP-DSP correspond to the TinyDSP library.</p>"},{"location":"MATH/math/#component-dependencies","title":"COMPONENT DEPENDENCIES","text":"<pre><code>set(src_dirs\n    .\n    vec\n    mat\n)\n\nset(include_dirs\n    .\n    include\n    vec\n    mat\n)\n\nset(requires\n    tiny_toolbox\n)\n\nidf_component_register(SRC_DIRS ${src_dirs} INCLUDE_DIRS ${include_dirs} REQUIRES ${requires})\n</code></pre>"},{"location":"MATH/math/#architecture-and-directory","title":"ARCHITECTURE AND DIRECTORY","text":""},{"location":"MATH/math/#dependency-diagram","title":"Dependency Diagram","text":""},{"location":"MATH/math/#code-tree","title":"Code Tree","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500 CMakeLists.txt\n    \u251c\u2500\u2500 include\n    |   \u251c\u2500\u2500 tiny_error_type.h // error type header file\n    |   \u251c\u2500\u2500 tiny_constant.h // constant header file\n    |   \u251c\u2500\u2500 tiny_math_config.h // configuration header file\n    |   \u2514\u2500\u2500 tiny_math.h // main header file, include this file where you want to use the library\n    \u251c\u2500\u2500 vec\n    |   \u251c\u2500\u2500 tiny_vec.h // vector header file\n    |   \u251c\u2500\u2500 tiny_vec.c // vector source file\n    |   \u251c\u2500\u2500 tiny_vec_test.c // vector test file\n    |   \u2514\u2500\u2500 tiny_vec_test.h // vector test header file\n    \u251c\u2500\u2500 mat\n    |   \u251c\u2500\u2500 tiny_mat.h // matrix header file - c\n    |   \u251c\u2500\u2500 tiny_mat.c // matrix source file - c\n    |   \u251c\u2500\u2500 tiny_mat_test.c // matrix test file - c \n    |   \u251c\u2500\u2500 tiny_mat_test.h // matrix test header file - c\n    |   \u251c\u2500\u2500 tiny_matrix.hpp // matrix header file - cpp\n    |   \u251c\u2500\u2500 tiny_matrix.cpp // matrix source file - cpp\n    |   \u251c\u2500\u2500 tiny_matrix_test.cpp // matrix test file - cpp\n    |   \u2514\u2500\u2500 tiny_matrix_test.hpp // matrix test header file - cpp\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"MATH/ESP-DSP/esp-dsp/","title":"ESP-DSP","text":"<ul> <li> <p> ESP-DSP</p> <p>An Espressif DSP Library (esp-dsp) it\u2019s library of functions, modules and components that provides possibility to use Espressif\u2019s CPUs as DSPs in efficient way.</p> <p>  Online Doc </p> </li> </ul>"},{"location":"MATH/ESP-DSP/esp-dsp/#function-naming","title":"Function Naming","text":"<p>Naming conventions for the Library functions are similar for all covered domains. You can distinguish signal processing functions by the dsps prefix, while image and video processing functions have dspi prefix, and functions that are specific for operations on small matrices have dspm prefix in their names. Function names in Library have the following general format:</p> <pre><code>dsp&lt;data-domain&gt;_&lt;name&gt;_&lt;datatype1&gt;&lt;datatype_ext&gt;_&lt;datatype2&gt;&lt;datatype_ext&gt;[_&lt;descriptor&gt;]&lt;_impl&gt;(&lt;parameters&gt;);\n</code></pre> <p>Where:</p> <ul> <li> <p><code>&lt;data-domain&gt;</code> is the domain of the function, e.g. <code>s</code> for signal processing, <code>i</code> for image processing, <code>v</code> for video processing, and <code>m</code> for small matrix operations.</p> </li> <li> <p><code>&lt;name&gt;</code> is the name of the function.</p> </li> <li> <p><code>&lt;datatype1&gt;</code> is the type of the first input parameter.</p> </li> <li> <p><code>&lt;datatype_ext&gt;</code> is the type of the first input parameter extended with a suffix that indicates the type of the data, e.g. <code>f</code> for float, <code>i</code> for integer, <code>c</code> for complex, etc.</p> </li> <li> <p><code>&lt;datatype2&gt;</code> is the type of the second input parameter.</p> </li> <li> <p><code>&lt;descriptor&gt;</code> is an optional descriptor that provides additional information about the function.</p> </li> <li> <p><code>&lt;impl&gt;</code> is an optional implementation descriptor that provides additional information about the implementation of the function.</p> </li> <li> <p><code>&lt;parameters&gt;</code> are the parameters of the function.</p> </li> </ul>"},{"location":"MATH/ESP-DSP/esp-dsp/#data-domain","title":"Data Domain","text":"<p>The data-domain is a single character that expresses the subset of functionality to which a given function belongs. The Library designed to supports the following data-domains:</p> <ul> <li> <p>s - for signals (expected data type is a 1D signal)</p> </li> <li> <p>i - for images and video (expected data type is a 2D image)</p> </li> <li> <p>m - for matrices (expected data type is a matrix)</p> </li> <li> <p>r - for realistic rendering functionality and 3D data processing (expected data type depends on supported rendering techniques)</p> </li> <li> <p>q - for signals of fixed length</p> </li> </ul> <p>For example, function names that begin with dspi signify that respective functions are used for image or video processing.</p>"},{"location":"MATH/ESP-DSP/esp-dsp/#name","title":"Name","text":"<p>The name is an abbreviation for the core operation that the function really does, for example Add, Sqrt, followed in some cases by a function-specific modifier: = [_modifier]</p> <p>This modifier, if present, denotes a slight modification or variation of the given function.</p>"},{"location":"MATH/ESP-DSP/esp-dsp/#data-types","title":"Data Types","text":"<p>The library supports two main data types \u2013 int16 for fixed point arithmetic and float for floating point arithmetic. The datatype described as:</p>"},{"location":"MATH/ESP-DSP/esp-dsp/#data-type-suffices","title":"Data type suffices:","text":"<ul> <li> <p>s - signed</p> </li> <li> <p>u - unsigned</p> </li> <li> <p>f - float</p> </li> </ul>"},{"location":"MATH/ESP-DSP/esp-dsp/#data-type-extensions","title":"Data type extensions:","text":"<ul> <li>c - complex</li> </ul>"},{"location":"MATH/ESP-DSP/esp-dsp/#data-type-bits-resolution","title":"Data type Bits resolution:","text":"<ul> <li> <p>16</p> </li> <li> <p>32</p> </li> </ul> <p>For example: dsps_mac_sc16 defines that mac operation with 1d array will be made with 16 bit signed complex data.</p>"},{"location":"MATH/ESP-DSP/esp-dsp/#implementation-type","title":"Implementation Type","text":"<p>Each function could be implemented different for different platform and could use different style and resources. That\u2019s why every implemented function will have name extension &lt;_impl&gt; that will define which kind of implementation it is. User can use universal function without extension.</p>"},{"location":"MATH/ESP-DSP/esp-dsp/#implementation-extensions","title":"Implementation extensions:","text":"<p>By default all functions could be used without extensions. The option that select optimized/ansi can be chosen in menuconfig.</p> <p>Inside library the extensions means:</p> <ul> <li> <p>_ansi - a universal function where body of function implemented on ANSI C. This implementation not includes any hardware optimization</p> </li> <li> <p>_ae32 - written on ESP32 assembler and optimized for ESP32</p> </li> <li> <p>_aes3 - written on ESP32S3 assembler and optimized for ESP32S3</p> </li> <li> <p>_arp4 - written on ESP32P4 assembler and optimized for ESP32P4</p> </li> <li> <p>_platform - header file with definitions of available CPUs instructions for different functions</p> </li> <li> <p>others- depends on amount of supported CPUs. This list will be extended in future</p> </li> </ul>"},{"location":"MATH/ESP-DSP/examples/","title":"ESP-DSP EXAMPLES","text":""},{"location":"MATH/ESP-DSP/examples/#list-of-esp-dsp-examples","title":"List of esp-dsp Examples","text":"<p>Signal processing APIs use dsps prefix. The following modules are available:</p> <ul> <li> <p>Basic math - the example shows how to use basic vector math operations</p> </li> <li> <p>Dot-product - the example demonstrates how to use dot product functions</p> </li> <li> <p>FFT - the example demonstrates how to use FFT functionality</p> </li> <li> <p>FFT Window - the example demonstrates how to use Window and FFT functionality</p> </li> <li> <p>FFT 4 Real - the example demonstrates how to use FFT functionality for real input signals</p> </li> <li> <p>IIR - the example demonstrates how to use IIR filters functionality</p> </li> <li> <p>FIR - the example demonstrates how to use FIR filter functionality</p> </li> <li> <p>Kalman Filter - Extended Kalman Filter (EKF) example</p> </li> <li> <p>Matrix - example demonstrates how to use Mat class functionality</p> </li> </ul>"},{"location":"MATH/ESP-DSP/examples/#basic-math","title":"Basic math","text":"<p>This example demonstrates how to use basic math functions from esp-dsp library. The example does the following steps:</p> <ul> <li> <p>Initialize the library</p> </li> <li> <p>Initialize input signals with 1024 samples</p> </li> <li> <p>Apply window to input signal by standard C loop.</p> </li> <li> <p>Calculate FFT for 1024 complex samples and show the result</p> </li> <li> <p>Show results on the plots</p> </li> <li> <p>Apply window to input signal by basic math functions dsps_mul_f32 and dsps_mulc_f32.</p> </li> <li> <p>Calculate FFT for 1024 complex samples</p> </li> <li> <p>Show results on the plots</p> </li> </ul> <p>For more details please look to the examples/basic_math/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#dot-product","title":"Dot-product","text":"<p>The example demonstrates how to use dotprod dsps_dotprod_f32 from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the input arrays</p> </li> <li> <p>Calculate dot product of two arrays</p> </li> <li> <p>Compare results and calculate execution time in cycles.</p> </li> </ul> <p>For more details please look to the examples/dotprod/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#fft","title":"FFT","text":"<p>This example demonstrates how to use FFT functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the library</p> </li> <li> <p>Initialize input signals with 1024 samples: one 0 dB, second with -20 dB</p> </li> <li> <p>Combine two signals as one complex input signal and apply window to input signals paar.</p> </li> <li> <p>Calculate FFT for 1024 complex samples</p> </li> <li> <p>Apply bit reverse operation for output complex vector</p> </li> <li> <p>Split one complex FFT output spectrum to two real signal spectrums</p> </li> <li> <p>Show results on the plots</p> </li> <li> <p>Show execution time of FFT</p> </li> </ul> <p>For more details please look to the examples/fft/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#fft-window","title":"FFT Window","text":"<p>This example demonstrates how to use Window and FFT functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the library</p> </li> <li> <p>Initialize input signals with 1024 samples</p> </li> <li> <p>Apply window to input signal.</p> </li> <li> <p>Calculate FFT for 1024 complex samples</p> </li> <li> <p>Apply bit reverse operation for output complex vector</p> </li> <li> <p>Split one complex FFT output spectrum to two real signal spectrums</p> </li> <li> <p>Show results on the plots</p> </li> </ul> <p>For more details please look to the examples/fft_window/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#fft-4-real","title":"FFT 4 Real","text":"<p>This example demonstrates how to use FFT functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the library</p> </li> <li> <p>Initialize input signals with 1024 samples: one 0 dB, second with -20 dB</p> </li> <li> <p>Calculate FFT Radix-2 for 1024 complex samples</p> </li> <li> <p>Calculate FFT Radix-4 for 1024 complex samples</p> </li> <li> <p>Apply bit reverse operation for output complex vectors</p> </li> <li> <p>Show results on the plots</p> </li> <li> <p>Show execution time of FFTs</p> </li> </ul> <p>For more details please look to the examples/fft4real/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#iir","title":"IIR","text":"<p>This example demonstrates how to use IIR filters functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the library</p> </li> <li> <p>Initialize input signal</p> </li> <li> <p>Show LPF filter with Q factor 1</p> <ul> <li> <p>Calculate iir filter coefficients</p> </li> <li> <p>Filter the input test signal (delta function)</p> </li> <li> <p>Shows impulse response on the plot</p> </li> <li> <p>Shows frequency response on the plot</p> </li> </ul> </li> <li> <p>Calculate execution performance</p> </li> <li> <p>The same for LPF filter with Q factor 10</p> </li> </ul> <p>For more details please look to the examples/fir/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#fir","title":"FIR","text":"<p>This example demonstrates how to use FIR filter functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize the FFT library</p> </li> <li> <p>Initialize input signal</p> </li> <li> <p>Show input signal</p> </li> <li> <p>Show filtered signal</p> </li> </ul> <p>For more details please look to the examples/fir/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#kalman-filter","title":"Kalman Filter","text":"<p>This example emulate system with IMU sensors and show how to use Extended Kalman Filter (EKF), with 13 values states vector, to estimate gyroscope errors and calculate system attitude. Also, this example show how to use esp-dsp library to operate with matrices and vectors.</p> <p>In real system, the emulated sensors values should be replace by the real sensors values. Then, in real system, a calibration phase should be implemented and after the calibration phase the state vector X and covariance matrix P should be saved and restored next time, when filter called. It will save time for initial phase.</p> <p>For more details please look to the examples/kalman/README.md</p>"},{"location":"MATH/ESP-DSP/examples/#matrix","title":"Matrix","text":"<p>This example demonstrates how to use Mat class functionality from esp-dsp library. Example does the following steps:</p> <ul> <li> <p>Initialize a matrix A and matirx x</p> </li> <li> <p>Calculate matrix b: b = A*x</p> </li> <li> <p>Find roots x1: A*x1 = b, with different methods</p> </li> <li> <p>Print result</p> </li> </ul>"},{"location":"MATH/HEADER-FILE/tiny_constants/","title":"CONSTANTS","text":"<p>Info</p> <p>This file contains the definition of some constants, which are used for upper-level calculations and applications. The documentation update speed is slow and may not be consistent with the actual code. Please refer to the code for accuracy.</p> <pre><code>/**\n * @file tiny_constants.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file contains the constants used in the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n// =======================================\n//  Logical Constants\n// =======================================\n#ifndef TRUE\n#define TRUE 1\n#endif\n\n#ifndef FALSE\n#define FALSE 0\n#endif\n\n#ifndef NULL\n#define NULL ((void *)0)\n#endif\n\n// =======================================\n//  Math Constants (float/double safe)\n// =======================================\n#define TINY_PI 3.14159265358979323846f\n#define TINY_TWO_PI 6.28318530717958647692f\n#define TINY_HALF_PI 1.57079632679489661923f\n#define TINY_E 2.71828182845904523536f\n#define TINY_SQRT2 1.41421356237309504880f\n#define TINY_INV_SQRT2 0.70710678118654752440f\n\n#define TINY_DEG2RAD(x) ((x) * TINY_PI / 180.0f)\n#define TINY_RAD2DEG(x) ((x) * 180.0f / TINY_PI)\n\n// =======================================\n//  Bitmask &amp; Bit Manipulation\n// =======================================\n\n// Bitwise operations\n#define TINY_BIT(n) (1U &lt;&lt; (n)) // e.g. TINY_BIT(3) = 0b00001000\n#define TINY_BIT_SET(x, n) ((x) |= TINY_BIT(n))\n#define TINY_BIT_CLEAR(x, n) ((x) &amp;= ~TINY_BIT(n))\n#define TINY_BIT_TOGGLE(x, n) ((x) ^= TINY_BIT(n))\n#define TINY_BIT_CHECK(x, n) (((x) &gt;&gt; (n)) &amp; 0x1U)\n\n// Common bit masks\n#define TINY_MASK_4BIT 0x0FU\n#define TINY_MASK_8BIT 0xFFU\n#define TINY_MASK_16BIT 0xFFFFU\n#define TINY_MASK_32BIT 0xFFFFFFFFU\n\n// =======================================\n//  Fixed-Point Scaling Factors\n// =======================================\n#define TINY_Q7_SCALE 128          // 2^7\n#define TINY_Q15_SCALE 32768       // 2^15\n#define TINY_Q31_SCALE 2147483648U // 2^31\n\n// =======================================\n//  User-Defined Constants (Optional)\n// =======================================\n#define TINY_MATH_MIN_DENOMINATOR 1e-6f         // Minimum denominator for safe division\n#define TINY_MATH_MIN_POSITIVE_INPUT_F32 1e-12f // Minimum positive input for float operations\n#define TINY_MATH_LARGE_VALUE_F32 1e38f         // Large value used to represent infinity-like results (safe for IEEE 754 float, max ~3.4e38)\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/HEADER-FILE/tiny_error_type/","title":"ERROR TYPES DEFINITION","text":"<p>Info</p> <p>This file defines some common error types in calculations to assist in determining the cause of errors. The documentation update speed is slow and may not match the actual code, please refer to the code for accuracy.</p> <pre><code>/**\n * @file tiny_error_type.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /* TYPE DEFINITIONS */\n    typedef int tiny_error_t; // Error type for the tiny_math middleware\n\n/* MACROS */\n/* Definitions for error constants. */\n#define TINY_OK 0    /*!&lt; tiny_err_t value indicating success (no error) */\n#define TINY_FAIL -1 /*!&lt; Generic tiny_err_t code indicating failure */\n\n#define TINY_ERR_NO_MEM 0x101           /*!&lt; Out of memory */\n#define TINY_ERR_INVALID_ARG 0x102      /*!&lt; Invalid argument */\n#define TINY_ERR_INVALID_STATE 0x103    /*!&lt; Invalid state */\n#define TINY_ERR_INVALID_SIZE 0x104     /*!&lt; Invalid size */\n#define TINY_ERR_NOT_FOUND 0x105        /*!&lt; Requested resource not found */\n#define TINY_ERR_NOT_SUPPORTED 0x106    /*!&lt; Operation or feature not supported */\n#define TINY_ERR_TIMEOUT 0x107          /*!&lt; Operation timed out */\n#define TINY_ERR_INVALID_RESPONSE 0x108 /*!&lt; Received response was invalid */\n#define TINY_ERR_INVALID_CRC 0x109      /*!&lt; CRC or checksum was invalid */\n#define TINY_ERR_INVALID_VERSION 0x10A  /*!&lt; Version was invalid */\n#define TINY_ERR_INVALID_MAC 0x10B      /*!&lt; MAC address was invalid */\n#define TINY_ERR_NOT_FINISHED 0x10C     /*!&lt; Operation has not fully completed */\n#define TINY_ERR_NOT_ALLOWED 0x10D      /*!&lt; Operation is not allowed */\n\n#define TINY_ERR_WIFI_BASE 0x3000      /*!&lt; Starting number of WiFi error codes */\n#define TINY_ERR_MESH_BASE 0x4000      /*!&lt; Starting number of MESH error codes */\n#define TINY_ERR_FLASH_BASE 0x6000     /*!&lt; Starting number of flash error codes */\n#define TINY_ERR_HW_CRYPTO_BASE 0xc000 /*!&lt; Starting number of HW cryptography module error codes */\n#define TINY_ERR_MEMPROT_BASE 0xd000   /*!&lt; Starting number of Memory Protection API error codes */\n\n#define TINY_ERR_MATH_BASE 0x70000\n#define TINY_ERR_MATH_INVALID_LENGTH (TINY_ERR_MATH_BASE + 1)\n#define TINY_ERR_MATH_INVALID_PARAM (TINY_ERR_MATH_BASE + 2)\n#define TINY_ERR_MATH_PARAM_OUTOFRANGE (TINY_ERR_MATH_BASE + 3)\n#define TINY_ERR_MATH_UNINITIALIZED (TINY_ERR_MATH_BASE + 4)\n#define TINY_ERR_MATH_REINITIALIZED (TINY_ERR_MATH_BASE + 5)\n#define TINY_ERR_MATH_ARRAY_NOT_ALIGNED (TINY_ERR_MATH_BASE + 6)\n#define TINY_ERR_MATH_NULL_POINTER (TINY_ERR_MATH_BASE + 7)\n#define TINY_ERR_MATH_ZERO_DIVISION (TINY_ERR_MATH_BASE + 8)\n#define TINY_ERR_MATH_NEGATIVE_SQRT (TINY_ERR_MATH_BASE + 9)\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/HEADER-FILE/tiny_math/","title":"TinyMath HEADER FILE","text":"<p>Info</p> <p>This is the main header file of the TinyMath library. It includes all necessary header files and provides a unified interface to use the functions of the library. After completing the porting of this library in the project, you can insert this header file where you want to use the relevant functions to use all functions in the library. The documentation update speed is slow and may not be consistent with the actual code, please refer to the actual code.</p> <pre><code>/**\n * @file tiny_math.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-03-26\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n\n// this layer\n#include \"tiny_math_config.h\"\n\n/* SUBMODULES */\n\n// vector operations\n#include \"tiny_vec.h\"\n\n// matrix operations\n#include \"tiny_mat.h\"\n\n// advanced matrix operations\n#ifdef __cplusplus\n\n#include \"tiny_matrix.hpp\"\n\n#endif\n\n/* TEST */ // NOTE: test files are platform specific and should not be included in the library\n\n// vector operations\n#include \"tiny_vec_test.h\"\n\n// matrix operations\n#include \"tiny_mat_test.h\"\n\n// advanced matrix operations\n#ifdef __cplusplus\n\n#include \"tiny_matrix_test.hpp\"\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/HEADER-FILE/tiny_math_config/","title":"TinyMath CONFIGURATION","text":"<p>Info</p> <p>This header file serves to configure the entire TinyMath module, and each submodule includes this header file. It defines the configuration options and macros for TinyMath, allowing users to customize settings as needed. By modifying the configuration options in this header file, users can easily adjust the behavior and functionality of TinyMath to meet specific requirements. The documentation may be updated slowly and may not match the actual code, so please refer to the code for accuracy.</p> <p>Tip</p> <p>This component includes macro definitions for selecting platforms, allowing users to choose different platforms for compilation as needed. By switching to the corresponding platform macro, users can leverage platform acceleration features to enhance performance. For example, for the ESP32 platform, TinyMath will automatically select the ESP32 DSP library for compilation, achieving more efficient mathematical operations.</p> <pre><code>/**\n * @file tiny_math_config.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-14\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* DEPENDENCIES */\n\n// ANSI C\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdint.h&gt;\n\n// lower level\n#include \"tiny_toolbox.h\"\n\n// this level\n#include \"tiny_error_type.h\"\n#include \"tiny_constants.h\"\n\n/* PLATFORM SELECTION */\n\n// available platforms\n#define MCU_PLATFORM_GENERIC 0\n#define MCU_PLATFORM_ESP32 1 // here, we utilize the ESP built-in DSP library, it will automatically select the optimized version\n#define MCU_PLATFORM_STM32 2\n#define MCU_PLATFORM_RISCV 3\n\n// choose one platform\n#define MCU_PLATFORM_SELECTED MCU_PLATFORM_ESP32\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-api/","title":"MATRIX OPERATIONS - TINY_MAT","text":"<p>About tiny_mat library</p> <p>tiny_mat is a C implementation of a matrix library that provides basic matrix operation functions. It supports operations such as addition, subtraction, and multiplication of floating-point matrices. This library is suitable for embedded systems and real-time applications that require matrix calculations. The library is based on the ANSI C standard, ensuring good portability and performance, while also supporting platform acceleration through configuration files (ESP32).</p> <p>About the usage of tiny_mat library</p> <p>The functionality of tiny_mat is completely covered by tiny_matrix, which means that the functions in tiny_matrix include all the functions of tiny_mat. For simple matrix operations, you can only include the tiny_mat library; for complex matrix operations, it is recommended to use the tiny_matrix library. The tiny_matrix library is a C++ implementation of a matrix library that provides richer functionality and better performance. It supports operations such as addition, subtraction, multiplication, transposition, and inversion of floating-point and integer matrices.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#list-of-functions","title":"LIST OF FUNCTIONS","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500Vector\n    \u2514\u2500\u2500Matrix\n        \u251c\u2500\u2500 tiny_mat (c) &lt;---\n        \u2514\u2500\u2500 tiny_matrix (c++)\n</code></pre> <pre><code>// print matrix\nvoid print_matrix(const char *name, const float *mat, int rows, int cols);\n// print matrix padded (row-major)\nvoid print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step);\n// addition\ntiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// subtraction\ntiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// multiplication\ntiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k);\ntiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding);\ntiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-api/#utility-functions","title":"UTILITY FUNCTIONS","text":""},{"location":"MATH/MATRIX/tiny-mat-api/#print-matrix","title":"Print Matrix","text":"<p><pre><code>void print_matrix(const char *name, const float *mat, int rows, int cols);\n</code></pre> Function: Print a matrix in row-major order.</p> <p>Parameters:</p> <ul> <li> <p><code>name</code>: Name of the matrix.</p> </li> <li> <p><code>mat</code>: Pointer to the matrix data.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrix.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrix.</p> </li> </ul> <p>Returns: None.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#print-padded-matrix","title":"Print Padded Matrix","text":"<p><pre><code>void print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step);\n</code></pre> Function: Print a matrix in row-major order with padding.</p> <p>Parameters: - <code>name</code>: Name of the matrix.</p> <ul> <li> <p><code>mat</code>: Pointer to the matrix data.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrix.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrix.</p> </li> <li> <p><code>step</code>: Step size for the matrix data.</p> </li> </ul> <p>Returns: None.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#addition","title":"ADDITION","text":""},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-addition","title":"Matrix Addition","text":"<pre><code>tiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\n</code></pre> <p>Function: Add two matrices.</p> <p>Parameters:</p> <ul> <li> <p><code>input1</code>: Pointer to the first input matrix.</p> </li> <li> <p><code>input2</code>: Pointer to the second input matrix.</p> </li> <li> <p><code>output</code>: Pointer to the output matrix.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrices.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrices.</p> </li> <li> <p><code>padd1</code>: Padding for the first input matrix.</p> </li> <li> <p><code>padd2</code>: Padding for the second input matrix.</p> </li> <li> <p><code>padd_out</code>: Padding for the output matrix.</p> </li> <li> <p><code>step1</code>: Step size for the first input matrix.</p> </li> <li> <p><code>step2</code>: Step size for the second input matrix.</p> </li> <li> <p><code>step_out</code>: Step size for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-addition-with-constant","title":"Matrix Addition with Constant","text":"<pre><code>tiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>Function: Add a constant to a matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input matrix.</p> </li> <li> <p><code>output</code>: Pointer to the output matrix.</p> </li> <li> <p><code>C</code>: Constant to add.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrix.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrix.</p> </li> <li> <p><code>padd_in</code>: Padding for the input matrix.</p> </li> <li> <p><code>padd_out</code>: Padding for the output matrix.</p> </li> <li> <p><code>step_in</code>: Step size for the input matrix.</p> </li> <li> <p><code>step_out</code>: Step size for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#subtraction","title":"SUBTRACTION","text":""},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-subtraction","title":"Matrix Subtraction","text":"<pre><code>tiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\n</code></pre> <p>Function: Subtract two matrices.</p> <p>Parameters:</p> <ul> <li> <p><code>input1</code>: Pointer to the first input matrix.</p> </li> <li> <p><code>input2</code>: Pointer to the second input matrix.</p> </li> <li> <p><code>output</code>: Pointer to the output matrix.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrices.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrices.</p> </li> <li> <p><code>padd1</code>: Padding for the first input matrix.</p> </li> <li> <p><code>padd2</code>: Padding for the second input matrix.</p> </li> <li> <p><code>padd_out</code>: Padding for the output matrix.</p> </li> <li> <p><code>step1</code>: Step size for the first input matrix.</p> </li> <li> <p><code>step2</code>: Step size for the second input matrix.</p> </li> <li> <p><code>step_out</code>: Step size for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-subtraction-with-constant","title":"Matrix Subtraction with Constant","text":"<pre><code>tiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>Function: Subtract a constant from a matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input matrix.</p> </li> <li> <p><code>output</code>: Pointer to the output matrix.</p> </li> <li> <p><code>C</code>: Constant to subtract.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrix.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrix.</p> </li> <li> <p><code>padd_in</code>: Padding for the input matrix.</p> </li> <li> <p><code>padd_out</code>: Padding for the output matrix.</p> </li> <li> <p><code>step_in</code>: Step size for the input matrix.</p> </li> <li> <p><code>step_out</code>: Step size for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#multiplication","title":"MULTIPLICATION","text":""},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-multiplication","title":"Matrix Multiplication","text":"<pre><code>tiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k);\n</code></pre> <p>Function: Multiply two matrices.</p> <p>Parameters:</p> <ul> <li> <p><code>A</code>: Pointer to the first input matrix.</p> </li> <li> <p><code>B</code>: Pointer to the second input matrix.</p> </li> <li> <p><code>C</code>: Pointer to the output matrix.</p> </li> <li> <p><code>m</code>: Number of rows in the first matrix.</p> </li> <li> <p><code>n</code>: Number of columns in the first matrix (and rows in the second matrix).</p> </li> <li> <p><code>k</code>: Number of columns in the second matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#extended-matrix-multiplication","title":"Extended Matrix Multiplication","text":"<pre><code>tiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding);\n</code></pre> <p>Function: Multiply two matrices with extended parameters.</p> <p>Parameters:</p> <ul> <li> <p><code>A</code>: Pointer to the first input matrix.</p> </li> <li> <p><code>B</code>: Pointer to the second input matrix.</p> </li> <li> <p><code>C</code>: Pointer to the output matrix.</p> </li> <li> <p><code>A_rows</code>: Number of rows in the first matrix.</p> </li> <li> <p><code>A_cols</code>: Number of columns in the first matrix.</p> </li> <li> <p><code>B_cols</code>: Number of columns in the second matrix.</p> </li> <li> <p><code>A_padding</code>: Padding for the first matrix.</p> </li> <li> <p><code>B_padding</code>: Padding for the second matrix.</p> </li> <li> <p><code>C_padding</code>: Padding for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure.</p>"},{"location":"MATH/MATRIX/tiny-mat-api/#matrix-multiplication-with-constant","title":"Matrix Multiplication with Constant","text":"<pre><code>tiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>Function: Multiply a matrix by a constant.</p> <p>Parameters:</p> <ul> <li> <p><code>input</code>: Pointer to the input matrix.</p> </li> <li> <p><code>output</code>: Pointer to the output matrix.</p> </li> <li> <p><code>C</code>: Constant to multiply.</p> </li> <li> <p><code>rows</code>: Number of rows in the matrix.</p> </li> <li> <p><code>cols</code>: Number of columns in the matrix.</p> </li> <li> <p><code>padd_in</code>: Padding for the input matrix.</p> </li> <li> <p><code>padd_out</code>: Padding for the output matrix.</p> </li> <li> <p><code>step_in</code>: Step size for the input matrix.</p> </li> <li> <p><code>step_out</code>: Step size for the output matrix.</p> </li> </ul> <p>Returns: <code>tiny_error_t</code> indicating success or failure. ```</p>"},{"location":"MATH/MATRIX/tiny-mat-code/","title":"CODE","text":""},{"location":"MATH/MATRIX/tiny-mat-code/#tiny_math","title":"tiny_mat.h","text":"<pre><code>/**\n * @file tiny_mat.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the submodule mat (basic matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_math_config.h\"\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n// ESP32 DSP library\n#include \"dspm_matrix.h\"\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* FUNCTION PROTOTYPES */\n// print matrix\nvoid print_matrix(const char *name, const float *mat, int rows, int cols);\n// print matrix padded (row-major)\nvoid print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step);\n// addition\ntiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// subtraction\ntiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// multiplication\ntiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k);\ntiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding);\ntiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-code/#tiny_matc","title":"tiny_mat.c","text":"<pre><code>/**\n * @file tiny_mat.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the source file for the submodule mat (basic matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_mat.h\"\n\n/* SUPPORTIVE FUNCTIONS */\n\n/**\n * @name print_matrix\n * @brief Prints a matrix to the console.\n * @param name Name of the matrix.\n * @param mat Pointer to the matrix data.\n * @param rows Number of rows in the matrix.\n * @param cols Number of columns in the matrix.\n */\nvoid print_matrix(const char *name, const float *mat, int rows, int cols)\n{\n    printf(\"%s =\\n\\r\", name);\n    for (int i = 0; i &lt; rows; i++)\n    {\n        for (int j = 0; j &lt; cols; j++)\n        {\n            printf(\"%10.6f \", mat[i * cols + j]); // padding not considered, row-major order\n        }\n        printf(\"\\n\\r\");\n    }\n    printf(\"\\n\\r\");\n}\n\n// print matrix padded\n/**\n * @name print_matrix\n * @brief Prints a matrix to the console.\n * @param name Name of the matrix.\n * @param mat Pointer to the matrix data.\n * @param rows Number of rows in the matrix.\n * @param cols Number of columns in the matrix.\n * @param step Step size (how many elements in a row) for the matrix data. row-major order.\n */\nvoid print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step)\n{\n    printf(\"%s =\\n\\r\", name);\n    for (int i = 0; i &lt; rows; i++)\n    {\n        for (int j = 0; j &lt; cols; j++)\n        {\n            printf(\"%10.6f \", mat[i * step + j]); // padding considered\n        }\n        printf(\"\\n\\r\");\n    }\n    printf(\"\\n\\r\");\n}\n\n/* ADDITION */\n\n// matrix + matrix | float\n\n/**\n * @name tiny_mat_add_f32\n * @brief Adds two matrices of type float32.\n * @param input1 Pointer to the first input matrix.\n * @param input2 Pointer to the second input matrix.\n * @param output Pointer to the output matrix.\n * @param rows Number of rows in the matrices.\n * @param cols Number of columns in the matrices.\n * @param padd1 Number of padding columns in the first input matrix.\n * @param padd2 Number of padding columns in the second input matrix.\n * @param padd_out Number of padding columns in the output matrix.\n * @param step1 Step size for the first input matrix.\n * @param step2 Step size for the second input matrix.\n * @param step_out Step size for the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix addition with the specified padding and step sizes.\n * @note The function assumes that the input matrices are in row-major order.\n */\ntiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    // paddings must be non-negative, steps must be at least 1.\n    if (rows &lt;= 0 || cols &lt;= 0 || padd1 &lt; 0 || padd2 &lt; 0 || padd_out &lt; 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n\n    // pad refers to the columns that are not used in the matrix operation\n\n    /* Use explicit index math instead of mutating the caller pointers.\n       This keeps input pointers const and avoids surprises from pointer\n       arithmetic. The storage model is row-major with per-row reserved\n       length = cols + padd. Logical column c is at base + c * step. */\n    const int in1_row_stride = cols + padd1;\n    const int in2_row_stride = cols + padd2;\n    const int out_row_stride = cols + padd_out;\n\n    // If we're on ESP32 and all paddings are 0 and all steps are 1 (contiguous),\n    // prefer to call the optimized ESP-DSP implementation.\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padd1 == 0 &amp;&amp; padd2 == 0 &amp;&amp; padd_out == 0 &amp;&amp; step1 == 1 &amp;&amp; step2 == 1 &amp;&amp; step_out == 1) {\n        dspm_add_f32(input1, input2, output, rows, cols, 0, 0, 0, 1, 1, 1);\n        return TINY_OK;\n    }\n#endif\n\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in1 = row * in1_row_stride;\n        int base_in2 = row * in2_row_stride;\n        int base_out = row * out_row_stride;\n\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in1 = base_in1 + col * step1;\n            int idx_in2 = base_in2 + col * step2;\n            int idx_out = base_out + col * step_out;\n\n            /* bounds are the caller's responsibility, but avoid undefined\n               behavior by checking indices minimally in debug builds if\n               needed (not enforced here for performance). */\n            output[idx_out] = input1[idx_in1] + input2[idx_in2];\n        }\n    }\n    return TINY_OK;\n}\n\n// matrix + constant | float\n\n/**\n * @name tiny_mat_addc_f32\n * @brief Adds a constant to each element of a matrix of type float32.\n * @param input Pointer to the input matrix.\n * @param output Pointer to the output matrix.\n * @param C Constant value to be added to each element of the matrix.\n * @param rows Number of rows in the matrices.\n * @param cols Number of columns in the matrices.\n * @param padd_in Number of padding columns in the input matrix.\n * @param padd_out Number of padding columns in the output matrix.\n * @param step_in Step size for the input matrix.\n * @param step_out Step size for the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix addition with a constant with the specified padding and step sizes.\n * @note The function assumes that the input matrix is in row-major order.\n */\ntiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    // paddings must be non-negative, steps must be at least 1.\n    if (rows &lt;= 0 || cols &lt;= 0 || padd_in &lt; 0 || padd_out &lt; 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    // pad refers to the columns that are not used in the matrix operation\n    // If running on ESP32 and all paddings are 0 and all steps are 1 (contiguous),\n    // prefer the optimized ESP-DSP implementation.\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padd_in == 0 &amp;&amp; padd_out == 0 &amp;&amp; step_in == 1 &amp;&amp; step_out == 1) {\n        dspm_addc_f32(input, output, C, rows, cols, 0, 0, 1, 1);\n        return TINY_OK;\n    }\n#endif\n\n    const int in_row_stride = cols + padd_in;\n    const int out_row_stride = cols + padd_out;\n\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in = row * in_row_stride;\n        int base_out = row * out_row_stride;\n\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = base_in + col * step_in;\n            int idx_out = base_out + col * step_out;\n\n            output[idx_out] = input[idx_in] + C;\n        }\n    }\n    return TINY_OK;\n}\n\n/* SUBTRACTION */\n\n// matrix - matrix | float\n\n/**\n * @name tiny_mat_sub_f32\n * @brief Subtracts two matrices of type float32.\n * @param input1 Pointer to the first input matrix.\n * @param input2 Pointer to the second input matrix.\n * @param output Pointer to the output matrix.\n * @param rows Number of rows in the matrices.\n * @param cols Number of columns in the matrices.\n * @param padd1 Number of padding columns in the first input matrix.\n * @param padd2 Number of padding columns in the second input matrix.\n * @param padd_out Number of padding columns in the output matrix.\n * @param step1 Step size for the first input matrix.\n * @param step2 Step size for the second input matrix.\n * @param step_out Step size for the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix subtraction with the specified padding and step sizes.\n * @note The function assumes that the input matrices are in row-major order.\n */\ntiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    // paddings must be non-negative, steps must be at least 1.\n    if (rows &lt;= 0 || cols &lt;= 0 || padd1 &lt; 0 || padd2 &lt; 0 || padd_out &lt; 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    // pad refers to the columns that are not used in the matrix operation\n    // Prefer ESP-DSP only when all paddings are 0 and all steps are 1 (contiguous)\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padd1 == 0 &amp;&amp; padd2 == 0 &amp;&amp; padd_out == 0 &amp;&amp; step1 == 1 &amp;&amp; step2 == 1 &amp;&amp; step_out == 1) {\n        dspm_sub_f32(input1, input2, output, rows, cols, 0, 0, 0, 1, 1, 1);\n        return TINY_OK;\n    }\n#endif\n\n    const int in1_row_stride = cols + padd1;\n    const int in2_row_stride = cols + padd2;\n    const int out_row_stride = cols + padd_out;\n\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in1 = row * in1_row_stride;\n        int base_in2 = row * in2_row_stride;\n        int base_out = row * out_row_stride;\n\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in1 = base_in1 + col * step1;\n            int idx_in2 = base_in2 + col * step2;\n            int idx_out = base_out + col * step_out;\n\n            output[idx_out] = input1[idx_in1] - input2[idx_in2];\n        }\n    }\n    return TINY_OK;\n}\n\n// matrix - constant | float\n\n/**\n * @name tiny_mat_subc_f32\n * @brief Subtracts a constant from each element of a matrix of type float32.\n * @param input Pointer to the input matrix.\n * @param output Pointer to the output matrix.\n * @param C Constant value to be subtracted from each element of the matrix.\n * @param rows Number of rows in the matrices.\n * @param cols Number of columns in the matrices.\n * @param padd_in Number of padding columns in the input matrix.\n * @param padd_out Number of padding columns in the output matrix.\n * @param step_in Step size for the input matrix.\n * @param step_out Step size for the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix subtraction with a constant with the specified padding and step sizes.\n * @note The function assumes that the input matrix is in row-major order.\n */\ntiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    // paddings must be non-negative, steps must be at least 1.\n    if (rows &lt;= 0 || cols &lt;= 0 || padd_in &lt; 0 || padd_out &lt; 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    // pad refers to the columns that are not used in the matrix operation\n    // Prefer ESP-DSP only when all paddings are 0 and all steps are 1 (contiguous)\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padd_in == 0 &amp;&amp; padd_out == 0 &amp;&amp; step_in == 1 &amp;&amp; step_out == 1) {\n        // dspm_addc_f32 performs addition; pass -C to implement subtraction-constant\n        dspm_addc_f32(input, output, -C, rows, cols, 0, 0, 1, 1);\n        return TINY_OK;\n    }\n#endif\n\n    const int in_row_stride = cols + padd_in;\n    const int out_row_stride = cols + padd_out;\n\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in = row * in_row_stride;\n        int base_out = row * out_row_stride;\n\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = base_in + col * step_in;\n            int idx_out = base_out + col * step_out;\n\n            output[idx_out] = input[idx_in] - C;\n        }\n    }\n    return TINY_OK;\n}\n\n/* MULTIPLICATION */\n\n// matrix * matrix | float\n\n/**\n * @name tiny_mat_mult_f32\n * @brief Multiplies two matrices of type float32.\n * @param A Pointer to the first input matrix.\n * @param B Pointer to the second input matrix.\n * @param C Pointer to the output matrix.\n * @param m Number of rows in the first matrix.\n * @param n Number of columns in the first matrix and rows in the second matrix.\n * @param k Number of columns in the second matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix multiplication with the specified padding and step sizes.\n * @note The function assumes that the input matrices are in row-major order.\n */\ntiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k)\n{\n    if (NULL == A || NULL == B || NULL == C)\n        return TINY_ERR_MATH_NULL_POINTER;\n    if (m &lt;= 0 || n &lt;= 0 || k &lt;= 0)\n        return TINY_ERR_MATH_INVALID_PARAM;\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized matrix multiplication\n    dspm_mult_f32(A, B, C, m, n, k);\n#else\n    // C[i][j] = sum_{s=0}^{n-1} A[i][s] * B[s][j]\n    for (int i = 0; i &lt; m; i++)\n    {\n        for (int j = 0; j &lt; k; j++)\n        {\n            C[i * k + j] = 0.0f;\n            for (int s = 0; s &lt; n; s++)\n            {\n                C[i * k + j] += A[i * n + s] * B[s * k + j];\n            }\n        }\n    }\n#endif\n    return TINY_OK;\n}\n\n// matrix * matrix | float with padding and step sizes\n/**\n * @name tiny_mat_mult_ex_f32\n * @brief Multiplies two matrices of type float32 with padding and step sizes.\n * @param A Pointer to the first input matrix.\n * @param B Pointer to the second input matrix.\n * @param C Pointer to the output matrix.\n * @param A_rows Number of rows in the first matrix.\n * @param A_cols Number of columns in the first matrix and rows in the second matrix.\n * @param B_cols Number of columns in the second matrix.\n * @param A_padding Number of padding columns in the first matrix.\n * @param B_padding Number of padding columns in the second matrix.\n * @param C_padding Number of padding columns in the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix multiplication with the specified padding and step sizes.\n * @note The function assumes that the input matrices are in row-major order.\n */\ntiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding)\n{\n    if (NULL == A || NULL == B || NULL == C)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (A_rows &lt;= 0 || A_cols &lt;= 0 || B_cols &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    if (A_padding &lt; 0 || B_padding &lt; 0 || C_padding &lt; 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    // Prefer ESP-DSP only when paddings are zero (contiguous storage)\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (A_padding == 0 &amp;&amp; B_padding == 0 &amp;&amp; C_padding == 0) {\n        dspm_mult_ex_f32(A, B, C, A_rows, A_cols, B_cols, 0, 0, 0);\n        return TINY_OK;\n    }\n#endif\n\n    // Matrix A(m,n), m - amount of rows, n - amount of columns\n    // C(m,k) = A(m,n)*B(n,k)\n    // C[i][j] = sum_{s=0}^{n-1} A[i][s] * B[s][j]\n    const int A_step = A_cols + A_padding;\n    const int B_step = B_cols + B_padding;\n    const int C_step = B_cols + C_padding;\n\n    for (int i = 0; i &lt; A_rows; i++)\n    {\n        for (int j = 0; j &lt; B_cols; j++)\n        {\n            float sum = 0.0f;\n            for (int s = 0; s &lt; A_cols; s++)\n            {\n                sum += A[i * A_step + s] * B[s * B_step + j];\n            }\n            C[i * C_step + j] = sum;\n        }\n    }\n    return TINY_OK;\n}\n\n// matrix * constant | float\n/**\n * @name tiny_mat_multc_f32\n * @brief Multiplies a matrix by a constant of type float32.\n * @param input Pointer to the input matrix.\n * @param output Pointer to the output matrix.\n * @param C Constant value to be multiplied with each element of the matrix.\n * @param rows Number of rows in the matrices.\n * @param cols Number of columns in the matrices.\n * @param padd_in Number of padding columns in the input matrix.\n * @param padd_out Number of padding columns in the output matrix.\n * @param step_in Step size for the input matrix.\n * @param step_out Step size for the output matrix.\n * @return Returns TINY_OK on success, or an error code on failure.\n * @note This function performs matrix multiplication with a constant with the specified padding and step sizes.\n * @note The function assumes that the input matrix is in row-major order.\n */\ntiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    // paddings must be non-negative, steps must be at least 1.\n    if (rows &lt;= 0 || cols &lt;= 0 || padd_in &lt; 0 || padd_out &lt; 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_MATH_INVALID_PARAM;\n    }\n    // pad refers to the columns that are not used in the matrix operation\n    // Prefer ESP-DSP only when all paddings are 0 and all steps are 1 (contiguous)\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padd_in == 0 &amp;&amp; padd_out == 0 &amp;&amp; step_in == 1 &amp;&amp; step_out == 1) {\n        dspm_mulc_f32(input, output, C, rows, cols, 0, 0, 1, 1);\n        return TINY_OK;\n    }\n#endif\n\n    const int in_row_stride = cols + padd_in;\n    const int out_row_stride = cols + padd_out;\n\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in = row * in_row_stride;\n        int base_out = row * out_row_stride;\n\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = base_in + col * step_in;\n            int idx_out = base_out + col * step_out;\n\n            output[idx_out] = input[idx_in] * C;\n        }\n    }\n    return TINY_OK;\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-test/","title":"TINY_MAT TEST","text":""},{"location":"MATH/MATRIX/tiny-mat-test/#test-code","title":"TEST CODE","text":""},{"location":"MATH/MATRIX/tiny-mat-test/#tiny_mat_testh","title":"tiny_mat_test.h","text":"<pre><code>/**\n * @file tiny_mat_test.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the test of the submodule mat (basic matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_math_config.h\"\n#include \"tiny_mat.h\"\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\nvoid tiny_mat_test(void);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-test/#tiny_mat_testc","title":"tiny_mat_test.c","text":"<pre><code>/**\n * @file tiny_mat_test.c\n * @brief Comprehensive stress tests for tiny_mat module, targeting edge cases and potential weaknesses.\n * @note Tests include: step parameters, different paddings, extreme values, boundary cases, and complex memory layouts.\n */\n\n#include \"tiny_mat_test.h\"\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n/**\n * @brief Test tiny_mat_add_f32 with pad=0 and step=1 (contiguous memory layout)\n * \n * Test Scenario:\n *   - This test case uses contiguous memory layout (no padding, step=1)\n *   - On ESP32 platform, this should trigger ESP-DSP optimized implementation\n *   - On other platforms, uses the standard implementation\n * \n * Memory Layout:\n *   - Input1: 3x4 matrix stored contiguously in memory\n *     [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n *     Logical view:\n *       1.0   2.0   3.0   4.0\n *       5.0   6.0   7.0   8.0\n *       9.0  10.0  11.0  12.0\n * \n *   - Input2: 3x4 matrix stored contiguously in memory\n *     [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5]\n *     Logical view:\n *       0.5   1.5   2.5   3.5\n *       4.5   5.5   6.5   7.5\n *       8.5   9.5  10.5  11.5\n * \n * Expected Output:\n *   - Output: 3x4 matrix, each element = input1[i][j] + input2[i][j]\n *     Expected logical view:\n *       1.5   3.5   5.5   7.5\n *       9.5  11.5  13.5  15.5\n *      17.5  19.5  21.5  23.5\n * \n * Parameters:\n *   - rows = 3, cols = 4\n *   - padd1 = 0, padd2 = 0, padd_out = 0 (no padding)\n *   - step1 = 1, step2 = 1, step_out = 1 (contiguous access)\n */\nvoid test_tiny_mat_add_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 1: tiny_mat_add_f32 - Contiguous Memory Layout (pad=0, step=1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=3, cols=4, pad=0, step=1\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 3;\n    const int cols = 4;\n\n    // Input matrices (contiguous, no padding)\n    float input1[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                        5.0f, 6.0f, 7.0f, 8.0f,\n                        9.0f, 10.0f, 11.0f, 12.0f};\n\n    float input2[12] = {0.5f, 1.5f, 2.5f, 3.5f,\n                        4.5f, 5.5f, 6.5f, 7.5f,\n                        8.5f, 9.5f, 10.5f, 11.5f};\n\n    float output[12];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input1 Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input1[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Input2 Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input2[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [4.5  5.5  6.5  7.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [8.5  9.5 10.5 11.5]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Expected Output Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input1[i] + input2[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [9.5 11.5 13.5 15.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [17.5 19.5 21.5 23.5] &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Test with pad=0, step=1 (should use ESP-DSP on ESP32)\n    tiny_error_t err = tiny_mat_add_f32(input1, input2, output, rows, cols, \n                                        0, 0, 0,  // padd1=0, padd2=0, padd_out=0\n                                        1, 1, 1); // step1=1, step2=1, step_out=1\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (12 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 12; i++) {\n            printf(\"%5.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\\n\\r\");\n        printf(\"          [9.5 11.5 13.5 15.5]  &lt;- Row 1\\n\\r\");\n        printf(\"          [17.5 19.5 21.5 23.5] &lt;- Row 2\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; rows * cols; i++) {\n            float expected = input1[i] + input2[i];\n            float tolerance = 1e-6f;\n            float diff = (output[i] &gt; expected) ? (output[i] - expected) : (expected - output[i]);\n            if (diff &gt; tolerance) {\n                all_correct = 0;\n                break;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_add_f32 with pad!=0 and step&gt;1 (non-contiguous memory layout)\n * \n * ================================================================================\n * Test Scenario:\n * ================================================================================\n * This test case demonstrates how to handle matrix addition with non-contiguous\n * memory layout. In real applications, matrix data may not be stored contiguously:\n *   1. Padding: Extra space at the end of each row\n *   2. Stride/Step: Gaps between elements\n * \n * For example, a 2x3 logical matrix with padding=2 and step=2 may have memory layout:\n *   Logical matrix:        Memory array (first 10 elements):\n *   [1.0  2.0  3.0]  [1.0, 0, 2.0, 0, 3.0, 0, 0, 0, 0, 0, ...]\n *   [4.0  5.0  6.0]  [4.0, 0, 5.0, 0, 6.0, 0, 0, 0, 0, 0, ...]\n * \n * ================================================================================\n * Index Calculation Formula:\n * ================================================================================\n * For matrix element [row][col], the memory index is calculated as:\n *   index = row * (cols + padding) + col * step\n * \n * Where:\n *   - row: Row number (starting from 0)\n *   - col: Column number (starting from 0)\n *   - cols: Number of columns in the matrix\n *   - padding: Number of padding elements at the end of each row\n *   - step: Stride between columns (element spacing)\n * \n * ================================================================================\n * Input1 Details (2x3 matrix, padding=2, step=2):\n * ================================================================================\n * Logical matrix view:\n *   [1.0  2.0  3.0]\n *   [4.0  5.0  6.0]\n * \n * Memory array (input1[20]) actual content:\n *   Index:  0    1    2    3    4    5    6    7    8    9   10   11   12   ...\n *   Value:  [1.0, 0.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]\n *           |---- row 0 ----|  |-- padding --|  |---- row 1 ----|  |-- padding --|\n * \n * Index calculation process:\n *   [0][0]: index = 0*(3+2) + 0*2 = 0*5 + 0 = 0  \u2192 input1[0] = 1.0\n *   [0][1]: index = 0*(3+2) + 1*2 = 0*5 + 2 = 2  \u2192 input1[2] = 2.0\n *   [0][2]: index = 0*(3+2) + 2*2 = 0*5 + 4 = 4  \u2192 input1[4] = 3.0\n *   [1][0]: index = 1*(3+2) + 0*2 = 1*5 + 0 = 5  \u2192 input1[5] = 4.0\n *   [1][1]: index = 1*(3+2) + 1*2 = 1*5 + 2 = 7  \u2192 input1[7] = 5.0\n *   [1][2]: index = 1*(3+2) + 2*2 = 1*5 + 4 = 9  \u2192 input1[9] = 6.0\n * \n * Memory layout visualization:\n *   Position: [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] ...\n *   Value:     1.0  0.0  2.0  0.0  3.0  0.0  0.0  0.0  0.0  6.0   0.0   0.0   0.0 ...\n *   Label:     R0C0      R0C1      R0C2  pad  pad  pad  pad  R1C2   pad   pad   pad ...\n * \n * ================================================================================\n * Input2 Details (2x3 matrix, padding=1, step=3):\n * ================================================================================\n * Logical matrix view:\n *   [0.5  1.5  2.5]\n *   [3.5  4.5  5.5]\n * \n * Memory array (input2[16]) actual content:\n *   Index:  0    1    2    3    4    5    6    7    8    9   10   11   ...\n *   Value:  [0.5, 0.0, 0.0, 1.5, 0.0, 0.0, 2.5, 0.0, 0.0, 0.0, 5.5, 0.0, ...]\n *           |---- row 0 ----|  |-- padding --|  |---- row 1 ----|  |-- padding --|\n * \n * Index calculation process:\n *   [0][0]: index = 0*(3+1) + 0*3 = 0*4 + 0 = 0  \u2192 input2[0] = 0.5\n *   [0][1]: index = 0*(3+1) + 1*3 = 0*4 + 3 = 3  \u2192 input2[3] = 1.5\n *   [0][2]: index = 0*(3+1) + 2*3 = 0*4 + 6 = 6  \u2192 input2[6] = 2.5\n *   [1][0]: index = 1*(3+1) + 0*3 = 1*4 + 0 = 4  \u2192 input2[4] = 3.5\n *   [1][1]: index = 1*(3+1) + 1*3 = 1*4 + 3 = 7  \u2192 input2[7] = 4.5\n *   [1][2]: index = 1*(3+1) + 2*3 = 1*4 + 6 = 10 \u2192 input2[10] = 5.5\n * \n * Memory layout visualization:\n *   Position: [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\n *   Value:     0.5  0.0  0.0  1.5  3.5  0.0  2.5  4.5  0.0  0.0  5.5   0.0 ...\n *   Label:     R0C0      R0C1      R1C0      R0C2  R1C1      R1C2   pad ...\n * \n * ================================================================================\n * Expected Output (2x3 matrix, padding=2, step=2):\n * ================================================================================\n * Logical matrix view:\n *   [1.5  3.5  5.5]\n *   [7.5  9.5 11.5]\n * \n * Calculation process:\n *   output[0][0] = input1[0][0] + input2[0][0] = 1.0 + 0.5 = 1.5\n *   output[0][1] = input1[0][1] + input2[0][1] = 2.0 + 1.5 = 3.5\n *   output[0][2] = input1[0][2] + input2[0][2] = 3.0 + 2.5 = 5.5\n *   output[1][0] = input1[1][0] + input2[1][0] = 4.0 + 3.5 = 7.5\n *   output[1][1] = input1[1][1] + input2[1][1] = 5.0 + 4.5 = 9.5\n *   output[1][2] = input1[1][2] + input2[1][2] = 6.0 + 5.5 = 11.5\n * \n * Output memory array (output[20]) expected content:\n *   Index:  0    1    2    3    4    5    6    7    8    9   10   11   12   ...\n *   Value:  [1.5, 0.0, 3.5, 0.0, 5.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]\n *           |---- row 0 ----|  |-- padding --|  |---- row 1 ----|  |-- padding --|\n * \n * ================================================================================\n * Test Parameters:\n * ================================================================================\n *   rows = 2, cols = 3\n *   padd1 = 2, padd2 = 1, padd_out = 2 (with padding)\n *   step1 = 2, step2 = 3, step_out = 2 (non-contiguous access)\n */\nvoid test_tiny_mat_add_f32_padded_strided(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 2: tiny_mat_add_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n\n    const int rows = 2;\n    const int cols = 3;\n    const int padd1 = 2;\n    const int padd2 = 1;\n    const int padd_out = 2;\n    const int step1 = 2;\n    const int step2 = 3;\n    const int step_out = 2;\n\n    printf(\"Parameters: rows=%d, cols=%d, pad1=%d, pad2=%d, pad_out=%d, step1=%d, step2=%d, step_out=%d\\n\\r\",\n           rows, cols, padd1, padd2, padd_out, step1, step2, step_out);\n    printf(\"Index formula: index = row * (cols + padding) + col * step\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Input1: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd1 = 3 + 2 = 5\n    // Memory index calculation: base = row * (cols + padd1), offset = col * step1\n    float input1[20] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 2, 4\n    input1[0 * 5 + 0 * 2] = 1.0f; // row 0, col 0: index = 0*5 + 0*2 = 0\n    input1[0 * 5 + 1 * 2] = 2.0f; // row 0, col 1: index = 0*5 + 1*2 = 2\n    input1[0 * 5 + 2 * 2] = 3.0f; // row 0, col 2: index = 0*5 + 2*2 = 4\n    // Row 1: elements at indices 5, 7, 9\n    input1[1 * 5 + 0 * 2] = 4.0f; // row 1, col 0: index = 1*5 + 0*2 = 5\n    input1[1 * 5 + 1 * 2] = 5.0f; // row 1, col 1: index = 1*5 + 1*2 = 7\n    input1[1 * 5 + 2 * 2] = 6.0f; // row 1, col 2: index = 1*5 + 2*2 = 9\n\n    // Input2: 2 rows, 3 cols, padding=1, step=3\n    // Row stride = cols + padd2 = 3 + 1 = 4\n    // Memory index calculation: base = row * (cols + padd2), offset = col * step2\n    float input2[16] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 3, 6\n    input2[0 * 4 + 0 * 3] = 0.5f; // row 0, col 0: index = 0*4 + 0*3 = 0\n    input2[0 * 4 + 1 * 3] = 1.5f; // row 0, col 1: index = 0*4 + 1*3 = 3\n    input2[0 * 4 + 2 * 3] = 2.5f; // row 0, col 2: index = 0*4 + 2*3 = 6\n    // Row 1: elements at indices 4, 7, 10\n    input2[1 * 4 + 0 * 3] = 3.5f; // row 1, col 0: index = 1*4 + 0*3 = 4\n    input2[1 * 4 + 1 * 3] = 4.5f; // row 1, col 1: index = 1*4 + 1*3 = 7\n    input2[1 * 4 + 2 * 3] = 5.5f; // row 1, col 2: index = 1*4 + 2*3 = 10\n\n    // Output: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_out = 3 + 2 = 5\n    float output[20] = {0}; // Allocate enough space, initialize to zero\n\n    printf(\"Input1 Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd1, step1);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", input1[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Input2 Memory Layout (16 elements, pad=%d, step=%d):\\n\\r\", padd2, step2);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%4.1f \", input2[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\\n\\r\");\n    printf(\"          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Calculate expected output\n    float expected_output[20] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        for (int col = 0; col &lt; cols; col++) {\n            int idx1 = row * (cols + padd1) + col * step1;\n            int idx2 = row * (cols + padd2) + col * step2;\n            int idx_out = row * (cols + padd_out) + col * step_out;\n            expected_output[idx_out] = input1[idx1] + input2[idx2];\n        }\n    }\n\n    printf(\"Expected Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", expected_output[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    tiny_error_t err = tiny_mat_add_f32(input1, input2, output, rows, cols,\n                                        padd1, padd2, padd_out,\n                                        step1, step2, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 15; i++) {\n            printf(\"%4.1f \", output[i]);\n        }\n        printf(\"...\\n\\r\");\n        printf(\"  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n        printf(\"          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n        printf(\"          (X = padding/unused)\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            for (int col = 0; col &lt; cols; col++) {\n                int idx_out = row * (cols + padd_out) + col * step_out;\n                int idx1 = row * (cols + padd1) + col * step1;\n                int idx2 = row * (cols + padd2) + col * step2;\n                float expected = input1[idx1] + input2[idx2];\n                float tolerance = 1e-6f;\n                float diff = (output[idx_out] &gt; expected) ? (output[idx_out] - expected) : (expected - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    all_correct = 0;\n                    break;\n                }\n            }\n            if (!all_correct) break;\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_addc_f32 with pad=0 and step=1 (contiguous memory layout)\n */\nvoid test_tiny_mat_addc_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 3: tiny_mat_addc_f32 - Contiguous Memory Layout (pad=0, step=1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=3, cols=4, pad=0, step=1, C=2.5\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 3;\n    const int cols = 4;\n    const float C = 2.5f;\n\n    // Input matrix (contiguous, no padding)\n    float input[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                       5.0f, 6.0f, 7.0f, 8.0f,\n                       9.0f, 10.0f, 11.0f, 12.0f};\n\n    float output[12];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Constant C = %5.1f\\n\\r\", C);\n    printf(\"\\n\\r\");\n\n    printf(\"Expected Output Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input[i] + C);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [7.5  8.5  9.5 10.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [11.5 12.5 13.5 14.5] &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Test with pad=0, step=1 (should use ESP-DSP on ESP32)\n    tiny_error_t err = tiny_mat_addc_f32(input, output, C, rows, cols,\n                                         0, 0,  // padd_in=0, padd_out=0\n                                         1, 1); // step_in=1, step_out=1\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (12 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 12; i++) {\n            printf(\"%5.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\\n\\r\");\n        printf(\"          [7.5  8.5  9.5 10.5]  &lt;- Row 1\\n\\r\");\n        printf(\"          [11.5 12.5 13.5 14.5] &lt;- Row 2\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; rows * cols; i++) {\n            float expected = input[i] + C;\n            float tolerance = 1e-6f;\n            float diff = (output[i] &gt; expected) ? (output[i] - expected) : (expected - output[i]);\n            if (diff &gt; tolerance) {\n                all_correct = 0;\n                break;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_addc_f32 with pad!=0 and step&gt;1 (non-contiguous memory layout)\n */\nvoid test_tiny_mat_addc_f32_padded_strided(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 4: tiny_mat_addc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n\n    const int rows = 2;\n    const int cols = 3;\n    const int padd_in = 2;\n    const int padd_out = 2;\n    const int step_in = 2;\n    const int step_out = 2;\n    const float C = 1.5f;\n\n    printf(\"Parameters: rows=%d, cols=%d, pad_in=%d, pad_out=%d, step_in=%d, step_out=%d, C=%5.1f\\n\\r\",\n           rows, cols, padd_in, padd_out, step_in, step_out, C);\n    printf(\"Index formula: index = row * (cols + padding) + col * step\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Input: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_in = 3 + 2 = 5\n    float input[20] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 2, 4\n    input[0 * 5 + 0 * 2] = 1.0f; // row 0, col 0: index = 0*5 + 0*2 = 0\n    input[0 * 5 + 1 * 2] = 2.0f; // row 0, col 1: index = 0*5 + 1*2 = 2\n    input[0 * 5 + 2 * 2] = 3.0f; // row 0, col 2: index = 0*5 + 2*2 = 4\n    // Row 1: elements at indices 5, 7, 9\n    input[1 * 5 + 0 * 2] = 4.0f; // row 1, col 0: index = 1*5 + 0*2 = 5\n    input[1 * 5 + 1 * 2] = 5.0f; // row 1, col 1: index = 1*5 + 1*2 = 7\n    input[1 * 5 + 2 * 2] = 6.0f; // row 1, col 2: index = 1*5 + 2*2 = 9\n\n    // Output: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_out = 3 + 2 = 5\n    float output[20] = {0}; // Allocate enough space, initialize to zero\n\n    printf(\"Input Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_in, step_in);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", input[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Constant C = %5.1f\\n\\r\", C);\n    printf(\"\\n\\r\");\n\n    // Calculate expected output\n    float expected_output[20] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = row * (cols + padd_in) + col * step_in;\n            int idx_out = row * (cols + padd_out) + col * step_out;\n            expected_output[idx_out] = input[idx_in] + C;\n        }\n    }\n\n    printf(\"Expected Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", expected_output[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    tiny_error_t err = tiny_mat_addc_f32(input, output, C, rows, cols,\n                                         padd_in, padd_out,\n                                         step_in, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 15; i++) {\n            printf(\"%4.1f \", output[i]);\n        }\n        printf(\"...\\n\\r\");\n        printf(\"  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n        printf(\"          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n        printf(\"          (X = padding/unused)\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            for (int col = 0; col &lt; cols; col++) {\n                int idx_in = row * (cols + padd_in) + col * step_in;\n                int idx_out = row * (cols + padd_out) + col * step_out;\n                float expected = input[idx_in] + C;\n                float tolerance = 1e-6f;\n                float diff = (output[idx_out] &gt; expected) ? (output[idx_out] - expected) : (expected - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    all_correct = 0;\n                    break;\n                }\n            }\n            if (!all_correct) break;\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_sub_f32 with pad=0 and step=1 (contiguous memory layout)\n */\nvoid test_tiny_mat_sub_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 5: tiny_mat_sub_f32 - Contiguous Memory Layout (pad=0, step=1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=3, cols=4, pad=0, step=1\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 3;\n    const int cols = 4;\n\n    // Input matrices (contiguous, no padding)\n    float input1[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                        5.0f, 6.0f, 7.0f, 8.0f,\n                        9.0f, 10.0f, 11.0f, 12.0f};\n\n    float input2[12] = {0.5f, 1.5f, 2.5f, 3.5f,\n                        4.5f, 5.5f, 6.5f, 7.5f,\n                        8.5f, 9.5f, 10.5f, 11.5f};\n\n    float output[12];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input1 Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input1[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Input2 Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input2[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [4.5  5.5  6.5  7.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [8.5  9.5 10.5 11.5]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Expected Output Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input1[i] - input2[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [0.5  0.5  0.5  0.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [0.5  0.5  0.5  0.5] &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Test with pad=0, step=1 (should use ESP-DSP on ESP32)\n    tiny_error_t err = tiny_mat_sub_f32(input1, input2, output, rows, cols, \n                                        0, 0, 0,  // padd1=0, padd2=0, padd_out=0\n                                        1, 1, 1); // step1=1, step2=1, step_out=1\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (12 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 12; i++) {\n            printf(\"%5.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\\n\\r\");\n        printf(\"          [0.5  0.5  0.5  0.5]  &lt;- Row 1\\n\\r\");\n        printf(\"          [0.5  0.5  0.5  0.5] &lt;- Row 2\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; rows * cols; i++) {\n            float expected = input1[i] - input2[i];\n            float tolerance = 1e-6f;\n            float diff = (output[i] &gt; expected) ? (output[i] - expected) : (expected - output[i]);\n            if (diff &gt; tolerance) {\n                all_correct = 0;\n                break;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_sub_f32 with pad!=0 and step&gt;1 (non-contiguous memory layout)\n */\nvoid test_tiny_mat_sub_f32_padded_strided(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 6: tiny_mat_sub_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n\n    const int rows = 2;\n    const int cols = 3;\n    const int padd1 = 2;\n    const int padd2 = 1;\n    const int padd_out = 2;\n    const int step1 = 2;\n    const int step2 = 3;\n    const int step_out = 2;\n\n    printf(\"Parameters: rows=%d, cols=%d, pad1=%d, pad2=%d, pad_out=%d, step1=%d, step2=%d, step_out=%d\\n\\r\",\n           rows, cols, padd1, padd2, padd_out, step1, step2, step_out);\n    printf(\"Index formula: index = row * (cols + padding) + col * step\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Input1: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd1 = 3 + 2 = 5\n    float input1[20] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 2, 4\n    input1[0 * 5 + 0 * 2] = 1.0f; // row 0, col 0: index = 0*5 + 0*2 = 0\n    input1[0 * 5 + 1 * 2] = 2.0f; // row 0, col 1: index = 0*5 + 1*2 = 2\n    input1[0 * 5 + 2 * 2] = 3.0f; // row 0, col 2: index = 0*5 + 2*2 = 4\n    // Row 1: elements at indices 5, 7, 9\n    input1[1 * 5 + 0 * 2] = 4.0f; // row 1, col 0: index = 1*5 + 0*2 = 5\n    input1[1 * 5 + 1 * 2] = 5.0f; // row 1, col 1: index = 1*5 + 1*2 = 7\n    input1[1 * 5 + 2 * 2] = 6.0f; // row 1, col 2: index = 1*5 + 2*2 = 9\n\n    // Input2: 2 rows, 3 cols, padding=1, step=3\n    // Row stride = cols + padd2 = 3 + 1 = 4\n    float input2[16] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 3, 6\n    input2[0 * 4 + 0 * 3] = 0.5f; // row 0, col 0: index = 0*4 + 0*3 = 0\n    input2[0 * 4 + 1 * 3] = 1.5f; // row 0, col 1: index = 0*4 + 1*3 = 3\n    input2[0 * 4 + 2 * 3] = 2.5f; // row 0, col 2: index = 0*4 + 2*3 = 6\n    // Row 1: elements at indices 4, 7, 10\n    input2[1 * 4 + 0 * 3] = 3.5f; // row 1, col 0: index = 1*4 + 0*3 = 4\n    input2[1 * 4 + 1 * 3] = 4.5f; // row 1, col 1: index = 1*4 + 1*3 = 7\n    input2[1 * 4 + 2 * 3] = 5.5f; // row 1, col 2: index = 1*4 + 2*3 = 10\n\n    // Output: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_out = 3 + 2 = 5\n    float output[20] = {0}; // Allocate enough space, initialize to zero\n\n    printf(\"Input1 Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd1, step1);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", input1[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Input2 Memory Layout (16 elements, pad=%d, step=%d):\\n\\r\", padd2, step2);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%4.1f \", input2[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\\n\\r\");\n    printf(\"          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Calculate expected output\n    float expected_output[20] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        for (int col = 0; col &lt; cols; col++) {\n            int idx1 = row * (cols + padd1) + col * step1;\n            int idx2 = row * (cols + padd2) + col * step2;\n            int idx_out = row * (cols + padd_out) + col * step_out;\n            expected_output[idx_out] = input1[idx1] - input2[idx2];\n        }\n    }\n\n    printf(\"Expected Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", expected_output[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    tiny_error_t err = tiny_mat_sub_f32(input1, input2, output, rows, cols,\n                                        padd1, padd2, padd_out,\n                                        step1, step2, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 15; i++) {\n            printf(\"%4.1f \", output[i]);\n        }\n        printf(\"...\\n\\r\");\n        printf(\"  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n        printf(\"          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n        printf(\"          (X = padding/unused)\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            for (int col = 0; col &lt; cols; col++) {\n                int idx1 = row * (cols + padd1) + col * step1;\n                int idx2 = row * (cols + padd2) + col * step2;\n                int idx_out = row * (cols + padd_out) + col * step_out;\n                float expected = input1[idx1] - input2[idx2];\n                float tolerance = 1e-6f;\n                float diff = (output[idx_out] &gt; expected) ? (output[idx_out] - expected) : (expected - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    all_correct = 0;\n                    break;\n                }\n            }\n            if (!all_correct) break;\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_subc_f32 with pad=0 and step=1 (contiguous memory layout)\n */\nvoid test_tiny_mat_subc_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 7: tiny_mat_subc_f32 - Contiguous Memory Layout (pad=0, step=1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=3, cols=4, pad=0, step=1, C=2.5\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 3;\n    const int cols = 4;\n    const float C = 2.5f;\n\n    // Input matrix (contiguous, no padding)\n    float input[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                       5.0f, 6.0f, 7.0f, 8.0f,\n                       9.0f, 10.0f, 11.0f, 12.0f};\n\n    float output[12];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Constant C = %5.1f\\n\\r\", C);\n    printf(\"\\n\\r\");\n\n    printf(\"Expected Output Memory Layout (12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", input[i] - C);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Test with pad=0, step=1 (should use ESP-DSP on ESP32)\n    tiny_error_t err = tiny_mat_subc_f32(input, output, C, rows, cols,\n                                         0, 0,  // padd_in=0, padd_out=0\n                                         1, 1); // step_in=1, step_out=1\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (12 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 12; i++) {\n            printf(\"%5.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\\n\\r\");\n        printf(\"          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\\n\\r\");\n        printf(\"          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; rows * cols; i++) {\n            float expected = input[i] - C;\n            float tolerance = 1e-6f;\n            float diff = (output[i] &gt; expected) ? (output[i] - expected) : (expected - output[i]);\n            if (diff &gt; tolerance) {\n                all_correct = 0;\n                break;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_subc_f32 with pad!=0 and step&gt;1 (non-contiguous memory layout)\n */\nvoid test_tiny_mat_subc_f32_padded_strided(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 8: tiny_mat_subc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n\n    const int rows = 2;\n    const int cols = 3;\n    const int padd_in = 2;\n    const int padd_out = 2;\n    const int step_in = 2;\n    const int step_out = 2;\n    const float C = 1.5f;\n\n    printf(\"Parameters: rows=%d, cols=%d, pad_in=%d, pad_out=%d, step_in=%d, step_out=%d, C=%5.1f\\n\\r\",\n           rows, cols, padd_in, padd_out, step_in, step_out, C);\n    printf(\"Index formula: index = row * (cols + padding) + col * step\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Input: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_in = 3 + 2 = 5\n    float input[20] = {0}; // Allocate enough space, initialize to zero\n    // Row 0: elements at indices 0, 2, 4\n    input[0 * 5 + 0 * 2] = 1.0f; // row 0, col 0: index = 0*5 + 0*2 = 0\n    input[0 * 5 + 1 * 2] = 2.0f; // row 0, col 1: index = 0*5 + 1*2 = 2\n    input[0 * 5 + 2 * 2] = 3.0f; // row 0, col 2: index = 0*5 + 2*2 = 4\n    // Row 1: elements at indices 5, 7, 9\n    input[1 * 5 + 0 * 2] = 4.0f; // row 1, col 0: index = 1*5 + 0*2 = 5\n    input[1 * 5 + 1 * 2] = 5.0f; // row 1, col 1: index = 1*5 + 1*2 = 7\n    input[1 * 5 + 2 * 2] = 6.0f; // row 1, col 2: index = 1*5 + 2*2 = 9\n\n    // Output: 2 rows, 3 cols, padding=2, step=2\n    // Row stride = cols + padd_out = 3 + 2 = 5\n    float output[20] = {0}; // Allocate enough space, initialize to zero\n\n    printf(\"Input Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_in, step_in);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", input[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Constant C = %5.1f\\n\\r\", C);\n    printf(\"\\n\\r\");\n\n    // Calculate expected output\n    float expected_output[20] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = row * (cols + padd_in) + col * step_in;\n            int idx_out = row * (cols + padd_out) + col * step_out;\n            expected_output[idx_out] = input[idx_in] - C;\n        }\n    }\n\n    printf(\"Expected Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 15; i++) {\n        printf(\"%4.1f \", expected_output[i]);\n    }\n    printf(\"...\\n\\r\");\n    printf(\"  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    tiny_error_t err = tiny_mat_subc_f32(input, output, C, rows, cols,\n                                         padd_in, padd_out,\n                                         step_in, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Memory Layout (20 elements, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 15; i++) {\n            printf(\"%4.1f \", output[i]);\n        }\n        printf(\"...\\n\\r\");\n        printf(\"  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\\n\\r\");\n        printf(\"          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\\n\\r\");\n        printf(\"          (X = padding/unused)\\n\\r\");\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            for (int col = 0; col &lt; cols; col++) {\n                int idx_in = row * (cols + padd_in) + col * step_in;\n                int idx_out = row * (cols + padd_out) + col * step_out;\n                float expected = input[idx_in] - C;\n                float tolerance = 1e-6f;\n                float diff = (output[idx_out] &gt; expected) ? (output[idx_out] - expected) : (expected - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    all_correct = 0;\n                    break;\n                }\n            }\n            if (!all_correct) break;\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_mult_f32 with basic matrix multiplication\n */\nvoid test_tiny_mat_mult_f32_basic(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 9: tiny_mat_mult_f32 - Basic Matrix Multiplication\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: m=3, n=4, k=2 (A is 3x4, B is 4x2, C is 3x2)\\n\\r\");\n    printf(\"Note: This function always uses ESP-DSP on ESP32, standard implementation otherwise\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int m = 3; // rows of A\n    const int n = 4; // cols of A and rows of B\n    const int k = 2; // cols of B\n\n    // Matrix A: 3x4\n    // Memory layout: [row0_col0, row0_col1, row0_col2, row0_col3, row1_col0, ...]\n    float A[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                    5.0f, 6.0f, 7.0f, 8.0f,\n                    9.0f, 10.0f, 11.0f, 12.0f};\n\n    // Matrix B: 4x2\n    float B[8] = {0.5f, 1.5f,\n                  2.5f, 3.5f,\n                  4.5f, 5.5f,\n                  6.5f, 7.5f};\n\n    // Matrix C: 3x2 (output)\n    float C[6];\n    memset(C, 0, sizeof(C));\n\n    printf(\"Matrix A Memory Layout (3x4, 12 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", A[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Matrix B Memory Layout (4x2, 8 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 8; i++) {\n        printf(\"%5.1f \", B[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [2.5  3.5]  &lt;- Row 1\\n\\r\");\n    printf(\"          [4.5  5.5]  &lt;- Row 2\\n\\r\");\n    printf(\"          [6.5  7.5]  &lt;- Row 3\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: C = A * B\n    // C[i][j] = sum_{s=0}^{n-1} A[i][s] * B[s][j]\n    float expected_C[6] = {0};\n    for (int i = 0; i &lt; m; i++) {\n        for (int j = 0; j &lt; k; j++) {\n            float sum = 0.0f;\n            for (int s = 0; s &lt; n; s++) {\n                sum += A[i * n + s] * B[s * k + j];\n            }\n            expected_C[i * k + j] = sum;\n        }\n    }\n\n    printf(\"Expected Output Matrix C Memory Layout (3x2, 6 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 6; i++) {\n        printf(\"%6.1f \", expected_C[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f]  &lt;- Row 0\\n\\r\", expected_C[0], expected_C[1]);\n    printf(\"          [%5.1f  %5.1f]  &lt;- Row 1\\n\\r\", expected_C[2], expected_C[3]);\n    printf(\"          [%5.1f  %5.1f] &lt;- Row 2\\n\\r\", expected_C[4], expected_C[5]);\n    printf(\"  Calculation:\\n\\r\");\n    printf(\"    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\\n\\r\");\n    printf(\"            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 = %5.1f\\n\\r\", expected_C[0]);\n    printf(\"    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1] + A[0][3]*B[3][1]\\n\\r\");\n    printf(\"            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 + 4.0*7.5 = %5.1f\\n\\r\", expected_C[1]);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiplication\n    tiny_error_t err = tiny_mat_mult_f32(A, B, C, m, n, k);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix C Memory Layout (3x2, 6 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 6; i++) {\n            printf(\"%6.1f \", C[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f]  &lt;- Row 0\\n\\r\", C[0], C[1]);\n        printf(\"          [%5.1f  %5.1f]  &lt;- Row 1\\n\\r\", C[2], C[3]);\n        printf(\"          [%5.1f  %5.1f] &lt;- Row 2\\n\\r\", C[4], C[5]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; m * k; i++) {\n            float tolerance = 1e-5f;\n            float diff = (C[i] &gt; expected_C[i]) ? (C[i] - expected_C[i]) : (expected_C[i] - C[i]);\n            if (diff &gt; tolerance) {\n                int row = i / k;\n                int col = i % k;\n                printf(\"  ERROR at [%d][%d]: output = %10.6f, expected = %10.6f, diff = %e\\n\\r\", \n                       row, col, C[i], expected_C[i], diff);\n                all_correct = 0;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_mult_f32 with square matrices\n */\nvoid test_tiny_mat_mult_f32_square(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 10: tiny_mat_mult_f32 - Square Matrix Multiplication\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: m=3, n=3, k=3 (A is 3x3, B is 3x3, C is 3x3)\\n\\r\");\n    printf(\"Note: This function always uses ESP-DSP on ESP32, standard implementation otherwise\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int m = 3; // rows of A\n    const int n = 3; // cols of A and rows of B\n    const int k = 3; // cols of B\n\n    // Matrix A: 3x3\n    float A[9] = {1.0f, 2.0f, 3.0f,\n                  4.0f, 5.0f, 6.0f,\n                  7.0f, 8.0f, 9.0f};\n\n    // Matrix B: 3x3\n    float B[9] = {0.5f, 1.0f, 1.5f,\n                  2.0f, 2.5f, 3.0f,\n                  3.5f, 4.0f, 4.5f};\n\n    // Matrix C: 3x3 (output)\n    float C[9];\n    memset(C, 0, sizeof(C));\n\n    printf(\"Matrix A Memory Layout (3x3, 9 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%5.1f \", A[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0]  &lt;- Row 0\\n\\r\");\n    printf(\"          [4.0  5.0  6.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [7.0  8.0  9.0]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Matrix B Memory Layout (3x3, 9 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%5.1f \", B[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.0  1.5]  &lt;- Row 0\\n\\r\");\n    printf(\"          [2.0  2.5  3.0]  &lt;- Row 1\\n\\r\");\n    printf(\"          [3.5  4.0  4.5]  &lt;- Row 2\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: C = A * B\n    float expected_C[9] = {0};\n    for (int i = 0; i &lt; m; i++) {\n        for (int j = 0; j &lt; k; j++) {\n            float sum = 0.0f;\n            for (int s = 0; s &lt; n; s++) {\n                sum += A[i * n + s] * B[s * k + j];\n            }\n            expected_C[i * k + j] = sum;\n        }\n    }\n\n    printf(\"Expected Output Matrix C Memory Layout (3x3, 9 elements, contiguous):\\n\\r\");\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%6.1f \", expected_C[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f  %5.1f]  &lt;- Row 0\\n\\r\", expected_C[0], expected_C[1], expected_C[2]);\n    printf(\"          [%5.1f  %5.1f  %5.1f]  &lt;- Row 1\\n\\r\", expected_C[3], expected_C[4], expected_C[5]);\n    printf(\"          [%5.1f  %5.1f  %5.1f] &lt;- Row 2\\n\\r\", expected_C[6], expected_C[7], expected_C[8]);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiplication\n    tiny_error_t err = tiny_mat_mult_f32(A, B, C, m, n, k);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix C Memory Layout (3x3, 9 elements, contiguous):\\n\\r\");\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 9; i++) {\n            printf(\"%6.1f \", C[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f  %5.1f]  &lt;- Row 0\\n\\r\", C[0], C[1], C[2]);\n        printf(\"          [%5.1f  %5.1f  %5.1f]  &lt;- Row 1\\n\\r\", C[3], C[4], C[5]);\n        printf(\"          [%5.1f  %5.1f  %5.1f] &lt;- Row 2\\n\\r\", C[6], C[7], C[8]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; m * k; i++) {\n            float tolerance = 1e-5f;\n            float diff = (C[i] &gt; expected_C[i]) ? (C[i] - expected_C[i]) : (expected_C[i] - C[i]);\n            if (diff &gt; tolerance) {\n                all_correct = 0;\n                break;\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_mult_ex_f32 with contiguous matrices (pad=0)\n */\nvoid test_tiny_mat_mult_ex_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 11: tiny_mat_mult_ex_f32 - Contiguous Matrix Multiplication\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: A_rows=3, A_cols=4, B_cols=2, A_padding=0, B_padding=0, C_padding=0\\n\\r\");\n    printf(\"Matrix dimensions: A is 3x4, B is 4x2, C is 3x2\\n\\r\");\n    printf(\"Note: This should use ESP-DSP on ESP32 when all paddings are 0\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int A_rows = 3;\n    const int A_cols = 4;\n    const int B_cols = 2;\n    const int A_padding = 0;\n    const int B_padding = 0;\n    const int C_padding = 0;\n\n    const int A_step = A_cols + A_padding; // 4\n    const int B_step = B_cols + B_padding; // 2\n    const int C_step = B_cols + C_padding; // 2\n\n    // Matrix A: 3x4 (contiguous, no padding)\n    float A[12] = {1.0f, 2.0f, 3.0f, 4.0f,\n                   5.0f, 6.0f, 7.0f, 8.0f,\n                   9.0f, 10.0f, 11.0f, 12.0f};\n\n    // Matrix B: 4x2 (contiguous, no padding)\n    float B[8] = {0.5f, 1.5f,\n                  2.5f, 3.5f,\n                  4.5f, 5.5f,\n                  6.5f, 7.5f};\n\n    // Matrix C: 3x2 (output, contiguous, no padding)\n    float C[6];\n    memset(C, 0, sizeof(C));\n\n    printf(\"Matrix A Memory Layout (3x4, 12 elements, contiguous, pad=%d):\\n\\r\", A_padding);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 12; i++) {\n        printf(\"%5.1f \", A[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0 (indices: 0-3)\\n\\r\");\n    printf(\"          [5.0  6.0  7.0  8.0]  &lt;- Row 1 (indices: 4-7)\\n\\r\");\n    printf(\"          [9.0 10.0 11.0 12.0]  &lt;- Row 2 (indices: 8-11)\\n\\r\");\n    printf(\"  Step size: %d (A_cols + A_padding = %d + %d)\\n\\r\", A_step, A_cols, A_padding);\n    printf(\"\\n\\r\");\n\n    printf(\"Matrix B Memory Layout (4x2, 8 elements, contiguous, pad=%d):\\n\\r\", B_padding);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 8; i++) {\n        printf(\"%5.1f \", B[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.5]  &lt;- Row 0 (indices: 0-1)\\n\\r\");\n    printf(\"          [2.5  3.5]  &lt;- Row 1 (indices: 2-3)\\n\\r\");\n    printf(\"          [4.5  5.5]  &lt;- Row 2 (indices: 4-5)\\n\\r\");\n    printf(\"          [6.5  7.5]  &lt;- Row 3 (indices: 6-7)\\n\\r\");\n    printf(\"  Step size: %d (B_cols + B_padding = %d + %d)\\n\\r\", B_step, B_cols, B_padding);\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: C = A * B\n    // C[i][j] = sum_{s=0}^{A_cols-1} A[i][s] * B[s][j]\n    // Index calculation: A[i * A_step + s], B[s * B_step + j], C[i * C_step + j]\n    float expected_C[6] = {0};\n    for (int i = 0; i &lt; A_rows; i++) {\n        for (int j = 0; j &lt; B_cols; j++) {\n            float sum = 0.0f;\n            for (int s = 0; s &lt; A_cols; s++) {\n                int idx_A = i * A_step + s;\n                int idx_B = s * B_step + j;\n                sum += A[idx_A] * B[idx_B];\n            }\n            expected_C[i * C_step + j] = sum;\n        }\n    }\n\n    printf(\"Expected Output Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=%d):\\n\\r\", C_padding);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 6; i++) {\n        printf(\"%6.1f \", expected_C[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f]  &lt;- Row 0 (indices: 0-1)\\n\\r\", expected_C[0], expected_C[1]);\n    printf(\"          [%5.1f  %5.1f]  &lt;- Row 1 (indices: 2-3)\\n\\r\", expected_C[2], expected_C[3]);\n    printf(\"          [%5.1f  %5.1f] &lt;- Row 2 (indices: 4-5)\\n\\r\", expected_C[4], expected_C[5]);\n    printf(\"  Step size: %d (B_cols + C_padding = %d + %d)\\n\\r\", C_step, B_cols, C_padding);\n    printf(\"  Calculation example:\\n\\r\");\n    printf(\"    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\\n\\r\");\n    printf(\"            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 = %5.1f\\n\\r\", expected_C[0]);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiplication\n    tiny_error_t err = tiny_mat_mult_ex_f32(A, B, C, A_rows, A_cols, B_cols, A_padding, B_padding, C_padding);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=%d):\\n\\r\", C_padding);\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 6; i++) {\n            printf(\"%6.1f \", C[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f]  &lt;- Row 0 (indices: 0-1)\\n\\r\", C[0], C[1]);\n        printf(\"          [%5.1f  %5.1f]  &lt;- Row 1 (indices: 2-3)\\n\\r\", C[2], C[3]);\n        printf(\"          [%5.1f  %5.1f] &lt;- Row 2 (indices: 4-5)\\n\\r\", C[4], C[5]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; A_rows; i++) {\n            for (int j = 0; j &lt; B_cols; j++) {\n                int idx = i * C_step + j;\n                float tolerance = 1e-5f;\n                float diff = (C[idx] &gt; expected_C[idx]) ? (C[idx] - expected_C[idx]) : (expected_C[idx] - C[idx]);\n                if (diff &gt; tolerance) {\n                    printf(\"  ERROR at [%d][%d] (index %d): output = %10.6f, expected = %10.6f, diff = %e\\n\\r\", \n                           i, j, idx, C[idx], expected_C[idx], diff);\n                    all_correct = 0;\n                }\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_mult_ex_f32 with padded matrices (pad!=0)\n */\nvoid test_tiny_mat_mult_ex_f32_padded(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 12: tiny_mat_mult_ex_f32 - Padded Matrix Multiplication\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: A_rows=2, A_cols=3, B_cols=2, A_padding=2, B_padding=1, C_padding=1\\n\\r\");\n    printf(\"Matrix dimensions: A is 2x3, B is 3x2, C is 2x2\\n\\r\");\n    printf(\"Note: This should use own implementation when padding is non-zero\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int A_rows = 2;\n    const int A_cols = 3;\n    const int B_cols = 2;\n    const int A_padding = 2;\n    const int B_padding = 1;\n    const int C_padding = 1;\n\n    const int A_step = A_cols + A_padding; // 3 + 2 = 5\n    const int B_step = B_cols + B_padding; // 2 + 1 = 3\n    const int C_step = B_cols + C_padding; // 2 + 1 = 3\n\n    // Matrix A: 2x3 with padding=2, so each row has 5 elements (3 data + 2 padding)\n    // Total memory: 2 rows * 5 elements = 10 elements\n    float A[10] = {1.0f, 2.0f, 3.0f, 0.0f, 0.0f,  // Row 0: [1.0, 2.0, 3.0, X, X]\n                   4.0f, 5.0f, 6.0f, 0.0f, 0.0f}; // Row 1: [4.0, 5.0, 6.0, X, X]\n\n    // Matrix B: 3x2 with padding=1, so each row has 3 elements (2 data + 1 padding)\n    // Total memory: 3 rows * 3 elements = 9 elements\n    float B[9] = {0.5f, 1.5f, 0.0f,  // Row 0: [0.5, 1.5, X]\n                  2.5f, 3.5f, 0.0f,  // Row 1: [2.5, 3.5, X]\n                  4.5f, 5.5f, 0.0f}; // Row 2: [4.5, 5.5, X]\n\n    // Matrix C: 2x2 with padding=1, so each row has 3 elements (2 data + 1 padding)\n    // Total memory: 2 rows * 3 elements = 6 elements\n    float C[6];\n    memset(C, 0, sizeof(C));\n\n    printf(\"Matrix A Memory Layout (2x3, pad=%d, step=%d, 10 elements):\\n\\r\", A_padding, A_step);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 10; i++) {\n        printf(\"%4.1f \", A[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0  X   X]  &lt;- Row 0 (indices: 0, 1, 2, 3, 4)\\n\\r\");\n    printf(\"          [4.0  5.0  6.0  X   X]  &lt;- Row 1 (indices: 5, 6, 7, 8, 9)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"  Index calculation: A[i][j] = A[i * %d + j]\\n\\r\", A_step);\n    printf(\"    Row 0: indices 0, 1, 2 (data), 3, 4 (padding)\\n\\r\");\n    printf(\"    Row 1: indices 5, 6, 7 (data), 8, 9 (padding)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    printf(\"Matrix B Memory Layout (3x2, pad=%d, step=%d, 9 elements):\\n\\r\", B_padding, B_step);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%4.1f \", B[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [0.5  1.5  X]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\");\n    printf(\"          [2.5  3.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\");\n    printf(\"          [4.5  5.5  X]  &lt;- Row 2 (indices: 6, 7, 8)\\n\\r\");\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"  Index calculation: B[i][j] = B[i * %d + j]\\n\\r\", B_step);\n    printf(\"    Row 0: indices 0, 1 (data), 2 (padding)\\n\\r\");\n    printf(\"    Row 1: indices 3, 4 (data), 5 (padding)\\n\\r\");\n    printf(\"    Row 2: indices 6, 7 (data), 8 (padding)\\n\\r\");\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: C = A * B\n    // C[i][j] = sum_{s=0}^{A_cols-1} A[i][s] * B[s][j]\n    // Index calculation: A[i * A_step + s], B[s * B_step + j], C[i * C_step + j]\n    float expected_C[6] = {0};\n    for (int i = 0; i &lt; A_rows; i++) {\n        for (int j = 0; j &lt; B_cols; j++) {\n            float sum = 0.0f;\n            for (int s = 0; s &lt; A_cols; s++) {\n                int idx_A = i * A_step + s;\n                int idx_B = s * B_step + j;\n                sum += A[idx_A] * B[idx_B];\n            }\n            expected_C[i * C_step + j] = sum;\n        }\n    }\n\n    printf(\"Expected Output Matrix C Memory Layout (2x2, pad=%d, step=%d, 6 elements):\\n\\r\", C_padding, C_step);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 6; i++) {\n        printf(\"%6.1f \", expected_C[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f  X]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\", expected_C[0], expected_C[1]);\n    printf(\"          [%5.1f  %5.1f  X]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\", expected_C[3], expected_C[4]);\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"  Index calculation: C[i][j] = C[i * %d + j]\\n\\r\", C_step);\n    printf(\"  Calculation:\\n\\r\");\n    printf(\"    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0]\\n\\r\");\n    printf(\"            = A[%d]*B[%d] + A[%d]*B[%d] + A[%d]*B[%d]\\n\\r\", \n           0*A_step+0, 0*B_step+0, 0*A_step+1, 1*B_step+0, 0*A_step+2, 2*B_step+0);\n    printf(\"            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 = %5.1f\\n\\r\", expected_C[0]);\n    printf(\"    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1]\\n\\r\");\n    printf(\"            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 = %5.1f\\n\\r\", expected_C[1]);\n    printf(\"    C[1][0] = A[1][0]*B[0][0] + A[1][1]*B[1][0] + A[1][2]*B[2][0]\\n\\r\");\n    printf(\"            = 4.0*0.5 + 5.0*2.5 + 6.0*4.5 = %5.1f\\n\\r\", expected_C[3]);\n    printf(\"    C[1][1] = A[1][0]*B[0][1] + A[1][1]*B[1][1] + A[1][2]*B[2][1]\\n\\r\");\n    printf(\"            = 4.0*1.5 + 5.0*3.5 + 6.0*5.5 = %5.1f\\n\\r\", expected_C[4]);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiplication\n    tiny_error_t err = tiny_mat_mult_ex_f32(A, B, C, A_rows, A_cols, B_cols, A_padding, B_padding, C_padding);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix C Memory Layout (2x2, pad=%d, step=%d, 6 elements):\\n\\r\", C_padding, C_step);\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 6; i++) {\n            printf(\"%6.1f \", C[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f  X]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\", C[0], C[1]);\n        printf(\"          [%5.1f  %5.1f  X]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\", C[3], C[4]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int i = 0; i &lt; A_rows; i++) {\n            for (int j = 0; j &lt; B_cols; j++) {\n                int idx = i * C_step + j;\n                float tolerance = 1e-5f;\n                float diff = (C[idx] &gt; expected_C[idx]) ? (C[idx] - expected_C[idx]) : (expected_C[idx] - C[idx]);\n                if (diff &gt; tolerance) {\n                    printf(\"  ERROR at [%d][%d] (index %d): output = %10.6f, expected = %10.6f, diff = %e\\n\\r\", \n                           i, j, idx, C[idx], expected_C[idx], diff);\n                    all_correct = 0;\n                }\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_multc_f32 with contiguous matrix (pad=0, step=1)\n */\nvoid test_tiny_mat_multc_f32_contiguous(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 13: tiny_mat_multc_f32 - Contiguous Matrix Multiply Constant\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=3, cols=3, padd_in=0, padd_out=0, step_in=1, step_out=1\\n\\r\");\n    printf(\"Matrix dimensions: 3x3\\n\\r\");\n    printf(\"Constant C: 2.5\\n\\r\");\n    printf(\"Note: This should use ESP-DSP on ESP32 when all paddings are 0 and all steps are 1\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 3;\n    const int cols = 3;\n    const int padd_in = 0;\n    const int padd_out = 0;\n    const int step_in = 1;\n    const int step_out = 1;\n    const float C = 2.5f;\n\n    const int in_row_stride = cols + padd_in;  // 3\n    const int out_row_stride = cols + padd_out; // 3\n\n    // Input matrix: 3x3 (contiguous, no padding)\n    float input[9] = {1.0f, 2.0f, 3.0f,\n                       4.0f, 5.0f, 6.0f,\n                       7.0f, 8.0f, 9.0f};\n\n    // Output matrix: 3x3 (contiguous, no padding)\n    float output[9];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input Matrix Memory Layout (3x3, 9 elements, contiguous, pad=%d, step=%d):\\n\\r\", padd_in, step_in);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%5.1f \", input[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  2.0  3.0]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\");\n    printf(\"          [4.0  5.0  6.0]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\");\n    printf(\"          [7.0  8.0  9.0]  &lt;- Row 2 (indices: 6, 7, 8)\\n\\r\");\n    printf(\"  Row stride: %d (cols + padd_in = %d + %d)\\n\\r\", in_row_stride, cols, padd_in);\n    printf(\"  Index calculation: input[i][j] = input[i * %d + j * %d]\\n\\r\", in_row_stride, step_in);\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: output[i][j] = input[i][j] * C\n    float expected_output[9] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in = row * in_row_stride;\n        int base_out = row * out_row_stride;\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = base_in + col * step_in;\n            int idx_out = base_out + col * step_out;\n            expected_output[idx_out] = input[idx_in] * C;\n        }\n    }\n\n    printf(\"Expected Output Matrix Memory Layout (3x3, 9 elements, contiguous, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 9; i++) {\n        printf(\"%6.1f \", expected_output[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f  %5.1f]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\", \n           expected_output[0], expected_output[1], expected_output[2]);\n    printf(\"          [%5.1f  %5.1f  %5.1f]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\", \n           expected_output[3], expected_output[4], expected_output[5]);\n    printf(\"          [%5.1f  %5.1f  %5.1f] &lt;- Row 2 (indices: 6, 7, 8)\\n\\r\", \n           expected_output[6], expected_output[7], expected_output[8]);\n    printf(\"  Row stride: %d (cols + padd_out = %d + %d)\\n\\r\", out_row_stride, cols, padd_out);\n    printf(\"  Index calculation: output[i][j] = output[i * %d + j * %d]\\n\\r\", out_row_stride, step_out);\n    printf(\"  Calculation: output[i][j] = input[i][j] * %.1f\\n\\r\", C);\n    printf(\"    Example: output[0][0] = input[0][0] * %.1f = 1.0 * %.1f = %.1f\\n\\r\", C, C, expected_output[0]);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiply constant\n    tiny_error_t err = tiny_mat_multc_f32(input, output, C, rows, cols, padd_in, padd_out, step_in, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix Memory Layout (3x3, 9 elements, contiguous, pad=%d, step=%d):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 9; i++) {\n            printf(\"%6.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f  %5.1f]  &lt;- Row 0 (indices: 0, 1, 2)\\n\\r\", \n               output[0], output[1], output[2]);\n        printf(\"          [%5.1f  %5.1f  %5.1f]  &lt;- Row 1 (indices: 3, 4, 5)\\n\\r\", \n               output[3], output[4], output[5]);\n        printf(\"          [%5.1f  %5.1f  %5.1f] &lt;- Row 2 (indices: 6, 7, 8)\\n\\r\", \n               output[6], output[7], output[8]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            int base_out = row * out_row_stride;\n            for (int col = 0; col &lt; cols; col++) {\n                int idx_out = base_out + col * step_out;\n                float tolerance = 1e-5f;\n                float diff = (output[idx_out] &gt; expected_output[idx_out]) ? \n                            (output[idx_out] - expected_output[idx_out]) : \n                            (expected_output[idx_out] - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    printf(\"  ERROR at [%d][%d] (index %d): output = %10.6f, expected = %10.6f, diff = %e\\n\\r\", \n                           row, col, idx_out, output[idx_out], expected_output[idx_out], diff);\n                    all_correct = 0;\n                }\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\n/**\n * @brief Test tiny_mat_multc_f32 with padded and strided matrix (pad!=0, step&gt;1)\n */\nvoid test_tiny_mat_multc_f32_padded_strided(void)\n{\n    printf(\"\\n\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Test Case 14: tiny_mat_multc_f32 - Padded and Strided Matrix Multiply Constant\\n\\r\");\n    printf(\"================================================================================\\n\\r\");\n    printf(\"Parameters: rows=2, cols=3, padd_in=2, padd_out=1, step_in=2, step_out=1\\n\\r\");\n    printf(\"Matrix dimensions: 2x3\\n\\r\");\n    printf(\"Constant C: 3.0\\n\\r\");\n    printf(\"Note: This should use own implementation when padding is non-zero or step &gt; 1\\n\\r\");\n    printf(\"\\n\\r\");\n\n    const int rows = 2;\n    const int cols = 3;\n    const int padd_in = 2;\n    const int padd_out = 1;\n    const int step_in = 2;\n    const int step_out = 1;\n    const float C = 3.0f;\n\n    const int in_row_stride = cols + padd_in;  // 3 + 2 = 5\n    const int out_row_stride = cols + padd_out; // 3 + 1 = 4\n\n    // Input matrix: 2x3 with padding=2, step=2\n    // Each row has 5 elements in memory, but we only use every 2nd element (step=2)\n    // Total memory: 2 rows * 5 elements = 10 elements\n    // Row 0: indices 0, 2, 4 (data), 1, 3 (unused)\n    // Row 1: indices 5, 7, 9 (data), 6, 8 (unused)\n    float input[10] = {1.0f, 0.0f, 2.0f, 0.0f, 3.0f,  // Row 0: [1.0, X, 2.0, X, 3.0]\n                       4.0f, 0.0f, 5.0f, 0.0f, 6.0f}; // Row 1: [4.0, X, 5.0, X, 6.0]\n\n    // Output matrix: 2x3 with padding=1, step=1\n    // Each row has 4 elements (3 data + 1 padding)\n    // Total memory: 2 rows * 4 elements = 8 elements\n    float output[8];\n    memset(output, 0, sizeof(output));\n\n    printf(\"Input Matrix Memory Layout (2x3, pad=%d, step=%d, 10 elements):\\n\\r\", padd_in, step_in);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 10; i++) {\n        printf(\"%4.1f \", input[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (data indices: 0, 2, 4)\\n\\r\");\n    printf(\"          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (data indices: 5, 7, 9)\\n\\r\");\n    printf(\"          (X = unused/padding)\\n\\r\");\n    printf(\"  Row stride: %d (cols + padd_in = %d + %d)\\n\\r\", in_row_stride, cols, padd_in);\n    printf(\"  Index calculation: input[i][j] = input[i * %d + j * %d]\\n\\r\", in_row_stride, step_in);\n    printf(\"    Row 0: input[0][0]=input[%d]=%.1f, input[0][1]=input[%d]=%.1f, input[0][2]=input[%d]=%.1f\\n\\r\",\n           0*in_row_stride+0*step_in, input[0*in_row_stride+0*step_in],\n           0*in_row_stride+1*step_in, input[0*in_row_stride+1*step_in],\n           0*in_row_stride+2*step_in, input[0*in_row_stride+2*step_in]);\n    printf(\"    Row 1: input[1][0]=input[%d]=%.1f, input[1][1]=input[%d]=%.1f, input[1][2]=input[%d]=%.1f\\n\\r\",\n           1*in_row_stride+0*step_in, input[1*in_row_stride+0*step_in],\n           1*in_row_stride+1*step_in, input[1*in_row_stride+1*step_in],\n           1*in_row_stride+2*step_in, input[1*in_row_stride+2*step_in]);\n    printf(\"\\n\\r\");\n\n    // Calculate expected output: output[i][j] = input[i][j] * C\n    float expected_output[8] = {0};\n    for (int row = 0; row &lt; rows; row++) {\n        int base_in = row * in_row_stride;\n        int base_out = row * out_row_stride;\n        for (int col = 0; col &lt; cols; col++) {\n            int idx_in = base_in + col * step_in;\n            int idx_out = base_out + col * step_out;\n            expected_output[idx_out] = input[idx_in] * C;\n        }\n    }\n\n    printf(\"Expected Output Matrix Memory Layout (2x3, pad=%d, step=%d, 8 elements):\\n\\r\", padd_out, step_out);\n    printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\\n\\r\");\n    printf(\"  Value:  \");\n    for (int i = 0; i &lt; 8; i++) {\n        printf(\"%6.1f \", expected_output[i]);\n    }\n    printf(\"\\n\\r\");\n    printf(\"  Matrix: [%5.1f  %5.1f  %5.1f  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\\n\\r\", \n           expected_output[0], expected_output[1], expected_output[2]);\n    printf(\"          [%5.1f  %5.1f  %5.1f  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\\n\\r\", \n           expected_output[4], expected_output[5], expected_output[6]);\n    printf(\"          (X = padding/unused)\\n\\r\");\n    printf(\"  Row stride: %d (cols + padd_out = %d + %d)\\n\\r\", out_row_stride, cols, padd_out);\n    printf(\"  Index calculation: output[i][j] = output[i * %d + j * %d]\\n\\r\", out_row_stride, step_out);\n    printf(\"  Calculation: output[i][j] = input[i][j] * %.1f\\n\\r\", C);\n    printf(\"    Row 0: output[0][0] = input[0][0] * %.1f = %.1f * %.1f = %.1f (index %d)\\n\\r\",\n           C, input[0*in_row_stride+0*step_in], C, expected_output[0*out_row_stride+0*step_out], 0*out_row_stride+0*step_out);\n    printf(\"           output[0][1] = input[0][1] * %.1f = %.1f * %.1f = %.1f (index %d)\\n\\r\",\n           C, input[0*in_row_stride+1*step_in], C, expected_output[0*out_row_stride+1*step_out], 0*out_row_stride+1*step_out);\n    printf(\"           output[0][2] = input[0][2] * %.1f = %.1f * %.1f = %.1f (index %d)\\n\\r\",\n           C, input[0*in_row_stride+2*step_in], C, expected_output[0*out_row_stride+2*step_out], 0*out_row_stride+2*step_out);\n    printf(\"\\n\\r\");\n\n    // Test matrix multiply constant\n    tiny_error_t err = tiny_mat_multc_f32(input, output, C, rows, cols, padd_in, padd_out, step_in, step_out);\n\n    if (err == TINY_OK) {\n        printf(\"Output Matrix Memory Layout (2x3, pad=%d, step=%d, 8 elements):\\n\\r\", padd_out, step_out);\n        printf(\"  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\\n\\r\");\n        printf(\"  Value:  \");\n        for (int i = 0; i &lt; 8; i++) {\n            printf(\"%6.1f \", output[i]);\n        }\n        printf(\"\\n\\r\");\n        printf(\"  Matrix: [%5.1f  %5.1f  %5.1f  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\\n\\r\", \n               output[0], output[1], output[2]);\n        printf(\"          [%5.1f  %5.1f  %5.1f  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\\n\\r\", \n               output[4], output[5], output[6]);\n        printf(\"\\n\\r\");\n\n        // Verify results\n        int all_correct = 1;\n        for (int row = 0; row &lt; rows; row++) {\n            int base_out = row * out_row_stride;\n            for (int col = 0; col &lt; cols; col++) {\n                int idx_out = base_out + col * step_out;\n                float tolerance = 1e-5f;\n                float diff = (output[idx_out] &gt; expected_output[idx_out]) ? \n                            (output[idx_out] - expected_output[idx_out]) : \n                            (expected_output[idx_out] - output[idx_out]);\n                if (diff &gt; tolerance) {\n                    printf(\"  ERROR at [%d][%d] (index %d): output = %10.6f, expected = %10.6f, diff = %e\\n\\r\", \n                           row, col, idx_out, output[idx_out], expected_output[idx_out], diff);\n                    all_correct = 0;\n                }\n            }\n        }\n\n        if (all_correct) {\n            printf(\"\u2713 Test PASSED\\n\\r\");\n        } else {\n            printf(\"\u2717 Test FAILED\\n\\r\");\n        }\n    } else {\n        printf(\"\u2717 Test FAILED: Error code = %d\\n\\r\", err);\n    }\n\n    printf(\"================================================================================\\n\\r\\n\\r\");\n}\n\nvoid tiny_mat_test(void)\n{\n    printf(\"============ [tiny_mat_test] ============\\n\\r\");\n\n    // Test 1: Contiguous matrices (pad=0, step=1) - should use ESP-DSP on ESP32\n    test_tiny_mat_add_f32_contiguous();\n\n    // Test 2: Padded and strided matrices (pad!=0, step&gt;1) - should use own implementation\n    test_tiny_mat_add_f32_padded_strided();\n\n    // Test 3: Contiguous matrix add constant (pad=0, step=1) - should use ESP-DSP on ESP32\n    test_tiny_mat_addc_f32_contiguous();\n\n    // Test 4: Padded and strided matrix add constant (pad!=0, step&gt;1) - should use own implementation\n    test_tiny_mat_addc_f32_padded_strided();\n\n    // Test 5: Contiguous matrices subtraction (pad=0, step=1) - should use ESP-DSP on ESP32\n    test_tiny_mat_sub_f32_contiguous();\n\n    // Test 6: Padded and strided matrices subtraction (pad!=0, step&gt;1) - should use own implementation\n    test_tiny_mat_sub_f32_padded_strided();\n\n    // Test 7: Contiguous matrix subtract constant (pad=0, step=1) - should use ESP-DSP on ESP32\n    test_tiny_mat_subc_f32_contiguous();\n\n    // Test 8: Padded and strided matrix subtract constant (pad!=0, step&gt;1) - should use own implementation\n    test_tiny_mat_subc_f32_padded_strided();\n\n    // Test 9: Basic matrix multiplication (3x4 * 4x2 = 3x2)\n    test_tiny_mat_mult_f32_basic();\n\n    // Test 10: Square matrix multiplication (3x3 * 3x3 = 3x3)\n    test_tiny_mat_mult_f32_square();\n\n    // Test 11: Contiguous matrix multiplication with padding (pad=0) - should use ESP-DSP on ESP32\n    test_tiny_mat_mult_ex_f32_contiguous();\n\n    // Test 12: Padded matrix multiplication (pad!=0) - should use own implementation\n    test_tiny_mat_mult_ex_f32_padded();\n\n    // Test 13: Contiguous matrix multiply constant (pad=0, step=1) - should use ESP-DSP on ESP32\n    test_tiny_mat_multc_f32_contiguous();\n\n    // Test 14: Padded and strided matrix multiply constant (pad!=0, step&gt;1) - should use own implementation\n    test_tiny_mat_multc_f32_padded_strided();\n\n    printf(\"============ [test complete] ============\\n\\r\");\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-mat-test/#maincpp","title":"main.cpp","text":"<pre><code>#include \"tiny_mat_test.hpp\"\n\nextern \"C\" void app_main(void)\n{\n    tiny_mat_test();\n}\n\n\n## TEST RESULTS\n\n```bash\n============ [tiny_mat_test] ============\n\n================================================================================\nTest Case 1: tiny_mat_add_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1\n\nInput1 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nInput2 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5 \n  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\n          [4.5  5.5  6.5  7.5]  &lt;- Row 1\n          [8.5  9.5 10.5 11.5]  &lt;- Row 2\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.5   3.5   5.5   7.5   9.5  11.5  13.5  15.5  17.5  19.5  21.5  23.5 \n  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\n          [9.5 11.5 13.5 15.5]  &lt;- Row 1\n          [17.5 19.5 21.5 23.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.5   3.5   5.5   7.5   9.5  11.5  13.5  15.5  17.5  19.5  21.5  23.5 \n  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\n          [9.5 11.5 13.5 15.5]  &lt;- Row 1\n          [17.5 19.5 21.5 23.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 2: tiny_mat_add_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad1=2, pad2=1, pad_out=2, step1=2, step2=3, step_out=2\nIndex formula: index = row * (cols + padding) + col * step\n\nInput1 Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nInput2 Memory Layout (16 elements, pad=1, step=3):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\n  Value:   0.5  0.0  0.0  1.5  3.5  0.0  2.5  4.5  0.0  0.0  5.5  0.0 ...\n  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\n          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\n          (X = padding/unused)\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.5  0.0  3.5  0.0  5.5  7.5  0.0  9.5  0.0 11.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.5  0.0  3.5  0.0  5.5  7.5  0.0  9.5  0.0 11.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 3: tiny_mat_addc_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1, C=2.5\n\nInput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nConstant C =   2.5\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5  12.5  13.5  14.5 \n  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\n          [7.5  8.5  9.5 10.5]  &lt;- Row 1\n          [11.5 12.5 13.5 14.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5  12.5  13.5  14.5 \n  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\n          [7.5  8.5  9.5 10.5]  &lt;- Row 1\n          [11.5 12.5 13.5 14.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 4: tiny_mat_addc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad_in=2, pad_out=2, step_in=2, step_out=2, C=  1.5\nIndex formula: index = row * (cols + padding) + col * step\n\nInput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nConstant C =   1.5\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   2.5  0.0  3.5  0.0  4.5  5.5  0.0  6.5  0.0  7.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   2.5  0.0  3.5  0.0  4.5  5.5  0.0  6.5  0.0  7.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 5: tiny_mat_sub_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1\n\nInput1 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nInput2 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5 \n  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\n          [4.5  5.5  6.5  7.5]  &lt;- Row 1\n          [8.5  9.5 10.5 11.5]  &lt;- Row 2\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5 \n  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\n          [0.5  0.5  0.5  0.5]  &lt;- Row 1\n          [0.5  0.5  0.5  0.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5 \n  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\n          [0.5  0.5  0.5  0.5]  &lt;- Row 1\n          [0.5  0.5  0.5  0.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 6: tiny_mat_sub_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad1=2, pad2=1, pad_out=2, step1=2, step2=3, step_out=2\nIndex formula: index = row * (cols + padding) + col * step\n\nInput1 Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nInput2 Memory Layout (16 elements, pad=1, step=3):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\n  Value:   0.5  0.0  0.0  1.5  3.5  0.0  2.5  4.5  0.0  0.0  5.5  0.0 ...\n  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\n          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\n          (X = padding/unused)\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   0.5  0.0  0.5  0.0  0.5  0.5  0.0  0.5  0.0  0.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   0.5  0.0  0.5  0.0  0.5  0.5  0.0  0.5  0.0  0.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 7: tiny_mat_subc_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1, C=2.5\n\nInput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nConstant C =   2.5\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:   -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5 \n  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\n          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\n          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:   -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5 \n  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\n          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\n          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 8: tiny_mat_subc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad_in=2, pad_out=2, step_in=2, step_out=2, C=  1.5\nIndex formula: index = row * (cols + padding) + col * step\n\nInput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nConstant C =   1.5\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:  -0.5  0.0  0.5  0.0  1.5  2.5  0.0  3.5  0.0  4.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:  -0.5  0.0  0.5  0.0  1.5  2.5  0.0  3.5  0.0  4.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 9: tiny_mat_mult_f32 - Basic Matrix Multiplication\n================================================================================\nParameters: m=3, n=4, k=2 (A is 3x4, B is 4x2, C is 3x2)\nNote: This function always uses ESP-DSP on ESP32, standard implementation otherwise\n\nMatrix A Memory Layout (3x4, 12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nMatrix B Memory Layout (4x2, 8 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5 \n  Matrix: [0.5  1.5]  &lt;- Row 0\n          [2.5  3.5]  &lt;- Row 1\n          [4.5  5.5]  &lt;- Row 2\n          [6.5  7.5]  &lt;- Row 3\n\nExpected Output Matrix C Memory Layout (3x2, 6 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0\n          [101.0  127.0]  &lt;- Row 1\n          [157.0  199.0] &lt;- Row 2\n  Calculation:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 =  45.0\n    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1] + A[0][3]*B[3][1]\n            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 + 4.0*7.5 =  55.0\n\nOutput Matrix C Memory Layout (3x2, 6 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0\n          [101.0  127.0]  &lt;- Row 1\n          [157.0  199.0] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 10: tiny_mat_mult_f32 - Square Matrix Multiplication\n================================================================================\nParameters: m=3, n=3, k=3 (A is 3x3, B is 3x3, C is 3x3)\nNote: This function always uses ESP-DSP on ESP32, standard implementation otherwise\n\nMatrix A Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0 \n  Matrix: [1.0  2.0  3.0]  &lt;- Row 0\n          [4.0  5.0  6.0]  &lt;- Row 1\n          [7.0  8.0  9.0]  &lt;- Row 2\n\nMatrix B Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    0.5   1.0   1.5   2.0   2.5   3.0   3.5   4.0   4.5 \n  Matrix: [0.5  1.0  1.5]  &lt;- Row 0\n          [2.0  2.5  3.0]  &lt;- Row 1\n          [3.5  4.0  4.5]  &lt;- Row 2\n\nExpected Output Matrix C Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    15.0   18.0   21.0   33.0   40.5   48.0   51.0   63.0   75.0 \n  Matrix: [ 15.0   18.0   21.0]  &lt;- Row 0\n          [ 33.0   40.5   48.0]  &lt;- Row 1\n          [ 51.0   63.0   75.0] &lt;- Row 2\n\nOutput Matrix C Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    15.0   18.0   21.0   33.0   40.5   48.0   51.0   63.0   75.0 \n  Matrix: [ 15.0   18.0   21.0]  &lt;- Row 0\n          [ 33.0   40.5   48.0]  &lt;- Row 1\n          [ 51.0   63.0   75.0] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 11: tiny_mat_mult_ex_f32 - Contiguous Matrix Multiplication\n================================================================================\nParameters: A_rows=3, A_cols=4, B_cols=2, A_padding=0, B_padding=0, C_padding=0\nMatrix dimensions: A is 3x4, B is 4x2, C is 3x2\nNote: This should use ESP-DSP on ESP32 when all paddings are 0\n\nMatrix A Memory Layout (3x4, 12 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0 (indices: 0-3)\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1 (indices: 4-7)\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2 (indices: 8-11)\n  Step size: 4 (A_cols + A_padding = 4 + 0)\n\nMatrix B Memory Layout (4x2, 8 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5 \n  Matrix: [0.5  1.5]  &lt;- Row 0 (indices: 0-1)\n          [2.5  3.5]  &lt;- Row 1 (indices: 2-3)\n          [4.5  5.5]  &lt;- Row 2 (indices: 4-5)\n          [6.5  7.5]  &lt;- Row 3 (indices: 6-7)\n  Step size: 2 (B_cols + B_padding = 2 + 0)\n\nExpected Output Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0 (indices: 0-1)\n          [101.0  127.0]  &lt;- Row 1 (indices: 2-3)\n          [157.0  199.0] &lt;- Row 2 (indices: 4-5)\n  Step size: 2 (B_cols + C_padding = 2 + 0)\n  Calculation example:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 =  45.0\n\nOutput Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0 (indices: 0-1)\n          [101.0  127.0]  &lt;- Row 1 (indices: 2-3)\n          [157.0  199.0] &lt;- Row 2 (indices: 4-5)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 12: tiny_mat_mult_ex_f32 - Padded Matrix Multiplication\n================================================================================\nParameters: A_rows=2, A_cols=3, B_cols=2, A_padding=2, B_padding=1, C_padding=1\nMatrix dimensions: A is 2x3, B is 3x2, C is 2x2\nNote: This should use own implementation when padding is non-zero\n\nMatrix A Memory Layout (2x3, pad=2, step=5, 10 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\n  Value:   1.0  2.0  3.0  0.0  0.0  4.0  5.0  6.0  0.0  0.0 \n  Matrix: [1.0  2.0  3.0  X   X]  &lt;- Row 0 (indices: 0, 1, 2, 3, 4)\n          [4.0  5.0  6.0  X   X]  &lt;- Row 1 (indices: 5, 6, 7, 8, 9)\n          (X = padding/unused)\n  Index calculation: A[i][j] = A[i * 5 + j]\n    Row 0: indices 0, 1, 2 (data), 3, 4 (padding)\n    Row 1: indices 5, 6, 7 (data), 8, 9 (padding)\n\nMatrix B Memory Layout (3x2, pad=1, step=3, 9 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:   0.5  1.5  0.0  2.5  3.5  0.0  4.5  5.5  0.0 \n  Matrix: [0.5  1.5  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [2.5  3.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n          [4.5  5.5  X]  &lt;- Row 2 (indices: 6, 7, 8)\n          (X = padding/unused)\n  Index calculation: B[i][j] = B[i * 3 + j]\n    Row 0: indices 0, 1 (data), 2 (padding)\n    Row 1: indices 3, 4 (data), 5 (padding)\n    Row 2: indices 6, 7 (data), 8 (padding)\n\nExpected Output Matrix C Memory Layout (2x2, pad=1, step=3, 6 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    19.0   25.0    0.0   41.5   56.5    0.0 \n  Matrix: [ 19.0   25.0  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 41.5   56.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n          (X = padding/unused)\n  Index calculation: C[i][j] = C[i * 3 + j]\n  Calculation:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0]\n            = A[0]*B[0] + A[1]*B[3] + A[2]*B[6]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 =  19.0\n    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1]\n            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 =  25.0\n    C[1][0] = A[1][0]*B[0][0] + A[1][1]*B[1][0] + A[1][2]*B[2][0]\n            = 4.0*0.5 + 5.0*2.5 + 6.0*4.5 =  41.5\n    C[1][1] = A[1][0]*B[0][1] + A[1][1]*B[1][1] + A[1][2]*B[2][1]\n            = 4.0*1.5 + 5.0*3.5 + 6.0*5.5 =  56.5\n\nOutput Matrix C Memory Layout (2x2, pad=1, step=3, 6 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    19.0   25.0    0.0   41.5   56.5    0.0 \n  Matrix: [ 19.0   25.0  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 41.5   56.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 13: tiny_mat_multc_f32 - Contiguous Matrix Multiply Constant\n================================================================================\nParameters: rows=3, cols=3, padd_in=0, padd_out=0, step_in=1, step_out=1\nMatrix dimensions: 3x3\nConstant C: 2.5\nNote: This should use ESP-DSP on ESP32 when all paddings are 0 and all steps are 1\n\nInput Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0 \n  Matrix: [1.0  2.0  3.0]  &lt;- Row 0 (indices: 0, 1, 2)\n          [4.0  5.0  6.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [7.0  8.0  9.0]  &lt;- Row 2 (indices: 6, 7, 8)\n  Row stride: 3 (cols + padd_in = 3 + 0)\n  Index calculation: input[i][j] = input[i * 3 + j * 1]\n\nExpected Output Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:     2.5    5.0    7.5   10.0   12.5   15.0   17.5   20.0   22.5 \n  Matrix: [  2.5    5.0    7.5]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 10.0   12.5   15.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [ 17.5   20.0   22.5] &lt;- Row 2 (indices: 6, 7, 8)\n  Row stride: 3 (cols + padd_out = 3 + 0)\n  Index calculation: output[i][j] = output[i * 3 + j * 1]\n  Calculation: output[i][j] = input[i][j] * 2.5\n    Example: output[0][0] = input[0][0] * 2.5 = 1.0 * 2.5 = 2.5\n\nOutput Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:     2.5    5.0    7.5   10.0   12.5   15.0   17.5   20.0   22.5 \n  Matrix: [  2.5    5.0    7.5]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 10.0   12.5   15.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [ 17.5   20.0   22.5] &lt;- Row 2 (indices: 6, 7, 8)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 14: tiny_mat_multc_f32 - Padded and Strided Matrix Multiply Constant\n================================================================================\nParameters: rows=2, cols=3, padd_in=2, padd_out=1, step_in=2, step_out=1\nMatrix dimensions: 2x3\nConstant C: 3.0\nNote: This should use own implementation when padding is non-zero or step &gt; 1\n\nInput Matrix Memory Layout (2x3, pad=2, step=2, 10 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0 \n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (data indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (data indices: 5, 7, 9)\n          (X = unused/padding)\n  Row stride: 5 (cols + padd_in = 3 + 2)\n  Index calculation: input[i][j] = input[i * 5 + j * 2]\n    Row 0: input[0][0]=input[0]=1.0, input[0][1]=input[2]=2.0, input[0][2]=input[4]=3.0\n    Row 1: input[1][0]=input[5]=4.0, input[1][1]=input[7]=5.0, input[1][2]=input[9]=6.0\n\nExpected Output Matrix Memory Layout (2x3, pad=1, step=1, 8 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:     3.0    6.0    9.0    0.0   12.0   15.0   18.0    0.0 \n  Matrix: [  3.0    6.0    9.0  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\n          [ 12.0   15.0   18.0  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\n          (X = padding/unused)\n  Row stride: 4 (cols + padd_out = 3 + 1)\n  Index calculation: output[i][j] = output[i * 4 + j * 1]\n  Calculation: output[i][j] = input[i][j] * 3.0\n    Row 0: output[0][0] = input[0][0] * 3.0 = 1.0 * 3.0 = 3.0 (index 0)\n           output[0][1] = input[0][1] * 3.0 = 2.0 * 3.0 = 6.0 (index 1)\n           output[0][2] = input[0][2] * 3.0 = 3.0 * 3.0 = 9.0 (index 2)\n\nOutput Matrix Memory Layout (2x3, pad=1, step=1, 8 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:     3.0    6.0    9.0    0.0   12.0   15.0   18.0    0.0 \n  Matrix: [  3.0    6.0    9.0  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\n          [ 12.0   15.0   18.0  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\n\n\u2713 Test PASSED\n================================================================================\n\n============ [test complete] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-api/","title":"MATRIX OPERATIONS - TINY_MATRIX","text":"<p>TINY_MATRIX Library</p> <ul> <li>This library is a lightweight matrix computation library implemented in C++, providing basic matrix operations and linear algebra functions.</li> <li>The design goal of this library is to provide a simple and easy-to-use matrix operation interface, suitable for embedded systems and resource-constrained environments.</li> </ul> <p>Usage Scenario</p> <p>Compared to the TINY_MAT library, the TINY_MATRIX library offers richer functionality and higher flexibility, suitable for applications that require complex matrix computations. However, please note that this library is written in C++.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#list-of-functions","title":"LIST OF FUNCTIONS","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500Vector\n    \u2514\u2500\u2500Matrix\n        \u251c\u2500\u2500 tiny_mat (c)\n        \u2514\u2500\u2500 tiny_matrix (c++) &lt;---\n</code></pre> <pre><code>/**\n * @file tiny_matrix.hpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @note This file is built on top of the mat.h file from the ESP-DSP library.\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// TinyMath\n#include \"tiny_math_config.h\"\n#include \"tiny_vec.h\"\n#include \"tiny_mat.h\"\n\n// Standard Libraries\n#include &lt;iostream&gt;\n#include &lt;stdint.h&gt;\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n// ESP32 DSP C++ Matrix library\n#include \"mat.h\"\n#endif\n\n/* STATEMENTS */\nnamespace tiny\n{\n    class Mat\n    {\n    public:\n        // ============================================================================\n        // Matrix Metadata\n        // ============================================================================\n        int row;         //&lt; number of rows\n        int col;         //&lt; number of columns\n        int pad;         //&lt; number of paddings between 2 rows\n        int stride;      //&lt; stride = (number of elements in a row) + padding\n        int element;     //&lt; number of elements = rows * cols\n        int memory;      //&lt; size of the data buffer = rows * stride\n        float *data;     //&lt; pointer to the data buffer\n        float *temp;     //&lt; pointer to the temporary data buffer\n        bool ext_buff;   //&lt; flag indicates that matrix use external buffer\n        bool sub_matrix; //&lt; flag indicates that matrix is a subset of another matrix\n\n        // ============================================================================\n        // Rectangular ROI Structure\n        // ============================================================================\n        /**\n         * @name Region of Interest (ROI) Structure\n         * @brief This is the structure for ROI\n         */\n        struct ROI\n        {\n            int pos_x;  ///&lt; starting column index\n            int pos_y;  ///&lt; starting row index\n            int width;  ///&lt; width of ROI (columns)\n            int height; ///&lt; height of ROI (rows)\n\n            ROI(int pos_x = 0, int pos_y = 0, int width = 0, int height = 0);\n            void resize_roi(int pos_x, int pos_y, int width, int height);\n            int area_roi(void) const;\n        };\n\n        // ============================================================================\n        // Printing Functions\n        // ============================================================================\n        void print_info() const;\n        void print_matrix(bool show_padding);\n\n        // ============================================================================\n        // Constructors &amp; Destructor\n        // ============================================================================\n        void alloc_mem(); // Allocate internal memory\n        Mat();\n        Mat(int rows, int cols);\n        Mat(int rows, int cols, int stride);\n        Mat(float *data, int rows, int cols);\n        Mat(float *data, int rows, int cols, int stride);\n        Mat(const Mat &amp;src);\n        ~Mat();\n\n        // ============================================================================\n        // Element Access\n        // ============================================================================\n        inline float &amp;operator()(int row, int col) { return data[row * stride + col]; }\n        inline const float &amp;operator()(int row, int col) const { return data[row * stride + col]; }\n\n        // ============================================================================\n        // Data Manipulation\n        // ============================================================================\n        tiny_error_t copy_paste(const Mat &amp;src, int row_pos, int col_pos);\n        tiny_error_t copy_head(const Mat &amp;src);\n        Mat view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const;\n        Mat view_roi(const Mat::ROI &amp;roi) const;\n        Mat copy_roi(int start_row, int start_col, int roi_rows, int roi_cols);\n        Mat copy_roi(const Mat::ROI &amp;roi);\n        Mat block(int start_row, int start_col, int block_rows, int block_cols);\n        void swap_rows(int row1, int row2);\n        void swap_cols(int col1, int col2);\n        void clear(void);\n\n        // ============================================================================\n        // Arithmetic Operators\n        // ============================================================================\n        Mat &amp;operator=(const Mat &amp;src);    // Copy assignment\n        Mat &amp;operator+=(const Mat &amp;A);     // Add matrix\n        Mat &amp;operator+=(float C);          // Add constant\n        Mat &amp;operator-=(const Mat &amp;A);     // Subtract matrix\n        Mat &amp;operator-=(float C);          // Subtract constant \n        Mat &amp;operator*=(const Mat &amp;A);     // Multiply matrix\n        Mat &amp;operator*=(float C);          // Multiply constant\n        Mat &amp;operator/=(const Mat &amp;B);     // Divide matrix\n        Mat &amp;operator/=(float C);          // Divide constant\n        Mat operator^(int C);              // Exponentiation\n\n        // ============================================================================\n        // Linear Algebra - Basic Operations\n        // ============================================================================\n        Mat transpose();                   // Transpose matrix\n        float determinant();               // Compute determinant (auto-selects method based on size)\n        float determinant_laplace();        // Compute determinant using Laplace expansion (O(n!), for small matrices)\n        float determinant_lu();            // Compute determinant using LU decomposition (O(n\u00b3), efficient for large matrices)\n        float determinant_gaussian();      // Compute determinant using Gaussian elimination (O(n\u00b3), efficient for large matrices)\n        Mat adjoint();                     // Compute adjoint matrix\n        Mat inverse_adjoint();            // Compute inverse using adjoint method\n        void normalize();                  // Normalize matrix\n        float norm() const;                // Compute matrix norm\n        float dotprod(const Mat &amp;A, const Mat &amp;B);  // Dot product\n\n        // ============================================================================\n        // Linear Algebra - Matrix Utilities\n        // ============================================================================\n        static Mat eye(int size);          // Create identity matrix\n        static Mat ones(int rows, int cols);  // Create matrix filled with ones\n        static Mat ones(int size);         // Create square matrix filled with ones\n        static Mat augment(const Mat &amp;A, const Mat &amp;B);  // Horizontal concatenation [A | B]\n        static Mat vstack(const Mat &amp;A, const Mat &amp;B);   // Vertical concatenation [A; B]\n\n        /**\n         * @brief Gram-Schmidt orthogonalization process\n         * @note Orthogonalizes a set of vectors using the Gram-Schmidt process\n         * @param vectors Input matrix where each column is a vector to be orthogonalized\n         * @param orthogonal_vectors Output matrix for orthogonalized vectors (each column is orthogonal)\n         * @param coefficients Output matrix for projection coefficients (R matrix in QR decomposition)\n         * @param tolerance Minimum norm threshold for linear independence check\n         * @return true if successful, false if input is invalid\n         */\n        static bool gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                               Mat &amp;coefficients, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Linear Algebra - Matrix Operations\n        // ============================================================================\n        Mat minor(int row, int col);       // Minor matrix (submatrix after removing row and col)\n        Mat cofactor(int row, int col);    // Cofactor matrix\n        Mat gaussian_eliminate() const;    // Gaussian elimination\n        Mat row_reduce_from_gaussian();   // Row reduction from Gaussian form\n        Mat inverse_gje();                 // Inverse using Gaussian-Jordan elimination\n\n        // ============================================================================\n        // Linear Algebra - Linear System Solving\n        // ============================================================================\n        Mat solve(const Mat &amp;A, const Mat &amp;b) const;  // Solve Ax = b using Gaussian elimination\n        Mat band_solve(Mat A, Mat b, int k);          // Solve banded system\n        Mat roots(Mat A, Mat y);                      // Alternative solve method\n\n        // ============================================================================\n        // Matrix Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct LUDecomposition;\n        struct CholeskyDecomposition;\n        struct QRDecomposition;\n        struct SVDDecomposition;\n\n        // Matrix property checks\n        bool is_symmetric(float tolerance = 1e-6f) const;\n        bool is_positive_definite(float tolerance = 1e-6f) const;\n\n        // Decomposition methods\n        LUDecomposition lu_decompose(bool use_pivoting = true) const;\n        CholeskyDecomposition cholesky_decompose() const;\n        QRDecomposition qr_decompose() const;\n        SVDDecomposition svd_decompose(int max_iter = 100, float tolerance = 1e-6f) const;\n\n        // Solve using decomposition (more efficient for multiple RHS)\n        static Mat solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b);\n        static Mat solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b);\n        static Mat solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b);  // Least squares solution\n\n        // Pseudo-inverse using SVD (for rank-deficient or non-square matrices)\n        static Mat pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Eigenvalue &amp; Eigenvector Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct EigenPair;\n        struct EigenDecomposition;\n\n        // Single eigenvalue methods (fast, for real-time applications)\n        EigenPair power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n        EigenPair inverse_power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n\n        // Complete eigendecomposition methods\n        EigenDecomposition eigendecompose_jacobi(float tolerance = 1e-6f, int max_iter = 100) const;\n        EigenDecomposition eigendecompose_qr(int max_iter = 100, float tolerance = 1e-6f) const;\n        EigenDecomposition eigendecompose(float tolerance = 1e-6f) const;  // Auto-select method\n\n    protected:\n\n    private:\n\n    };\n\n    // ============================================================================\n    // Matrix Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold LU decomposition results\n     * @note A = L * U, where L is lower triangular and U is upper triangular\n     */\n    struct Mat::LUDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix (with unit diagonal)\n        Mat U;                 ///&lt; Upper triangular matrix\n        Mat P;                 ///&lt; Permutation matrix (if pivoting used)\n        bool pivoted;          ///&lt; Whether pivoting was used\n        tiny_error_t status;   ///&lt; Computation status\n\n        LUDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold Cholesky decomposition results\n     * @note A = L * L^T, where L is lower triangular (for symmetric positive definite matrices)\n     */\n    struct Mat::CholeskyDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        CholeskyDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold QR decomposition results\n     * @note A = Q * R, where Q is orthogonal and R is upper triangular\n     */\n    struct Mat::QRDecomposition\n    {\n        Mat Q;                 ///&lt; Orthogonal matrix (Q^T * Q = I)\n        Mat R;                 ///&lt; Upper triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        QRDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold SVD decomposition results\n     * @note A = U * S * V^T, where U and V are orthogonal, S is diagonal (singular values)\n     */\n    struct Mat::SVDDecomposition\n    {\n        Mat U;                 ///&lt; Left singular vectors (orthogonal matrix)\n        Mat S;                 ///&lt; Singular values (diagonal matrix or vector)\n        Mat V;                 ///&lt; Right singular vectors (orthogonal matrix, V^T)\n        int rank;              ///&lt; Numerical rank of the matrix\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        SVDDecomposition();\n    };\n\n    // ============================================================================\n    // Eigenvalue &amp; Eigenvector Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold a single eigenvalue-eigenvector pair\n     * @note Used primarily for power iteration method\n     */\n    struct Mat::EigenPair\n    {\n        float eigenvalue;      ///&lt; Eigenvalue (real part)\n        Mat eigenvector;       ///&lt; Corresponding eigenvector (column vector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenPair();\n    };\n\n    /**\n     * @brief Structure to hold complete eigenvalue decomposition results\n     * @note Contains all eigenvalues and eigenvectors\n     */\n    struct Mat::EigenDecomposition\n    {\n        Mat eigenvalues;       ///&lt; Eigenvalues (diagonal matrix or vector)\n        Mat eigenvectors;      ///&lt; Eigenvector matrix (each column is an eigenvector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenDecomposition();\n    };\n\n    // ============================================================================\n    // Stream Operators\n    // ============================================================================\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m);\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi);\n    std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m);\n\n    // ============================================================================\n    // Global Arithmetic Operators\n    // ============================================================================\n    Mat operator+(const Mat &amp;A, const Mat &amp;B);\n    Mat operator+(const Mat &amp;A, float C);\n    Mat operator-(const Mat &amp;A, const Mat &amp;B);\n    Mat operator-(const Mat &amp;A, float C);\n    Mat operator*(const Mat &amp;A, const Mat &amp;B);\n    Mat operator*(const Mat &amp;A, float C);\n    Mat operator*(float C, const Mat &amp;A);\n    Mat operator/(const Mat &amp;A, float C);\n    Mat operator/(const Mat &amp;A, const Mat &amp;B);\n    bool operator==(const Mat &amp;A, const Mat &amp;B);\n\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-metadata","title":"MATRIX METADATA","text":"<p>Matrix Structure</p> <p>The Mat class uses a row-major storage layout with support for padding and stride. This design enables efficient memory access patterns and compatibility with DSP libraries.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#core-dimensions","title":"Core Dimensions","text":"<ul> <li> <p><code>int row</code> : Number of rows in the matrix.</p> </li> <li> <p><code>int col</code> : Number of columns in the matrix.</p> </li> <li> <p><code>int element</code> : Total number of elements = rows \u00d7 cols.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#memory-layout","title":"Memory Layout","text":"<ul> <li> <p><code>int stride</code> : Stride = (number of elements in a row) + padding. The stride determines how many elements to skip to move to the next row in memory.</p> </li> <li> <p><code>int pad</code> : Number of padding elements between two rows. Padding is used for memory alignment and DSP optimization.</p> </li> <li> <p><code>int memory</code> : Size of the data buffer = rows \u00d7 stride (in number of float elements).</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#data-pointers","title":"Data Pointers","text":"<ul> <li> <p><code>float *data</code> : Pointer to the data buffer containing matrix elements. Elements are stored in row-major order: element at (i, j) is at <code>data[i * stride + j]</code>.</p> </li> <li> <p><code>float *temp</code> : Pointer to the temporary data buffer (if allocated). Used internally for certain operations.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#memory-management-flags","title":"Memory Management Flags","text":"<ul> <li> <p><code>bool ext_buff</code> : Flag indicating that the matrix uses an external buffer. When <code>true</code>, the destructor will not free the memory (caller is responsible).</p> </li> <li> <p><code>bool sub_matrix</code> : Flag indicating that the matrix is a subset/view of another matrix. When <code>true</code>, the matrix shares data with the parent matrix.</p> </li> </ul> <p>Memory Layout Example</p> <p>For a 3\u00d74 matrix with stride=4 (no padding): <pre><code>[a b c d]   row 0: data[0*4+0] to data[0*4+3]\n[e f g h]   row 1: data[1*4+0] to data[1*4+3]\n[i j k l]   row 2: data[2*4+0] to data[2*4+3]\n</code></pre></p> <p>For a 3\u00d74 matrix with stride=6 (padding=2): <pre><code>[a b c d _ _]   row 0: data[0*6+0] to data[0*6+3], padding at data[0*6+4,5]\n[e f g h _ _]   row 1: data[1*6+0] to data[1*6+3], padding at data[1*6+4,5]\n[i j k l _ _]   row 2: data[2*6+0] to data[2*6+3], padding at data[2*6+4,5]\n</code></pre></p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roi-structure","title":"ROI STRUCTURE","text":"<p>Region of Interest</p> <p>The ROI (Region of Interest) structure represents a rectangular subregion of a matrix. It's used with <code>view_roi()</code> and <code>copy_roi()</code> functions to extract or reference submatrices efficiently.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roi-metadata","title":"ROI Metadata","text":"<ul> <li> <p><code>int pos_x</code> : Starting column index (x-coordinate of the top-left corner).</p> </li> <li> <p><code>int pos_y</code> : Starting row index (y-coordinate of the top-left corner).</p> </li> <li> <p><code>int width</code> : Width of the ROI in columns.</p> </li> <li> <p><code>int height</code> : Height of the ROI in rows.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roi-constructor","title":"ROI Constructor","text":"<pre><code>Mat::ROI::ROI(int pos_x = 0, int pos_y = 0, int width = 0, int height = 0);\n</code></pre> <p>Description: </p> <p>ROI constructor initializes the ROI with the specified position and size.</p> <p>Parameters:</p> <ul> <li> <p><code>int pos_x</code> : Starting column index.</p> </li> <li> <p><code>int pos_y</code> : Starting row index.</p> </li> <li> <p><code>int width</code> : Width of the ROI (columns).</p> </li> <li> <p><code>int height</code> : Height of the ROI (rows).</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roi-resize","title":"ROI RESIZE","text":"<pre><code>void Mat::ROI::resize_roi(int pos_x, int pos_y, int width, int height);\n</code></pre> <p>Description: </p> <p>Resizes the ROI to the specified position and size.</p> <p>Parameters:</p> <ul> <li> <p><code>int pos_x</code> : Starting column index.</p> </li> <li> <p><code>int pos_y</code> : Starting row index.</p> </li> <li> <p><code>int width</code> : Width of the ROI (columns).</p> </li> <li> <p><code>int height</code> : Height of the ROI (rows).</p> </li> </ul> <p>Returns:</p> <p>void</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#area-roi","title":"AREA ROI","text":"<pre><code>int Mat::ROI::area_roi(void) const;\n</code></pre> <p>Description: </p> <p>Calculates the area of the ROI.</p> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>int - Area of the ROI.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#printing-functions","title":"PRINTING FUNCTIONS","text":"<p>Debugging Tools</p> <p>These functions are essential for debugging and understanding matrix state. Use them to verify matrix dimensions, memory layout, and data values during development.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#print-matrix-information","title":"Print Matrix Information","text":"<pre><code>void Mat::print_info() const;\n</code></pre> <p>Description: </p> <p>Prints comprehensive matrix information including:</p> <ul> <li> <p>Dimensions: rows, columns, elements</p> </li> <li> <p>Memory layout: paddings, stride, memory size</p> </li> <li> <p>Pointers: data buffer address, temporary buffer address</p> </li> <li> <p>Flags: external buffer usage, sub-matrix status</p> </li> <li> <p>Warnings: dimension mismatches, invalid states</p> </li> </ul> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>void</p> <p>Usage Insights:</p> <ul> <li> <p>Debugging: Essential for verifying matrix state and detecting memory issues.</p> </li> <li> <p>Memory Analysis: Shows actual memory usage vs. logical size, helping identify memory inefficiencies.</p> </li> <li> <p>Sub-Matrix Detection: Clearly indicates if a matrix is a view, which affects memory management.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#print-matrix-elements","title":"Print Matrix Elements","text":"<pre><code>void Mat::print_matrix(bool show_padding);\n</code></pre> <p>Description: </p> <p>Prints the matrix elements in a formatted table. Optionally displays padding elements separated by a visual separator.</p> <p>Parameters: </p> <ul> <li><code>bool show_padding</code> : If <code>true</code>, displays padding values with a separator <code>|</code>. If <code>false</code>, only shows actual matrix elements.</li> </ul> <p>Returns:</p> <p>void</p> <p>Usage Insights:</p> <ul> <li> <p>Formatting: Elements are formatted with fixed width (12 characters) for alignment.</p> </li> <li> <p>Padding Visualization: The <code>show_padding</code> option helps understand memory layout and verify padding values.</p> </li> <li> <p>Large Matrices: For very large matrices, consider using <code>view_roi()</code> to print specific regions.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#constructors-destructor","title":"CONSTRUCTORS &amp; DESTRUCTOR","text":"<p>Memory Management</p> <p>Constructors handle memory allocation automatically. The destructor safely frees memory only if it was internally allocated (not external buffers or views). Always check the <code>data</code> pointer after construction to ensure successful allocation.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#memory-allocation","title":"Memory Allocation","text":"<pre><code>void Mat::alloc_mem();\n</code></pre> <p>Description: </p> <p>Internal function that allocates memory for the matrix according to the computed memory requirements. Sets <code>ext_buff = false</code> and allocates <code>row * stride</code> float elements.</p> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>void</p> <p>Usage Insights:</p> <ul> <li> <p>Automatic Call: Called automatically by constructors. Rarely needs manual invocation.</p> </li> <li> <p>Memory Calculation: Allocates <code>row * stride</code> elements, which may include padding.</p> </li> <li> <p>Error Handling: If allocation fails, <code>data</code> remains <code>nullptr</code>. Always check <code>data</code> after construction.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#default-constructor","title":"Default Constructor","text":"<pre><code>Mat::Mat();\n</code></pre> <p>Description: </p> <p>Default constructor creates a 1\u00d71 zero matrix. This is useful for initialization and as a return value for error cases.</p> <p>Mathematical Principle:</p> <p>Creates the identity element for matrix operations in some contexts, though typically you'll want to specify dimensions.</p> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>Mat - A 1\u00d71 matrix with element = 0.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#constructor-matint-rows-int-cols","title":"Constructor - Mat(int rows, int cols)","text":"<pre><code>Mat::Mat(int rows, int cols);\n</code></pre> <p>Description: </p> <p>Constructor creates a matrix with specified dimensions. All elements are initialized to zero. This is the most commonly used constructor.</p> <p>Parameters:</p> <ul> <li> <p><code>int rows</code> : Number of rows (must be &gt; 0).</p> </li> <li> <p><code>int cols</code> : Number of columns (must be &gt; 0).</p> </li> </ul> <p>Returns:</p> <p>Mat - A rows\u00d7cols matrix with all elements initialized to 0.</p> <p>Usage Insights:</p> <ul> <li> <p>Zero Initialization: All elements are set to zero using <code>memset</code>, ensuring clean state.</p> </li> <li> <p>Memory Layout: Creates a contiguous memory layout with no padding (stride = cols).</p> </li> <li> <p>Error Handling: If memory allocation fails, <code>data</code> will be <code>nullptr</code>. Always verify allocation success.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#constructor-matint-rows-int-cols-int-stride","title":"Constructor - Mat(int rows, int cols, int stride)","text":"<pre><code>Mat::Mat(int rows, int cols, int stride);\n</code></pre> <p>Description: </p> <p>Constructor creates a matrix with specified dimensions and stride. Useful when you need padding for memory alignment or DSP optimization.</p> <p>Parameters:</p> <ul> <li> <p><code>int rows</code> : Number of rows.</p> </li> <li> <p><code>int cols</code> : Number of columns.</p> </li> <li> <p><code>int stride</code> : Stride (must be \u2265 cols). Padding = stride - cols.</p> </li> </ul> <p>Returns:</p> <p>Mat - A rows\u00d7cols matrix with stride, all elements initialized to 0.</p> <p>Usage Insights:</p> <ul> <li> <p>DSP Optimization: Some DSP libraries require aligned memory. Use stride to ensure proper alignment.</p> </li> <li> <p>Memory Efficiency: Padding allows efficient vectorized operations on aligned boundaries.</p> </li> <li> <p>Compatibility: Enables compatibility with external libraries that use strided memory layouts.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#constructor-matfloat-data-int-rows-int-cols","title":"Constructor - Mat(float *data, int rows, int cols)","text":"<pre><code>Mat::Mat(float *data, int rows, int cols);\n</code></pre> <p>Description: </p> <p>Constructor creates a matrix view over an external data buffer. The matrix does not own the memory; the caller is responsible for managing it. Useful for interfacing with existing data arrays.</p> <p>Parameters:</p> <ul> <li> <p><code>float *data</code> : Pointer to external data buffer (must remain valid for matrix lifetime).</p> </li> <li> <p><code>int rows</code> : Number of rows.</p> </li> <li> <p><code>int cols</code> : Number of columns.</p> </li> </ul> <p>Returns:</p> <p>Mat - A matrix view with <code>ext_buff = true</code>.</p> <p>Usage Insights:</p> <ul> <li> <p>Zero-Copy: No memory copy occurs; the matrix directly references external data.</p> </li> <li> <p>Lifetime Management: The external buffer must remain valid while the matrix exists. The destructor will not free this memory.</p> </li> <li> <p>Data Layout: Assumes row-major layout with no padding (stride = cols).</p> </li> <li> <p>Use Cases: </p> </li> <li> <p>Wrapping C arrays</p> </li> <li> <p>Interfacing with other libraries</p> </li> <li> <p>Avoiding unnecessary copies</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#constructor-matfloat-data-int-rows-int-cols-int-stride","title":"Constructor - Mat(float *data, int rows, int cols, int stride)","text":"<pre><code>Mat::Mat(float *data, int rows, int cols, int stride);\n</code></pre> <p>Description: </p> <p>Constructor creates a matrix view over an external data buffer with specified stride. Supports strided memory layouts for DSP compatibility.</p> <p>Parameters:</p> <ul> <li> <p><code>float *data</code> : Pointer to external data buffer (must remain valid for matrix lifetime).</p> </li> <li> <p><code>int rows</code> : Number of rows.</p> </li> <li> <p><code>int cols</code> : Number of columns.</p> </li> <li> <p><code>int stride</code> : Stride (must be \u2265 cols).</p> </li> </ul> <p>Returns:</p> <p>Mat - A matrix view with <code>ext_buff = true</code> and specified stride.</p> <p>Usage Insights:</p> <ul> <li> <p>Strided Layouts: Essential for working with DSP libraries that use strided memory layouts.</p> </li> <li> <p>Memory Safety: Same lifetime requirements as the previous constructor - external buffer must remain valid.</p> </li> <li> <p>Padding Support: Can handle buffers with padding between rows.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#copy-constructor-matconst-mat-src","title":"Copy Constructor - Mat(const Mat &amp;src)","text":"<pre><code>Mat::Mat(const Mat &amp;src);\n</code></pre> <p>Description: </p> <p>Copy constructor creates a new matrix from a source matrix. Uses intelligent copying: deep copy for regular matrices, shallow copy for sub-matrix views.</p> <p>Copy Strategy:</p> <ul> <li> <p>Regular matrices: Deep copy - allocates new memory and copies all data</p> </li> <li> <p>Sub-matrix views: Shallow copy - shares data with source (creates another view)</p> </li> </ul> <p>Parameters:</p> <ul> <li><code>const Mat &amp;src</code> : Source matrix.</li> </ul> <p>Returns:</p> <p>Mat - A new matrix with copied or shared data depending on source type.</p> <p>Usage Insights:</p> <ul> <li> <p>Automatic Selection: Automatically chooses deep or shallow copy based on source matrix type.</p> </li> <li> <p>Memory Efficiency: Sub-matrix views are copied shallowly to avoid unnecessary memory allocation.</p> </li> <li> <p>Independence: Deep copies are independent; modifications don't affect the source.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#destructor","title":"Destructor","text":"<pre><code>Mat::~Mat();\n</code></pre> <p>Description: </p> <p>Destructor safely releases allocated memory. Only frees memory if it was internally allocated (<code>ext_buff = false</code>). External buffers and views are not freed.</p> <p>Memory Management:</p> <ul> <li> <p>Frees <code>data</code> buffer if <code>ext_buff = false</code></p> </li> <li> <p>Frees <code>temp</code> buffer if allocated</p> </li> <li> <p>Does nothing for external buffers or views</p> </li> </ul> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>void</p> <p>Constructor and Destructor Rules</p> <ul> <li>Constructor functions must have the same name as the class and no return type</li> <li>C++ allows function overloading by changing parameter number/order</li> <li>The destructor is automatically called when the object goes out of scope</li> <li>Always check <code>data != nullptr</code> after construction to verify successful allocation</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#element-access","title":"ELEMENT ACCESS","text":"<p>Matrix Indexing</p> <p>The Mat class uses operator overloading to provide intuitive matrix element access. The <code>operator()</code> allows natural syntax like <code>A(i, j)</code> instead of <code>A.data[i * stride + j]</code>. The implementation automatically handles stride and padding.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#access-matrix-elements-non-const","title":"Access Matrix Elements (Non-Const)","text":"<pre><code>inline float &amp;operator()(int row, int col);\n</code></pre> <p>Description:</p> <p>Accesses matrix elements with read-write capability. Returns a reference to the element, allowing both reading and modification.</p> <p>Mathematical Principle:</p> <p>Element at position (row, col) is accessed as <code>data[row * stride + col]</code>, where stride accounts for padding.</p> <p>Parameters\uff1a</p> <ul> <li> <p><code>int row</code> : Row index (0-based, must be in range [0, row-1]).</p> </li> <li> <p><code>int col</code> : Column index (0-based, must be in range [0, col-1]).</p> </li> </ul> <p>Returns:</p> <p><code>float&amp;</code> - Reference to the matrix element, enabling modification.</p> <p>Usage Insights:</p> <ul> <li> <p>Bounds Checking: No automatic bounds checking for performance. Ensure indices are valid.</p> </li> <li> <p>Stride Handling: Automatically accounts for stride, so it works correctly with padded matrices.</p> </li> <li> <p>Performance: Inline function with minimal overhead, suitable for tight loops.</p> </li> <li> <p>Example: <code>A(2, 3) = 5.0f;</code> sets element at row 2, column 3 to 5.0.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#access-matrix-elements-const","title":"Access Matrix Elements (Const)","text":"<pre><code>inline const float &amp;operator()(int row, int col) const;\n</code></pre> <p>Description:</p> <p>Accesses matrix elements in read-only mode. Returns a const reference, preventing modification. Used when the matrix is const.</p> <p>Parameters\uff1a</p> <ul> <li> <p><code>int row</code> : Row index (0-based).</p> </li> <li> <p><code>int col</code> : Column index (0-based).</p> </li> </ul> <p>Returns:</p> <p><code>const float&amp;</code> - Const reference to the matrix element (read-only).</p> <p>Usage Insights:</p> <ul> <li> <p>Const Correctness: Enables proper const-correct code. Use this version in const member functions.</p> </li> <li> <p>Safety: Prevents accidental modification of const matrices.</p> </li> </ul> <p>Operator Overloading</p> <p>These functions overload the <code>()</code> operator, enabling natural matrix indexing syntax: <pre><code>Mat A(3, 4);\nA(1, 2) = 3.14f;        // Write access\nfloat val = A(1, 2);    // Read access\nconst Mat&amp; B = A;\nfloat val2 = B(1, 2);   // Read-only access (uses const version)\n</code></pre></p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#data-manipulation","title":"DATA MANIPULATION","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#copy-other-matrix-into-this-matrix-as-a-sub-matrix","title":"Copy other matrix into this matrix as a sub-matrix","text":"<pre><code>tiny_error_t Mat::copy_paste(const Mat &amp;src, int row_pos, int col_pos);\n</code></pre> <p>Description:</p> <p>Copies the specified source matrix into this matrix as a sub-matrix starting from the specified row and column positions, not sharing the data buffer.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;src</code> : Source matrix.</p> </li> <li> <p><code>int row_pos</code> : Starting row position.</p> </li> <li> <p><code>int col_pos</code> : Starting column position.</p> </li> </ul> <p>Returns:</p> <p>tiny_error_t - Error code (TINY_OK on success).</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#copy-header-of-other-matrix-to-this-matrix","title":"Copy header of other matrix to this matrix","text":"<pre><code>tiny_error_t Mat::copy_head(const Mat &amp;src);\n</code></pre> <p>Description:</p> <p>Copies the header of the specified source matrix to this matrix, sharing the data buffer. All items copy the source matrix.</p> <p>Parameters:</p> <ul> <li><code>const Mat &amp;src</code> : Source matrix.</li> </ul> <p>Returns:</p> <p>tiny_error_t - Error code.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#get-a-view-shallow-copy-of-sub-matrix-roi-from-this-matrix","title":"Get a view (shallow copy) of sub-matrix (ROI) from this matrix","text":"<pre><code>Mat Mat::view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const;\n</code></pre> <p>Description:</p> <p>Gets a view (shallow copy) of the sub-matrix (ROI) from this matrix starting from the specified row and column positions.</p> <p>Parameters:</p> <ul> <li> <p><code>int start_row</code> : Starting row position.</p> </li> <li> <p><code>int start_col</code> : Starting column position.</p> </li> <li> <p><code>int roi_rows</code> : Number of rows in the ROI.</p> </li> <li> <p><code>int roi_cols</code> : Number of columns in the ROI.</p> </li> </ul> <p>Warning</p> <p>Unlike ESP-DSP, view_roi does not allow to setup stride as it will automatically calculate the stride based on the number of columns and paddings. The function will also refuse illegal requests, i.e., out of bound requests. </p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#get-a-view-shallow-copy-of-sub-matrix-roi-from-this-matrix-using-roi-structure","title":"Get a view (shallow copy) of sub-matrix (ROI) from this matrix using ROI structure","text":"<pre><code>Mat Mat::view_roi(const Mat::ROI &amp;roi) const;\n</code></pre> <p>Description:</p> <p>Gets a view (shallow copy) of the sub-matrix (ROI) from this matrix using the specified ROI structure. This function will call the previous function in low level by passing the ROI structure to the parameters.</p> <p>Parameters:</p> <ul> <li><code>const Mat::ROI &amp;roi</code> : ROI structure.</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#get-a-replica-deep-copy-of-sub-matrix-roi","title":"Get a replica (deep copy) of sub-matrix (ROI)","text":"<pre><code>Mat Mat::copy_roi(int start_row, int start_col, int roi_rows, int roi_cols);\n</code></pre> <p>Description:</p> <p>Gets a replica (deep copy) of the sub-matrix (ROI) from this matrix starting from the specified row and column positions. This function will return a new matrix object that does not share the data buffer with the original matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>int start_row</code> : Starting row position.</p> </li> <li> <p><code>int start_col</code> : Starting column position.</p> </li> <li> <p><code>int roi_rows</code> : Number of rows in the ROI.</p> </li> <li> <p><code>int roi_cols</code> : Number of columns in the ROI.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#get-a-replica-deep-copy-of-sub-matrix-roi-using-roi-structure","title":"Get a replica (deep copy) of sub-matrix (ROI) using ROI structure","text":"<pre><code>Mat Mat::copy_roi(const Mat::ROI &amp;roi);\n</code></pre> <p>Description:</p> <p>Gets a replica (deep copy) of the sub-matrix (ROI) from this matrix using the specified ROI structure. This function will call the previous function in low level by passing the ROI structure to the parameters.</p> <p>Parameters:</p> <ul> <li><code>const Mat::ROI &amp;roi</code> : ROI structure.</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#get-a-block-of-matrix","title":"Get a block of matrix","text":"<pre><code>Mat Mat::block(int start_row, int start_col, int block_rows, int block_cols);\n</code></pre> <p>Description:</p> <p>Gets a block of the matrix starting from the specified row and column positions.</p> <p>Parameters:</p> <ul> <li> <p><code>int start_row</code> : Starting row position.</p> </li> <li> <p><code>int start_col</code> : Starting column position.</p> </li> <li> <p><code>int block_rows</code> : Number of rows in the block.</p> </li> <li> <p><code>int block_cols</code> : Number of columns in the block.</p> </li> </ul> <p>Differences between view_roi | copy_roi | block</p> <ul> <li> <p><code>view_roi</code> : Shallow copy of the sub-matrix (ROI) from this matrix.</p> </li> <li> <p><code>copy_roi</code> : Deep copy of the sub-matrix (ROI) from this matrix. Rigid and faster.</p> </li> <li> <p><code>block</code> : Deep copy of the block from this matrix. Flexible and slower.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#swap-rows","title":"Swap rows","text":"<pre><code>void Mat::swap_rows(int row1, int row2);\n</code></pre> <p>Description:</p> <p>Swaps the specified rows in the matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>int row1</code> : First row index.</p> </li> <li> <p><code>int row2</code> : Second row index.</p> </li> </ul> <p>Returns:</p> <p>void</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#swap-columns","title":"Swap columns","text":"<pre><code>void Mat::swap_cols(int col1, int col2);\n</code></pre> <p>Description:</p> <p>Swaps the specified columns in the matrix. </p> <p>Parameters:</p> <ul> <li> <p><code>int col1</code> : First column index.</p> </li> <li> <p><code>int col2</code> : Second column index.</p> </li> </ul> <p>Returns:</p> <p>void</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#clear-matrix","title":"Clear matrix","text":"<pre><code>void Mat::clear(void);\n</code></pre> <p>Description:</p> <p>Clears the matrix by setting all elements to zero.</p> <p>Parameters:</p> <p>void</p> <p>Returns:</p> <p>void</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#arithmetic-operators","title":"ARITHMETIC OPERATORS","text":"<p>In-Place Operations</p> <p>This section defines the arithmetic operators that act on the current matrix itself (in-place operations). These operators modify the matrix and return a reference to it, enabling chained operations like <code>A += B += C</code>. The operators are optimized to handle padding and use DSP-accelerated functions when available.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#copy-assignment","title":"Copy assignment","text":"<pre><code>Mat &amp;operator=(const Mat &amp;src);\n</code></pre> <p>Description:</p> <p>Copy assignment operator for the matrix. Copies elements from source matrix to current matrix. Handles dimension changes by reallocating memory if necessary. Prevents assignment to sub-matrix views for safety.</p> <p>Mathematical Principle:</p> <p>Creates an independent copy of the source matrix. Unlike copy constructor, this is used for existing matrices.</p> <p>Parameters:</p> <ul> <li><code>const Mat &amp;src</code> : Source matrix.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix (enables chaining).</p> <p>Usage Insights:</p> <ul> <li> <p>Memory Management: Automatically reallocates memory if dimensions differ. Frees old memory if it was internally allocated.</p> </li> <li> <p>Sub-Matrix Protection: Assignment to sub-matrix views is forbidden to prevent accidental data corruption.</p> </li> <li> <p>Self-Assignment: Handles self-assignment safely (A = A).</p> </li> <li> <p>Performance: O(n\u00b2) for n\u00d7n matrices. For large matrices, consider if a view would suffice.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#add-matrix","title":"Add matrix","text":"<pre><code>Mat &amp;operator+=(const Mat &amp;A);\n</code></pre> <p>Description:</p> <p>Adds the specified matrix to this matrix.</p> <p>Parameters:</p> <ul> <li><code>const Mat &amp;A</code> : Matrix to be added.</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#add-constant","title":"Add constant","text":"<pre><code>Mat &amp;operator+=(float C);\n</code></pre> <p>Description:</p> <p>Element-wise addition of a constant to this matrix.</p> <p>Parameters:</p> <ul> <li><code>float C</code> : The constant to add.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#subtract-matrix","title":"Subtract matrix","text":"<pre><code>Mat &amp;operator-=(const Mat &amp;A);\n</code></pre> <p>Description:</p> <p>Subtracts the specified matrix from this matrix.</p> <p>Parameters:</p> <ul> <li><code>const Mat &amp;A</code> : Matrix to be subtracted.</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#subtract-constant","title":"Subtract constant","text":"<pre><code>Mat &amp;operator-=(float C);\n</code></pre> <p>Description:</p> <p>Element-wise subtraction of a constant from this matrix.</p> <p>Parameters:</p> <ul> <li><code>float C</code> : The constant to subtract.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#multiply-matrix","title":"Multiply matrix","text":"<pre><code>Mat &amp;operator*=(const Mat &amp;A);\n</code></pre> <p>Description:</p> <p>Matrix multiplication: this = this * A. Performs standard matrix multiplication (not element-wise). The number of columns of the current matrix must equal the number of rows of A.</p> <p>Mathematical Principle:</p> <p>Matrix multiplication C = A * B where C\u1d62\u2c7c = \u03a3\u2096 A\u1d62\u2096 * B\u2096\u2c7c. This is the standard matrix product, not element-wise multiplication.</p> <p>Dimension Requirements:  - Current matrix: m \u00d7 n</p> <ul> <li> <p>Matrix A: n \u00d7 p</p> </li> <li> <p>Result: m \u00d7 p</p> </li> </ul> <p>Parameters:</p> <ul> <li><code>const Mat &amp;A</code> : Matrix to be multiplied (must have n rows, where n = current matrix columns).</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p> <p>Usage Insights:</p> <ul> <li> <p>Memory Efficiency: Creates a temporary copy to avoid overwriting data during computation, then updates the current matrix.</p> </li> <li> <p>Padding Support: Handles matrices with padding using specialized DSP functions when available.</p> </li> <li> <p>Performance: O(mnp) for m\u00d7n * n\u00d7p multiplication. Uses optimized DSP functions on ESP32 platform.</p> </li> <li> <p>Common Mistake: This is matrix multiplication, not element-wise. For element-wise, use a loop with <code>operator()()</code>.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#multiply-constant","title":"Multiply constant","text":"<pre><code>Mat &amp;operator*=(float C);\n</code></pre> <p>Description:</p> <p>Element-wise multiplication by a constant.</p> <p>Parameters:</p> <ul> <li><code>float C</code> : The constant multiplier.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#divide-matrix-element-wise","title":"Divide matrix (element-wise)","text":"<pre><code>Mat &amp;operator/=(const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Element-wise division: this = this / B.</p> <p>Parameters:</p> <ul> <li><code>const Mat &amp;B</code> : The matrix divisor.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#divide-constant","title":"Divide constant","text":"<pre><code>Mat &amp;operator/=(float C);\n</code></pre> <p>Description:</p> <p>Element-wise division of this matrix by a constant.</p> <p>Parameters:</p> <ul> <li><code>float C</code> : The constant divisor.</li> </ul> <p>Returns:</p> <p>Mat&amp; - Reference to the current matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#exponentiation","title":"Exponentiation","text":"<pre><code>Mat operator^(int C);\n</code></pre> <p>Description:</p> <p>Element-wise integer exponentiation. Returns a new matrix where each element is raised to the given power.</p> <p>Parameters:</p> <ul> <li><code>int C</code> : The exponent (integer).</li> </ul> <p>Returns:</p> <p>Mat - New matrix after exponentiation.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#linear-algebra","title":"LINEAR ALGEBRA","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#transpose","title":"Transpose","text":"<pre><code>Mat Mat::transpose();\n</code></pre> <p>Description:</p> <p>Calculates the transpose of the matrix, returning a new matrix. The transpose A^T of a matrix A is obtained by interchanging rows and columns: (A^T)\u1d62\u2c7c = A\u2c7c\u1d62.</p> <p>Mathematical Principle:  - For any matrix A, (A<sup>T)</sup>T = A</p> <ul> <li> <p>(A + B)^T = A^T + B^T</p> </li> <li> <p>(AB)^T = B^T * A^T</p> </li> <li> <p>For square matrices, det(A) = det(A^T)</p> </li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - Transposed matrix (col \u00d7 row).</p> <p>Usage Insights:</p> <ul> <li> <p>Memory Layout: Creates a new matrix, so memory usage doubles temporarily. For large matrices, consider memory constraints.</p> </li> <li> <p>Symmetric Matrices: If A = A^T, the matrix is symmetric. Use <code>is_symmetric()</code> to check.</p> </li> <li> <p>Applications: </p> </li> <li> <p>Inner products: u^T * v</p> </li> <li> <p>Quadratic forms: x^T * A * x</p> </li> <li> <p>Matrix equations: A^T * A (normal equations)</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#minor-matrix","title":"Minor matrix","text":"<pre><code>Mat Mat::minor(int row, int col);\n</code></pre> <p>Description:</p> <p>Calculates the minor matrix by removing the specified row and column. The minor is the submatrix obtained by removing one row and one column.</p> <p>Parameters: </p> <ul> <li> <p><code>int row</code>: Row index to remove.</p> </li> <li> <p><code>int col</code>: Column index to remove.</p> </li> </ul> <p>Returns:</p> <p>Mat - The (n-1)x(n-1) minor matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#cofactor-matrix","title":"Cofactor matrix","text":"<pre><code>Mat Mat::cofactor(int row, int col);\n</code></pre> <p>Description:</p> <p>Calculates the cofactor matrix (same as minor matrix). The cofactor matrix is the same as the minor matrix. The sign (-1)^(i+j) is applied when computing the cofactor value, not to the matrix elements themselves.</p> <p>Parameters: </p> <ul> <li> <p><code>int row</code>: Row index to remove.</p> </li> <li> <p><code>int col</code>: Column index to remove.</p> </li> </ul> <p>Returns:</p> <p>Mat - The (n-1)x(n-1) cofactor matrix (same as minor matrix).</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#determinant-auto-select-method","title":"Determinant (Auto-select Method)","text":"<pre><code>float Mat::determinant();\n</code></pre> <p>Description: </p> <p>Computes the determinant of a square matrix, automatically selecting the optimal method based on matrix size. For small matrices (n \u2264 4), uses Laplace expansion; for larger matrices (n &gt; 4), uses LU decomposition for better efficiency.</p> <p>Mathematical Principle: </p> <p>The determinant is an important numerical characteristic of a square matrix with the following properties:</p> <ul> <li> <p>det(AB) = det(A) * det(B)</p> </li> <li> <p>det(A^T) = det(A)</p> </li> <li> <p>det(A^(-1)) = 1 / det(A)</p> </li> <li> <p>If A is singular, det(A) = 0</p> </li> </ul> <p>Method Selection:</p> <ul> <li> <p>Small matrices (n \u2264 4): Uses <code>determinant_laplace()</code> - Laplace expansion method, time complexity O(n!), more accurate for small matrices</p> </li> <li> <p>Large matrices (n &gt; 4): Uses <code>determinant_lu()</code> - LU decomposition method, time complexity O(n\u00b3), more efficient</p> </li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns: </p> <p>float - The determinant value.</p> <p>Usage Insights:</p> <ul> <li> <p>Automatic Selection: For most applications, simply use <code>determinant()</code> and the function will automatically select the optimal method</p> </li> <li> <p>Performance Optimization: If you need to compute determinants of matrices of the same size multiple times, consider directly calling <code>determinant_lu()</code> or <code>determinant_gaussian()</code></p> </li> <li> <p>Precision Requirements: For small matrices, <code>determinant_laplace()</code> may provide better numerical precision</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#determinant-laplace-expansion","title":"Determinant - Laplace Expansion","text":"<pre><code>float Mat::determinant_laplace();\n</code></pre> <p>Description: </p> <p>Computes the determinant of a square matrix using Laplace expansion (cofactor expansion). Time complexity is O(n!), suitable only for small matrices (n \u2264 4).</p> <p>Mathematical Principle: </p> <p>Laplace expansion is the recursive definition of the determinant:</p> <ul> <li> <p>For 1\u00d71 matrix: det([a]) = a</p> </li> <li> <p>For 2\u00d72 matrix: det([[a,b],[c,d]]) = ad - bc</p> </li> <li> <p>For n\u00d7n matrix: det(A) = \u03a3\u2c7c\u208c\u2081\u207f (-1)\u2071\u207a\u02b2 a\u1d62\u2c7c * det(M\u1d62\u2c7c), where M\u1d62\u2c7c is the minor matrix</p> </li> </ul> <p>This implementation uses first-row expansion, recursively computing the determinant of minors.</p> <p>Parameters:</p> <p>None.</p> <p>Returns: </p> <p>float - The determinant value.</p> <p>Performance Warning</p> <p>Time complexity is O(n!), suitable only for small matrices (n \u2264 4). For large matrices, use <code>determinant_lu()</code> or <code>determinant_gaussian()</code>.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#determinant-lu-decomposition","title":"Determinant - LU Decomposition","text":"<pre><code>float Mat::determinant_lu();\n</code></pre> <p>Description: </p> <p>Computes the determinant of a square matrix using LU decomposition. Time complexity is O(n\u00b3), suitable for large matrices.</p> <p>Mathematical Principle: </p> <p>LU decomposition factorizes the matrix as A = P * L * U, where:</p> <ul> <li> <p>P is a permutation matrix (if pivoting is used)</p> </li> <li> <p>L is a lower triangular matrix with unit diagonal</p> </li> <li> <p>U is an upper triangular matrix</p> </li> </ul> <p>Determinant formula: det(A) = det(P) * det(L) * det(U)</p> <p>Where:</p> <ul> <li> <p>det(P) = (-1)^(permutation signature), determined by the number of row swaps</p> </li> <li> <p>det(L) = 1 (since L has unit diagonal)</p> </li> <li> <p>det(U) = \u220f\u1d62 U\u1d62\u1d62 (product of diagonal elements of U)</p> </li> </ul> <p>Algorithm Steps:</p> <ol> <li>Perform LU decomposition (with pivoting for numerical stability)</li> <li>Compute the determinant of the permutation matrix det(P)</li> <li>Compute the product of diagonal elements of U: det(U)</li> <li>Return det(P) * det(U)</li> </ol> <p>Parameters:</p> <p>None.</p> <p>Returns: </p> <p>float - The determinant value. Returns 0.0 if the matrix is singular or near-singular.</p> <p>Usage Insights:</p> <ul> <li> <p>Efficiency: Much faster than Laplace expansion for matrices with n &gt; 4</p> </li> <li> <p>Numerical Stability: Uses pivoting to improve numerical stability</p> </li> <li> <p>Singular Matrices: If the matrix is singular, LU decomposition fails and the function returns 0.0</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#determinant-gaussian-elimination","title":"Determinant - Gaussian Elimination","text":"<pre><code>float Mat::determinant_gaussian();\n</code></pre> <p>Description: </p> <p>Computes the determinant of a square matrix using Gaussian elimination. Time complexity is O(n\u00b3), suitable for large matrices.</p> <p>Mathematical Principle: </p> <p>Gaussian elimination converts the matrix to upper triangular form, then computes the product of diagonal elements. The determinant value equals the product of diagonal elements of the upper triangular matrix, adjusted for the sign based on the number of row swaps.</p> <p>Algorithm Steps:</p> <ol> <li>Use partial pivoting Gaussian elimination to convert matrix to upper triangular form</li> <li>Track the number of row swaps</li> <li>Compute the product of diagonal elements of the upper triangular matrix</li> <li>Adjust the sign based on row swaps: each row swap multiplies the determinant by -1</li> </ol> <p>Parameters:</p> <p>None.</p> <p>Returns: </p> <p>float - The determinant value. Returns 0.0 if the matrix is singular.</p> <p>Usage Insights:</p> <ul> <li> <p>Efficiency: Time complexity O(n\u00b3) for large matrices, comparable to LU decomposition</p> </li> <li> <p>Numerical Stability: Uses partial pivoting to improve numerical stability</p> </li> <li> <p>Implementation Simplicity: More intuitive than LU decomposition, but less versatile (cannot be used for solving linear systems)</p> </li> <li> <p>Applications:</p> </li> <li> <p>Check invertibility: det(A) \u2260 0 means A is invertible</p> </li> <li> <p>Volume scaling: |det(A)| is the scaling factor of the linear transformation</p> </li> <li> <p>System solvability: det(A) = 0 indicates singular system</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#adjoint","title":"Adjoint","text":"<pre><code>Mat Mat::adjoint();\n</code></pre> <p>Description:</p> <p>Calculates the adjoint (adjugate) matrix of a square matrix.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - Adjoint matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#normalize","title":"Normalize","text":"<pre><code>void Mat::normalize();\n</code></pre> <p>Description:</p> <p>Normalizes the matrix using L2 norm (Frobenius norm). After normalization, ||Matrix|| = 1.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>void</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#norm","title":"Norm","text":"<pre><code>float Mat::norm() const;\n</code></pre> <p>Description:</p> <p>Calculates the Frobenius norm (also called Euclidean norm or L2 norm) of the matrix. The Frobenius norm is the square root of the sum of squares of all matrix elements.</p> <p>Mathematical Principle:  - Frobenius norm: ||A||_F = \u221a(\u03a3\u1d62 \u03a3\u2c7c |a\u1d62\u2c7c|\u00b2) = \u221a(trace(A^T * A)) - For vectors, this reduces to the standard L2 norm - Properties:   - ||A + B||_F \u2264 ||A||_F + ||B||_F (triangle inequality)   - ||AB||_F \u2264 ||A||_F * ||B||_F   - ||A||_F = ||A^T||_F</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>float - The computed matrix norm.</p> <p>Usage Insights:</p> <ul> <li> <p>Error Measurement: Useful for measuring the \"size\" of a matrix or error in numerical computations.</p> </li> <li> <p>Normalization: Used in <code>normalize()</code> to scale matrices to unit norm.</p> </li> <li> <p>Convergence: Often used as a convergence criterion in iterative algorithms.</p> </li> <li> <p>Comparison: For vectors, this is equivalent to the standard Euclidean norm ||v||\u2082.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#inverse-using-adjoint","title":"Inverse using Adjoint","text":"<pre><code>Mat Mat::inverse_adjoint();\n</code></pre> <p>Description:</p> <p>Computes the inverse of a square matrix using adjoint method. If the matrix is singular, returns a zero matrix.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - The inverse matrix. If singular, returns a zero matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#identity-matrix","title":"Identity Matrix","text":"<pre><code>static Mat Mat::eye(int size);\n</code></pre> <p>Description:</p> <p>Generates an identity matrix of given size.</p> <p>Parameters: </p> <ul> <li><code>int size</code> : Dimension of the square identity matrix.</li> </ul> <p>Returns:</p> <p>Mat - Identity matrix (size x size).</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#augmentation-matrix-horizontal-concatenation","title":"Augmentation Matrix (Horizontal Concatenation)","text":"<pre><code>static Mat Mat::augment(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Creates an augmented matrix by horizontally concatenating two matrices [A | B]. The row counts of A and B must match.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Left matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Right matrix.</p> </li> </ul> <p>Returns:</p> <p>Mat - Augmented matrix [A B].</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#vertical-stack","title":"Vertical Stack","text":"<pre><code>static Mat Mat::vstack(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Vertically stacks two matrices [A; B]. The column counts of A and B must match.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Top matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Bottom matrix.</p> </li> </ul> <p>Returns:</p> <p>Mat - Vertically stacked matrix [A; B].</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#gram-schmidt-orthogonalization","title":"Gram-Schmidt Orthogonalization","text":"<pre><code>static bool Mat::gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                            Mat &amp;coefficients, float tolerance = 1e-6f);\n</code></pre> <p>Description:</p> <p>Orthogonalizes a set of vectors using the Gram-Schmidt process. This is a general-purpose orthogonalization function that can be reused for QR decomposition and other applications requiring orthogonal bases. Uses the modified Gram-Schmidt algorithm with re-orthogonalization for improved numerical stability.</p> <p>Mathematical Principle:</p> <p>Given a set of vectors {v\u2081, v\u2082, ..., v\u2099}, the Gram-Schmidt process produces an orthogonal set {q\u2081, q\u2082, ..., q\u2099} where:</p> <ul> <li> <p>q\u2081 = v\u2081 / ||v\u2081||</p> </li> <li> <p>q\u2c7c = (v\u2c7c - \u03a3\u1d62\u208c\u2081\u02b2\u207b\u00b9\u27e8v\u2c7c, q\u1d62\u27e9q\u1d62) / ||v\u2c7c - \u03a3\u1d62\u208c\u2081\u02b2\u207b\u00b9\u27e8v\u2c7c, q\u1d62\u27e9q\u1d62||</p> </li> </ul> <p>The modified version subtracts projections immediately, which improves numerical stability.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;vectors</code> : Input matrix where each column is a vector to be orthogonalized (m \u00d7 n).</p> </li> <li> <p><code>Mat &amp;orthogonal_vectors</code> : Output matrix for orthogonalized vectors (m \u00d7 n), each column is orthogonal and normalized.</p> </li> <li> <p><code>Mat &amp;coefficients</code> : Output matrix for projection coefficients (n \u00d7 n, upper triangular), similar to R in QR decomposition.</p> </li> <li> <p><code>float tolerance</code> : Minimum norm threshold for linear independence check (default: 1e-6).</p> </li> </ul> <p>Returns:</p> <p><code>bool</code> - <code>true</code> if successful, <code>false</code> if input is invalid.</p> <p>Usage Insights:</p> <ul> <li> <p>Numerical Stability: The implementation uses modified Gram-Schmidt with re-orthogonalization, which significantly improves stability for near-linearly-dependent vectors.</p> </li> <li> <p>QR Decomposition: This function is internally used by <code>qr_decompose()</code>. For QR decomposition, the coefficients matrix corresponds to the R matrix.</p> </li> <li> <p>Basis Construction: Useful for constructing orthogonal bases from a set of vectors, which is fundamental in many linear algebra applications.</p> </li> <li> <p>Performance: For large matrices, consider the computational cost. The complexity is O(mn\u00b2) for m-dimensional vectors and n vectors.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#all-ones-matrix-rectangular","title":"All-Ones Matrix (Rectangular)","text":"<pre><code>static Mat Mat::ones(int rows, int cols);\n</code></pre> <p>Description:</p> <p>Creates a matrix of specified size filled with ones.</p> <p>Parameters:</p> <ul> <li> <p><code>int rows</code> : Number of rows.</p> </li> <li> <p><code>int cols</code> : Number of columns.</p> </li> </ul> <p>Returns:</p> <p>Mat - Matrix [rows x cols] with all elements = 1.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#all-ones-matrix-square","title":"All-Ones Matrix (Square)","text":"<pre><code>static Mat Mat::ones(int size);\n</code></pre> <p>Description:</p> <p>Creates a square matrix filled with ones of the specified size.</p> <p>Parameters:</p> <ul> <li><code>int size</code> : Size of the square matrix (rows = cols).</li> </ul> <p>Returns:</p> <p>Mat - Square matrix [size x size] with all elements = 1.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#gaussian-elimination","title":"Gaussian Elimination","text":"<pre><code>Mat Mat::gaussian_eliminate() const;\n</code></pre> <p>Description:</p> <p>Performs Gaussian Elimination to convert matrix to Row Echelon Form (REF). This is the first step in solving linear systems and computing matrix rank.</p> <p>Mathematical Principle:</p> <p>Gaussian elimination transforms a matrix into row echelon form through elementary row operations:</p> <ol> <li> <p>Row swapping: Exchange two rows</p> </li> <li> <p>Row scaling: Multiply a row by a non-zero scalar</p> </li> <li> <p>Row addition: Add a multiple of one row to another</p> </li> </ol> <p>Row Echelon Form (REF) properties:</p> <ul> <li> <p>All zero rows are at the bottom</p> </li> <li> <p>The leading coefficient (pivot) of each non-zero row is to the right of the pivot in the row above</p> </li> <li> <p>All entries below a pivot are zero</p> </li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - The upper triangular matrix (REF form).</p> <p>Usage Insights:</p> <ul> <li> <p>Linear System Solving: First step in solving Ax = b. After REF, use back substitution.</p> </li> <li> <p>Rank Computation: The rank equals the number of non-zero rows in REF.</p> </li> <li> <p>Determinant: Can compute determinant from REF (product of diagonal elements, adjusted for row swaps).</p> </li> <li> <p>Numerical Stability: The implementation uses partial pivoting to improve numerical stability.</p> </li> <li> <p>Performance: O(n\u00b3) for n\u00d7n matrices. For multiple systems, prefer LU decomposition.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#row-reduce-from-gaussian","title":"Row Reduce from Gaussian","text":"<pre><code>Mat Mat::row_reduce_from_gaussian();\n</code></pre> <p>Description:</p> <p>Converts a matrix (assumed in row echelon form) to Reduced Row Echelon Form (RREF).</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - The matrix in RREF form.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#inverse-using-gaussian-jordan-elimination","title":"Inverse using Gaussian-Jordan Elimination","text":"<pre><code>Mat Mat::inverse_gje();\n</code></pre> <p>Description:</p> <p>Computes the inverse of a square matrix using Gauss-Jordan elimination.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p>Mat - The inverse matrix if invertible, otherwise returns empty matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#dot-product","title":"Dot Product","text":"<pre><code>float Mat::dotprod(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Calculates the dot product of two vectors (Nx1).</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input vector A (Nx1).</p> </li> <li> <p><code>const Mat &amp;B</code> : Input vector B (Nx1).</p> </li> </ul> <p>Returns:</p> <p>float - The computed dot product value.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#solve-linear-system","title":"Solve Linear System","text":"<pre><code>Mat Mat::solve(const Mat &amp;A, const Mat &amp;b) const;\n</code></pre> <p>Description:</p> <p>Solves the linear system Ax = b using Gaussian elimination with back substitution. This is a direct method suitable for well-conditioned systems.</p> <p>Mathematical Principle:</p> <p>The method consists of two phases:</p> <ol> <li> <p>Forward elimination: Transform augmented matrix [A|b] to upper triangular form</p> </li> <li> <p>Back substitution: Solve Ux = y from bottom to top</p> </li> </ol> <p>Algorithm:</p> <ul> <li> <p>Create augmented matrix [A | b]</p> </li> <li> <p>Apply Gaussian elimination to get [U | y] where U is upper triangular</p> </li> <li> <p>Solve Ux = y using back substitution: x\u1d62 = (y\u1d62 - \u03a3\u2c7c\u208c\u1d62\u208a\u2081\u207f U\u1d62\u2c7cx\u2c7c) / U\u1d62\u1d62</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Coefficient matrix (N\u00d7N), must be square and non-singular.</p> </li> <li> <p><code>const Mat &amp;b</code> : Right-hand side vector (N\u00d71).</p> </li> </ul> <p>Returns:</p> <p>Mat - Solution vector (N\u00d71) containing the roots of the equation Ax = b. Returns empty matrix if system is singular or incompatible.</p> <p>Usage Insights:</p> <ul> <li> <p>Single System: Efficient for solving one system. For multiple systems with same A, use LU decomposition + <code>solve_lu()</code>.</p> </li> <li> <p>Condition Number: Performance degrades for ill-conditioned matrices. Check condition number if results are inaccurate.</p> </li> <li> <p>Singular Systems: Returns empty matrix if A is singular (det(A) = 0). Use SVD + pseudo-inverse for rank-deficient systems.</p> </li> <li> <p>Performance: O(n\u00b3) for elimination, O(n\u00b2) for back substitution. Total O(n\u00b3).</p> </li> <li> <p>Alternative Methods:</p> </li> <li> <p>For SPD matrices: Use Cholesky decomposition + <code>solve_cholesky()</code> (faster)</p> </li> <li> <p>For multiple RHS: Use LU decomposition + <code>solve_lu()</code> (more efficient)</p> </li> <li> <p>For overdetermined: Use QR decomposition + <code>solve_qr()</code> (least squares)</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#band-solve","title":"Band Solve","text":"<pre><code>Mat Mat::band_solve(Mat A, Mat b, int k);\n</code></pre> <p>Description:</p> <p>Solves the system of equations Ax = b using optimized Gaussian elimination for banded matrices.</p> <p>Parameters:</p> <ul> <li> <p><code>Mat A</code> : Coefficient matrix (NxN) - banded matrix.</p> </li> <li> <p><code>Mat b</code> : Result vector (Nx1).</p> </li> <li> <p><code>int k</code> : Bandwidth of the matrix (the width of the non-zero bands).</p> </li> </ul> <p>Returns:</p> <p>Mat - Solution vector (Nx1) containing the roots of the equation Ax = b.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roots","title":"Roots","text":"<pre><code>Mat Mat::roots(Mat A, Mat y);\n</code></pre> <p>Description:</p> <p>Solves the matrix using a different method. Another implementation of the 'solve' function, no difference in principle. This method solves the linear system A * x = y using Gaussian elimination.</p> <p>Parameters:</p> <ul> <li> <p><code>Mat A</code> : Matrix [N]x[N] with input coefficients.</p> </li> <li> <p><code>Mat y</code> : Vector [N]x[1] with result values.</p> </li> </ul> <p>Returns:</p> <p>Mat - Matrix [N]x[1] with roots.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-properties-decompositions","title":"MATRIX PROPERTIES &amp; DECOMPOSITIONS","text":"<p>Matrix Decompositions Overview</p> <p>Matrix decompositions are fundamental tools in numerical linear algebra. They break down a matrix into simpler components that reveal its structure and enable efficient computations. Different decompositions are suited for different types of matrices and applications.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-property-checks","title":"Matrix Property Checks","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#check-symmetry","title":"Check Symmetry","text":"<pre><code>bool Mat::is_symmetric(float tolerance = 1e-6f) const;\n</code></pre> <p>Description:</p> <p>Check whether a matrix is symmetric within the given tolerance. A matrix A is symmetric if A = A^T, i.e., A(i,j) = A(j,i) for all i, j.</p> <p>Mathematical Principle:</p> <p>For a symmetric matrix, all eigenvalues are real, and eigenvectors can be chosen to be orthogonal. Symmetric matrices are fundamental in many applications, especially in structural dynamics and optimization.</p> <p>Parameters:</p> <ul> <li><code>float tolerance</code> : Maximum allowed difference |A(i,j) - A(j,i)| (default: 1e-6).</li> </ul> <p>Returns:</p> <p><code>bool</code> - <code>true</code> if approximately symmetric, <code>false</code> otherwise.</p> <p>Usage Insights:</p> <ul> <li> <p>Eigendecomposition: Symmetric matrices can use more efficient and stable eigendecomposition methods (e.g., Jacobi method).</p> </li> <li> <p>Cholesky Decomposition: Only symmetric positive definite matrices can be decomposed using Cholesky decomposition.</p> </li> <li> <p>Structural Dynamics: Stiffness and mass matrices in structural analysis are typically symmetric.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#check-positive-definiteness","title":"Check Positive Definiteness","text":"<pre><code>bool Mat::is_positive_definite(float tolerance = 1e-6f) const;\n</code></pre> <p>Description:</p> <p>Check if a matrix is positive definite using Sylvester's criterion. A symmetric matrix A is positive definite if x^T A x &gt; 0 for all non-zero vectors x, or equivalently, all eigenvalues are positive.</p> <p>Mathematical Principle:</p> <p>Sylvester's criterion states that a symmetric matrix is positive definite if and only if all leading principal minors are positive. The function checks the first few leading minors and diagonal elements for efficiency.</p> <p>Parameters:</p> <ul> <li><code>float tolerance</code> : Tolerance for numerical checks (default: 1e-6).</li> </ul> <p>Returns:</p> <p><code>bool</code> - <code>true</code> if matrix is positive definite, <code>false</code> otherwise.</p> <p>Usage Insights:</p> <ul> <li> <p>Cholesky Decomposition: Positive definite matrices can be decomposed using Cholesky decomposition, which is faster and more stable than LU decomposition.</p> </li> <li> <p>Optimization: Positive definite Hessian matrices indicate local minima in optimization problems.</p> </li> <li> <p>Stability Analysis: In control systems, positive definiteness of certain matrices ensures system stability.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-decomposition-structures","title":"Matrix Decomposition Structures","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#lu-decomposition-structure","title":"LU Decomposition Structure","text":"<pre><code>struct Mat::LUDecomposition\n{\n    Mat L;                 // Lower triangular matrix (with unit diagonal)\n    Mat U;                 // Upper triangular matrix\n    Mat P;                 // Permutation matrix (if pivoting used)\n    bool pivoted;          // Whether pivoting was used\n    tiny_error_t status;   // Computation status\n\n    LUDecomposition();\n};\n</code></pre> <p>Description:</p> <p>Container for LU decomposition results. The decomposition A = P * L * U (with pivoting) or A = L * U (without pivoting), where L is lower triangular with unit diagonal, U is upper triangular, and P is a permutation matrix.</p> <p>Mathematical Principle:</p> <p>LU decomposition factors a matrix into lower and upper triangular matrices, enabling efficient solution of linear systems. With pivoting, it handles near-singular matrices better.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#cholesky-decomposition-structure","title":"Cholesky Decomposition Structure","text":"<pre><code>struct Mat::CholeskyDecomposition\n{\n    Mat L;                 // Lower triangular matrix\n    tiny_error_t status;   // Computation status\n\n    CholeskyDecomposition();\n};\n</code></pre> <p>Description:</p> <p>Container for Cholesky decomposition results. For symmetric positive definite matrices, A = L * L^T, where L is lower triangular.</p> <p>Mathematical Principle:</p> <p>Cholesky decomposition is a specialized LU decomposition for symmetric positive definite matrices. It requires only half the storage and computation of LU decomposition.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#qr-decomposition-structure","title":"QR Decomposition Structure","text":"<pre><code>struct Mat::QRDecomposition\n{\n    Mat Q;                 // Orthogonal matrix (Q^T * Q = I)\n    Mat R;                 // Upper triangular matrix\n    tiny_error_t status;   // Computation status\n\n    QRDecomposition();\n};\n</code></pre> <p>Description:</p> <p>Container for QR decomposition results. A = Q * R, where Q is orthogonal (Q^T * Q = I) and R is upper triangular.</p> <p>Mathematical Principle:</p> <p>QR decomposition expresses a matrix as the product of an orthogonal matrix and an upper triangular matrix. It's numerically stable and fundamental for least squares problems.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#svd-decomposition-structure","title":"SVD Decomposition Structure","text":"<pre><code>struct Mat::SVDDecomposition\n{\n    Mat U;                 // Left singular vectors (orthogonal matrix)\n    Mat S;                 // Singular values (diagonal matrix or vector)\n    Mat V;                 // Right singular vectors (orthogonal matrix, V^T)\n    int rank;              // Numerical rank of the matrix\n    int iterations;        // Number of iterations performed\n    tiny_error_t status;   // Computation status\n\n    SVDDecomposition();\n};\n</code></pre> <p>Description:</p> <p>Container for SVD decomposition results. A = U * S * V^T, where U and V are orthogonal matrices, and S contains singular values on the diagonal.</p> <p>Mathematical Principle:</p> <p>SVD is the most general matrix decomposition. The singular values reveal the matrix's rank, condition number, and enable computation of pseudo-inverse for rank-deficient matrices.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-decomposition-methods","title":"Matrix Decomposition Methods","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#lu-decomposition","title":"LU Decomposition","text":"<pre><code>Mat::LUDecomposition Mat::lu_decompose(bool use_pivoting = true) const;\n</code></pre> <p>Description:</p> <p>Compute LU decomposition: A = P * L * U (with pivoting) or A = L * U (without pivoting). Efficient for solving multiple systems with the same coefficient matrix.</p> <p>Mathematical Principle: </p> <ul> <li> <p>Without pivoting: A = L * U, where L has unit diagonal</p> </li> <li> <p>With pivoting: P * A = L * U, where P is a permutation matrix</p> </li> </ul> <p>The decomposition enables solving Ax = b by solving Ly = Pb (forward substitution) then Ux = y (back substitution).</p> <p>Parameters:</p> <ul> <li><code>bool use_pivoting</code> : Whether to use partial pivoting for numerical stability (default: true).</li> </ul> <p>Returns:</p> <p><code>LUDecomposition</code> containing L, U, P matrices and status.</p> <p>Usage Insights:</p> <ul> <li> <p>Multiple RHS: Once decomposed, solve multiple systems with different right-hand sides efficiently using <code>solve_lu()</code>.</p> </li> <li> <p>Determinant: det(A) = det(P) * det(L) * det(U) = det(P) * det(U) (since det(L) = 1).</p> </li> <li> <p>Inverse: Can compute A^(-1) by solving LUx = e\u1d62 for each unit vector e\u1d62.</p> </li> <li> <p>Performance: O(n\u00b3) for decomposition, O(n\u00b2) for each solve after decomposition.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<pre><code>Mat::CholeskyDecomposition Mat::cholesky_decompose() const;\n</code></pre> <p>Description:</p> <p>Compute Cholesky decomposition: A = L * L^T for symmetric positive definite matrices. Faster than LU for SPD matrices, commonly used in structural dynamics.</p> <p>Mathematical Principle:</p> <p>For a symmetric positive definite matrix A, there exists a unique lower triangular matrix L with positive diagonal elements such that A = L * L^T. This is essentially a specialized LU decomposition that takes advantage of symmetry.</p> <p>Parameters:</p> <p>None (matrix must be symmetric positive definite).</p> <p>Returns:</p> <p><code>CholeskyDecomposition</code> containing L matrix and status.</p> <p>Usage Insights:</p> <ul> <li> <p>Efficiency: Requires approximately half the computation and storage of LU decomposition.</p> </li> <li> <p>Stability: More stable than LU for symmetric positive definite matrices.</p> </li> <li> <p>Applications: </p> </li> <li> <p>Structural dynamics: Mass and stiffness matrices are often SPD</p> </li> <li> <p>Optimization: Hessian matrices in Newton's method</p> </li> <li> <p>Statistics: Covariance matrices</p> </li> <li> <p>Error Handling: Returns error if matrix is not symmetric or not positive definite.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#qr-decomposition","title":"QR Decomposition","text":"<pre><code>Mat::QRDecomposition Mat::qr_decompose() const;\n</code></pre> <p>Description:</p> <p>Compute QR decomposition: A = Q * R, where Q is orthogonal and R is upper triangular. Numerically stable, used for least squares and orthogonalization.</p> <p>Mathematical Principle:</p> <p>QR decomposition expresses any matrix as the product of an orthogonal matrix Q (Q^T * Q = I) and an upper triangular matrix R. The decomposition is computed using the modified Gram-Schmidt process with re-orthogonalization.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>QRDecomposition</code> containing Q and R matrices and status.</p> <p>Usage Insights:</p> <ul> <li> <p>Least Squares: For overdetermined system Ax \u2248 b, the solution minimizes ||Ax - b||\u2082 is x = R^(-1) * Q^T * b.</p> </li> <li> <p>Numerical Stability: QR decomposition is more stable than normal equations for least squares problems.</p> </li> <li> <p>Eigendecomposition: QR algorithm uses QR decomposition iteratively to find eigenvalues.</p> </li> <li> <p>Rank Revealing: The rank of A equals the number of non-zero diagonal elements of R.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#svd-decomposition","title":"SVD Decomposition","text":"<pre><code>Mat::SVDDecomposition Mat::svd_decompose(int max_iter = 100, float tolerance = 1e-6f) const;\n</code></pre> <p>Description:</p> <p>Compute Singular Value Decomposition: A = U * S * V^T. Most general decomposition, used for rank estimation, pseudo-inverse, dimension reduction. Uses iterative method based on eigendecomposition.</p> <p>Mathematical Principle:</p> <p>SVD decomposes any m \u00d7 n matrix A into: - U: m \u00d7 m orthogonal matrix (left singular vectors)</p> <ul> <li> <p>S: m \u00d7 n diagonal matrix (singular values \u03c3\u2081 \u2265 \u03c3\u2082 \u2265 ... \u2265 \u03c3\u1d63 \u2265 0)</p> </li> <li> <p>V: n \u00d7 n orthogonal matrix (right singular vectors)</p> </li> </ul> <p>The singular values reveal the matrix's fundamental properties: rank, condition number, and numerical behavior.</p> <p>Parameters:</p> <ul> <li> <p><code>int max_iter</code> : Maximum number of iterations (default: 100).</p> </li> <li> <p><code>float tolerance</code> : Convergence tolerance (default: 1e-6).</p> </li> </ul> <p>Returns:</p> <p><code>SVDDecomposition</code> containing U, S, V matrices, rank, and status.</p> <p>Usage Insights:</p> <ul> <li> <p>Rank Estimation: The numerical rank is the number of singular values above the tolerance threshold.</p> </li> <li> <p>Pseudo-Inverse: A\u207a = V * S\u207a * U^T, where S\u207a has 1/\u03c3\u1d62 for non-zero \u03c3\u1d62.</p> </li> <li> <p>Dimension Reduction: Truncated SVD (keeping only largest singular values) provides low-rank approximation.</p> </li> <li> <p>Condition Number: \u03ba(A) = \u03c3\u2081 / \u03c3\u1d63, where \u03c3\u1d63 is the smallest non-zero singular value.</p> </li> <li> <p>Applications: </p> </li> <li> <p>Least squares for rank-deficient systems</p> </li> <li> <p>Principal Component Analysis (PCA)</p> </li> <li> <p>Image compression</p> </li> <li> <p>Noise reduction</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#solving-linear-systems-using-decompositions","title":"Solving Linear Systems Using Decompositions","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#solve-using-lu-decomposition","title":"Solve using LU Decomposition","text":"<pre><code>static Mat Mat::solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b);\n</code></pre> <p>Description:</p> <p>Solve linear system Ax = b using precomputed LU decomposition. More efficient than <code>solve()</code> when solving multiple systems with the same coefficient matrix.</p> <p>Mathematical Principle:</p> <p>Given A = P * L * U, solve Ax = b by: 1. Solve Ly = Pb (forward substitution) 2. Solve Ux = y (back substitution)</p> <p>Parameters:</p> <ul> <li> <p><code>const LUDecomposition &amp;lu</code> : Precomputed LU decomposition.</p> </li> <li> <p><code>const Mat &amp;b</code> : Right-hand side vector (N\u00d71).</p> </li> </ul> <p>Returns:</p> <p>Mat - Solution vector (N\u00d71).</p> <p>Usage Insights:</p> <ul> <li> <p>Multiple RHS: After computing LU decomposition once, solve multiple systems efficiently.</p> </li> <li> <p>Performance: O(n\u00b2) per solve vs O(n\u00b3) for full solve, significant savings for multiple RHS.</p> </li> <li> <p>Memory: Reuses the decomposition, avoiding repeated computation.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#solve-using-cholesky-decomposition","title":"Solve using Cholesky Decomposition","text":"<pre><code>static Mat Mat::solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b);\n</code></pre> <p>Description:</p> <p>Solve linear system Ax = b using precomputed Cholesky decomposition. More efficient than LU for symmetric positive definite matrices.</p> <p>Mathematical Principle:</p> <p>Given A = L * L^T, solve Ax = b by: 1. Solve Ly = b (forward substitution) 2. Solve L^T x = y (back substitution)</p> <p>Parameters:</p> <ul> <li> <p><code>const CholeskyDecomposition &amp;chol</code> : Precomputed Cholesky decomposition.</p> </li> <li> <p><code>const Mat &amp;b</code> : Right-hand side vector (N\u00d71).</p> </li> </ul> <p>Returns:</p> <p>Mat - Solution vector (N\u00d71).</p> <p>Usage Insights:</p> <ul> <li> <p>Efficiency: Faster than LU for SPD matrices, both in decomposition and solving.</p> </li> <li> <p>Stability: More numerically stable for SPD matrices.</p> </li> <li> <p>Applications: Structural dynamics, optimization, statistics.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#solve-using-qr-decomposition-least-squares","title":"Solve using QR Decomposition (Least Squares)","text":"<pre><code>static Mat Mat::solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b);\n</code></pre> <p>Description:</p> <p>Solve linear system using QR decomposition. Provides least squares solution for overdetermined systems (more equations than unknowns).</p> <p>Mathematical Principle:</p> <p>For Ax \u2248 b (overdetermined), the least squares solution minimizes ||Ax - b||\u2082. Using A = Q * R: - x = R^(-1) * Q^T * b</p> <p>This avoids the numerically unstable normal equations A^T * A * x = A^T * b.</p> <p>Parameters:</p> <ul> <li> <p><code>const QRDecomposition &amp;qr</code> : Precomputed QR decomposition.</p> </li> <li> <p><code>const Mat &amp;b</code> : Right-hand side vector (M\u00d71, where M \u2265 N).</p> </li> </ul> <p>Returns:</p> <p>Mat - Least squares solution vector (N\u00d71).</p> <p>Usage Insights:</p> <ul> <li> <p>Overdetermined Systems: Handles cases where there are more equations than unknowns.</p> </li> <li> <p>Numerical Stability: More stable than solving normal equations directly.</p> </li> <li> <p>Applications: </p> </li> <li> <p>Curve fitting</p> </li> <li> <p>Data regression</p> </li> <li> <p>Signal processing</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#pseudo-inverse","title":"Pseudo-Inverse","text":"<pre><code>static Mat Mat::pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance = 1e-6f);\n</code></pre> <p>Description:</p> <p>Compute the Moore-Penrose pseudo-inverse A\u207a using SVD decomposition. Works for rank-deficient or non-square matrices where the regular inverse doesn't exist.</p> <p>Mathematical Principle:</p> <p>For A = U * S * V^T, the pseudo-inverse is A\u207a = V * S\u207a * U^T, where S\u207a has 1/\u03c3\u1d62 for singular values \u03c3\u1d62 &gt; tolerance, and 0 otherwise.</p> <p>Properties of Pseudo-Inverse:</p> <ul> <li> <p>A * A\u207a * A = A</p> </li> <li> <p>A\u207a * A * A\u207a = A\u207a</p> </li> <li> <p>(A * A\u207a)^T = A * A\u207a</p> </li> <li> <p>(A\u207a * A)^T = A\u207a * A</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>const SVDDecomposition &amp;svd</code> : Precomputed SVD decomposition.</p> </li> <li> <p><code>float tolerance</code> : Threshold for singular values (default: 1e-6). Singular values below this are treated as zero.</p> </li> </ul> <p>Returns:</p> <p>Mat - Pseudo-inverse matrix.</p> <p>Usage Insights:</p> <ul> <li> <p>Rank-Deficient Systems: Provides solution for systems where A is not full rank.</p> </li> <li> <p>Minimum Norm Solution: For underdetermined systems, gives the solution with minimum ||x||\u2082.</p> </li> <li> <p>Least Squares: For overdetermined systems, gives the least squares solution.</p> </li> <li> <p>Applications:</p> </li> <li> <p>Control systems</p> </li> <li> <p>Signal processing</p> </li> <li> <p>Machine learning (regularization)</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#linear-algebra-eigenvalues-eigenvectors","title":"LINEAR ALGEBRA - Eigenvalues &amp; Eigenvectors","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#struct-mateigenpair","title":"Struct: <code>Mat::EigenPair</code>","text":"<pre><code>Mat::EigenPair::EigenPair();\n// fields:\n// float eigenvalue;      // eigenvalue (largest-magnitude for power_iteration, smallest for inverse_power_iteration)\n// Mat eigenvector;       // corresponding eigenvector (n x 1)\n// int iterations;        // number of iterations (for iterative methods)\n// tiny_error_t status;   // computation status (TINY_OK / error code)\n</code></pre> <p>Description:</p> <p>Container for a single eigenvalue/eigenvector result and related metadata. Typically returned by <code>power_iteration</code> or <code>inverse_power_iteration</code>.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#struct-mateigendecomposition","title":"Struct: <code>Mat::EigenDecomposition</code>","text":"<pre><code>Mat::EigenDecomposition::EigenDecomposition();\n// fields:\n// Mat eigenvalues;    // n x 1 matrix storing eigenvalues\n// Mat eigenvectors;   // n x n matrix, columns are eigenvectors\n// int iterations;     // iterations used by the algorithm\n// tiny_error_t status; // computation status\n</code></pre> <p>Description:</p> <p>Container for a full eigendecomposition result (all eigenvalues and eigenvectors).</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#power-iteration-dominant-eigenpair","title":"Power Iteration (dominant eigenpair)","text":"<pre><code>Mat::EigenPair Mat::power_iteration(int max_iter, float tolerance) const;\n</code></pre> <p>Description:</p> <p>Compute the dominant (largest-magnitude) eigenvalue and its eigenvector using the power iteration method. Fast method suitable for real-time SHM applications to quickly identify primary frequency.</p> <p>Mathematical Principle:</p> <p>Power iteration finds the eigenvalue with the largest absolute value by iteratively applying the matrix to a vector:</p> <ol> <li> <p>Start with random vector v\u2080</p> </li> <li> <p>Iterate: v\u2096\u208a\u2081 = A * v\u2096 / ||A * v\u2096||</p> </li> <li> <p>Eigenvalue estimate: \u03bb\u2096 = (v\u2096^T * A * v\u2096) / (v\u2096^T * v\u2096) (Rayleigh quotient)</p> </li> </ol> <p>Convergence:</p> <p>The method converges to the dominant eigenvalue if:</p> <ul> <li> <p>The dominant eigenvalue is unique (|\u03bb\u2081| &gt; |\u03bb\u2082| \u2265 ... \u2265 |\u03bb\u2099|)</p> </li> <li> <p>The initial vector has a non-zero component in the direction of the dominant eigenvector</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>int max_iter</code> : Maximum number of iterations (typical default: 1000).</p> </li> <li> <p><code>float tolerance</code> : Convergence tolerance (e.g. 1e-6). Convergence is checked by |\u03bb\u2096 - \u03bb\u2096\u208b\u2081| &lt; tolerance * |\u03bb\u2096|.</p> </li> </ul> <p>Returns:</p> <p><code>EigenPair</code> containing <code>eigenvalue</code>, <code>eigenvector</code>, <code>iterations</code>, and <code>status</code>.</p> <p>Usage Insights:</p> <ul> <li> <p>Real-Time Applications: Fast convergence for well-separated eigenvalues, suitable for real-time structural health monitoring.</p> </li> <li> <p>Initialization: The implementation uses a smart initialization strategy (sum of column absolute values) to avoid convergence to smaller eigenvalues.</p> </li> <li> <p>Convergence Rate: Convergence is linear with rate |\u03bb\u2082|/|\u03bb\u2081|. Slower when eigenvalues are close.</p> </li> <li> <p>Limitations: </p> </li> <li> <p>Only finds one eigenvalue-eigenvector pair</p> </li> <li> <p>Requires |\u03bb\u2081| &gt; |\u03bb\u2082| (dominant eigenvalue must be unique)</p> </li> <li> <p>May converge slowly if eigenvalues are close</p> </li> <li> <p>Applications:</p> </li> <li> <p>Principal component analysis (first principal component)</p> </li> <li> <p>PageRank algorithm</p> </li> <li> <p>Structural dynamics (fundamental frequency)</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#inverse-power-iteration-smallest-eigenpair","title":"Inverse Power Iteration (smallest eigenpair)","text":"<pre><code>Mat::EigenPair Mat::inverse_power_iteration(int max_iter, float tolerance) const;\n</code></pre> <p>Description:</p> <p>Compute the smallest (minimum magnitude) eigenvalue and its eigenvector using the inverse power iteration method. Critical for system identification - finds fundamental frequency/lowest mode in structural dynamics. This method is essential for SHM applications where the smallest eigenvalue corresponds to the fundamental frequency of the system.</p> <p>Mathematical Principle:</p> <p>Inverse power iteration applies power iteration to A^(-1), which has eigenvalues 1/\u03bb\u1d62. Since 1/\u03bb\u2099 is the largest eigenvalue of A^(-1), the method converges to the smallest eigenvalue of A:</p> <ol> <li> <p>Start with vector v\u2080</p> </li> <li> <p>Iterate: Solve A * y\u2096 = v\u2096, then v\u2096\u208a\u2081 = y\u2096 / ||y\u2096||</p> </li> <li> <p>Eigenvalue estimate: \u03bb\u2096 = (v\u2096^T * A * v\u2096) / (v\u2096^T * v\u2096) (Rayleigh quotient)</p> </li> </ol> <p>Convergence:</p> <p>Converges to the smallest eigenvalue if:</p> <ul> <li> <p>The smallest eigenvalue is unique (|\u03bb\u2099| &lt; |\u03bb\u2099\u208b\u2081| \u2264 ... \u2264 |\u03bb\u2081|)</p> </li> <li> <p>Matrix A is invertible (non-singular)</p> </li> <li> <p>Initial vector has component in direction of smallest eigenvector</p> </li> </ul> <p>Parameters:</p> <ul> <li> <p><code>int max_iter</code> : Maximum number of iterations (default: 1000).</p> </li> <li> <p><code>float tolerance</code> : Convergence tolerance (default: 1e-6). Uses relative tolerance: |\u03bb\u2096 - \u03bb\u2096\u208b\u2081| &lt; tolerance * max(|\u03bb\u2096|, 1.0).</p> </li> </ul> <p>Returns:</p> <p><code>EigenPair</code> containing the smallest eigenvalue, eigenvector, iterations, and status.</p> <p>Algorithm Steps:</p> <ol> <li>Initialize normalized eigenvector v (with alternating signs to avoid alignment with dominant eigenvector)</li> <li>Iterate: Solve A * y = v (equivalent to y = A^(-1) * v) using <code>solve()</code></li> <li>Normalize y to get new v</li> <li>Compute eigenvalue estimate using Rayleigh quotient: \u03bb = (v^T * A * v) / (v^T * v)</li> <li>Check convergence using relative tolerance</li> </ol> <p>Usage Insights:</p> <ul> <li> <p>System Identification: Essential for finding fundamental frequencies in structural dynamics, where the smallest eigenvalue corresponds to the lowest natural frequency.</p> </li> <li> <p>Numerical Stability: The implementation includes checks for singular matrices and handles near-singular cases gracefully.</p> </li> <li> <p>Initialization Strategy: Uses alternating sign pattern to avoid convergence to larger eigenvalues, ensuring convergence to the smallest eigenvalue.</p> </li> <li> <p>Performance: Each iteration requires solving a linear system (O(n\u00b3) for dense matrices), but typically converges in fewer iterations than power iteration.</p> </li> <li> <p>Complementary to Power Iteration: </p> </li> <li> <p>Power iteration: finds \u03bb_max (highest frequency)</p> </li> <li> <p>Inverse power iteration: finds \u03bb_min (fundamental frequency)</p> </li> <li> <p>Together they provide the frequency range of the system</p> </li> <li> <p>Applications:</p> </li> <li> <p>Structural health monitoring (fundamental frequency detection)</p> </li> <li> <p>Modal analysis (lowest mode shape)</p> </li> <li> <p>System identification</p> </li> <li> <p>Stability analysis (smallest eigenvalue indicates stability margin)</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>Requires a square matrix and non-null data pointer; returns an error status otherwise.</p> </li> <li> <p>The matrix must be invertible (non-singular) for this method to work. If the matrix is singular or near-singular, the method will fail gracefully.</p> </li> <li> <p>Inverse power iteration only returns the smallest eigenpair. For full spectrum, use eigendecomposition functions below.</p> </li> <li> <p>This method is complementary to power iteration: power iteration finds the largest eigenvalue, while inverse power iteration finds the smallest eigenvalue.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#jacobi-eigendecomposition-symmetric-matrices","title":"Jacobi Eigendecomposition (symmetric matrices)","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose_jacobi(float tolerance, int max_iter) const;\n</code></pre> <p>Description:</p> <p>Compute full eigendecomposition using the Jacobi method. Recommended for symmetric matrices (good accuracy and stability for structural dynamics applications). Robust and accurate, ideal for structural dynamics matrices in SHM.</p> <p>Mathematical Principle:</p> <p>The Jacobi method diagonalizes a symmetric matrix through a series of orthogonal similarity transformations (Givens rotations):</p> <ol> <li> <p>Find largest off-diagonal element a\u209aq</p> </li> <li> <p>Compute rotation angle \u03b8 to zero this element</p> </li> <li> <p>Apply rotation: A' = J^T * A * J, where J is the rotation matrix</p> </li> <li> <p>Repeat until all off-diagonal elements are below tolerance</p> </li> </ol> <p>Convergence:</p> <p>The method converges when the maximum off-diagonal element is below tolerance. Each rotation zeros one off-diagonal element, and the process continues until the matrix is diagonal.</p> <p>Parameters:</p> <ul> <li> <p><code>float tolerance</code> : Convergence threshold (e.g. 1e-6). Maximum allowed magnitude of off-diagonal elements.</p> </li> <li> <p><code>int max_iter</code> : Maximum iterations (e.g. 100). Typically converges in O(n\u00b2) iterations for n\u00d7n matrices.</p> </li> </ul> <p>Returns:</p> <p><code>EigenDecomposition</code> with <code>eigenvalues</code>, <code>eigenvectors</code>, <code>iterations</code>, and <code>status</code>.</p> <p>Usage Insights:</p> <ul> <li> <p>Symmetric Matrices: Designed for symmetric matrices. For non-symmetric matrices, use QR method.</p> </li> <li> <p>Numerical Stability: Very stable for symmetric matrices, with good preservation of orthogonality.</p> </li> <li> <p>Accuracy: High accuracy, suitable for applications requiring precise eigenvalue/eigenvector pairs.</p> </li> <li> <p>Performance: O(n\u00b3) per iteration, but typically requires fewer iterations than QR for symmetric matrices.</p> </li> <li> <p>Applications:</p> </li> <li> <p>Structural dynamics: Stiffness and mass matrices are symmetric</p> </li> <li> <p>Principal Component Analysis (PCA)</p> </li> <li> <p>Spectral clustering</p> </li> <li> <p>Quadratic forms optimization</p> </li> </ul> <p>Notes:</p> <p>If the matrix is not approximately symmetric the function will warn, though it may still run. For non-symmetric matrices prefer the QR method.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#qr-eigendecomposition-general-matrices","title":"QR Eigendecomposition (general matrices)","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose_qr(int max_iter, float tolerance) const;\n</code></pre> <p>Description:</p> <p>Compute eigendecomposition using the QR algorithm. Works for general (possibly non-symmetric) matrices. Supports non-symmetric matrices, but may have complex eigenvalues (only real part returned).</p> <p>Mathematical Principle:</p> <p>The QR algorithm iteratively applies QR decomposition:</p> <ol> <li> <p>Start with A\u2080 = A</p> </li> <li> <p>For k = 0, 1, 2, ...: Compute QR decomposition: A\u2096 = Q\u2096 * R\u2096, then update: A\u2096\u208a\u2081 = R\u2096 * Q\u2096</p> </li> <li> <p>A\u2096 converges to upper triangular form (Schur form), with eigenvalues on the diagonal</p> </li> </ol> <p>Convergence:</p> <p>The algorithm converges when A\u2096 is approximately upper triangular (sub-diagonal elements &lt; tolerance). The eigenvalues appear on the diagonal, and eigenvectors are accumulated from Q matrices.</p> <p>Parameters:</p> <ul> <li> <p><code>int max_iter</code> : Maximum number of QR iterations (default: 100).</p> </li> <li> <p><code>float tolerance</code> : Convergence tolerance (e.g. 1e-6). Uses relative tolerance comparing sub-diagonal elements to diagonal elements.</p> </li> </ul> <p>Returns:</p> <p><code>EigenDecomposition</code> containing eigenvalues, eigenvectors, iterations and status.</p> <p>Usage Insights:</p> <ul> <li> <p>General Matrices: Can handle non-symmetric matrices, unlike Jacobi method.</p> </li> <li> <p>Complex Eigenvalues: Non-symmetric matrices may have complex eigenvalues; current implementation returns real parts only.</p> </li> <li> <p>Numerical Stability: Uses modified Gram-Schmidt with re-orthogonalization for improved stability.</p> </li> <li> <p>Performance: O(n\u00b3) per iteration. May require many iterations for convergence, especially for ill-conditioned matrices.</p> </li> <li> <p>Convergence Acceleration: The implementation could benefit from shifts (Wilkinson shift) for faster convergence, but current version uses basic QR iteration.</p> </li> <li> <p>Applications:</p> </li> <li> <p>General matrix eigenvalue problems</p> </li> <li> <p>Dynamical systems analysis</p> </li> <li> <p>Control theory (system poles)</p> </li> </ul> <p>Notes:</p> <p>QR uses Gram\u2013Schmidt for Q/R in this implementation; it can be less stable for ill-conditioned matrices. For symmetric matrices, Jacobi is preferred due to better stability and accuracy.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#automatic-eigendecomposition","title":"Automatic Eigendecomposition","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose(float tolerance) const;\n</code></pre> <p>Description:</p> <p>Convenience interface that automatically selects the optimal algorithm based on matrix properties. It tests symmetry with <code>is_symmetric(tolerance * 10.0f)</code>. If approximately symmetric, it uses Jacobi; otherwise it runs QR. Convenient interface for edge computing applications.</p> <p>Algorithm Selection:</p> <ol> <li>Test if matrix is symmetric: <code>is_symmetric(tolerance * 10.0f)</code></li> <li>If symmetric \u2192 use <code>eigendecompose_jacobi(tolerance, 100)</code> (more stable and accurate)</li> <li>If not symmetric \u2192 use <code>eigendecompose_qr(100, tolerance)</code> (handles general matrices)</li> </ol> <p>Parameters:</p> <ul> <li><code>float tolerance</code> : Used for symmetry test and decomposition convergence (recommended 1e-6).</li> </ul> <p>Returns:</p> <p><code>EigenDecomposition</code> containing all eigenvalues and eigenvectors.</p> <p>Usage Insights:</p> <ul> <li> <p>Automatic Optimization: Saves the user from manually choosing the algorithm, while still providing optimal performance.</p> </li> <li> <p>Edge Computing: Ideal for embedded systems where you want good performance without manual tuning.</p> </li> <li> <p>Robustness: The symmetry test uses a relaxed tolerance (10\u00d7) to handle numerical errors, ensuring symmetric matrices are correctly identified.</p> </li> </ul> <p>Usage Tips:</p> <ul> <li> <p>Known Symmetry: If the matrix is known to be symmetric (e.g. stiffness or mass matrices), call <code>eigendecompose_jacobi</code> directly for best stability and slightly better performance.</p> </li> <li> <p>Unknown Properties: For general matrices or unknown symmetry, use <code>eigendecompose</code> for automatic selection.</p> </li> <li> <p>Performance Considerations: </p> </li> <li>Eigendecomposition is computationally expensive for large matrices on embedded platforms</li> <li>For n &gt; 20, consider reduced-order methods or iterative methods (power iteration) when only a few eigenvalues are needed</li> <li> <p>For real-time applications, use <code>power_iteration()</code> or <code>inverse_power_iteration()</code> for single eigenvalues</p> </li> <li> <p>Memory Usage: Full eigendecomposition requires storing all eigenvectors (n\u00d7n matrix), which can be memory-intensive for large matrices.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#stream-operators","title":"STREAM OPERATORS","text":""},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-output-stream-operator","title":"Matrix output stream operator","text":"<pre><code>std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m);\n</code></pre> <p>Description:</p> <p>Overloaded output stream operator for the matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>std::ostream &amp;os</code> : Output stream.</p> </li> <li> <p><code>const Mat &amp;m</code> : Matrix to be output.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#roi-output-stream-operator","title":"ROI output stream operator","text":"<pre><code>std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi);\n</code></pre> <p>Description:</p> <p>Overloaded output stream operator for the ROI structure.</p> <p>Parameters:</p> <ul> <li> <p><code>std::ostream &amp;os</code> : Output stream.</p> </li> <li> <p><code>const Mat::ROI &amp;roi</code> : ROI structure.</p> </li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#matrix-input-stream-operator","title":"Matrix input stream operator","text":"<pre><code>std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m);\n</code></pre> <p>Description:</p> <p>Overloaded input stream operator for the matrix.</p> <p>Parameters:</p> <ul> <li> <p><code>std::istream &amp;is</code> : Input stream.</p> </li> <li> <p><code>Mat &amp;m</code> : Matrix to be input.</p> </li> </ul> <p>Tip</p> <p>This section is actually kind of overlapping with print function in terms of showing the matrix.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#global-arithmetic-operators","title":"GLOBAL ARITHMETIC OPERATORS","text":"<p>Non-Modifying Operations</p> <p>The operators in this section return a new matrix object, which is the result of the operation. The original matrices remain unchanged. These are functional-style operations that don't modify their operands, making them safe for use with const references and temporary objects.</p> <p>When to Use</p> <ul> <li>Use global operators (A + B) when you want to preserve original matrices</li> <li>Use member operators (A += B) when you want to modify the matrix in-place (more memory efficient)</li> </ul>"},{"location":"MATH/MATRIX/tiny-matrix-api/#add-matrix_1","title":"Add matrix","text":"<pre><code>Mat operator+(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Adds two matrices element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : First matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Second matrix.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A+B.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#add-constant_1","title":"Add constant","text":"<pre><code>Mat operator+(const Mat &amp;A, float C);\n</code></pre> <p>Description:</p> <p>Adds a constant to a matrix element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> <li> <p><code>float C</code> : Input constant.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A+C.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#subtract-matrix_1","title":"Subtract matrix","text":"<pre><code>Mat operator-(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Subtracts two matrices element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : First matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Second matrix.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A-B.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#subtract-constant_1","title":"Subtract constant","text":"<pre><code>Mat operator-(const Mat &amp;A, float C);\n</code></pre> <p>Description:</p> <p>Subtracts a constant from a matrix element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> <li> <p><code>float C</code> : Input constant.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A-C.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#multiply-matrix_1","title":"Multiply matrix","text":"<pre><code>Mat operator*(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Multiplies two matrices (matrix multiplication).</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : First matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Second matrix.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A*B.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#multiply-constant_1","title":"Multiply constant","text":"<pre><code>Mat operator*(const Mat &amp;A, float C);\n</code></pre> <p>Description:</p> <p>Multiplies a matrix by a constant element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> <li> <p><code>float C</code> : Floating point value.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A*C.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#multiply-constant-left-side","title":"Multiply constant (left side)","text":"<pre><code>Mat operator*(float C, const Mat &amp;A);\n</code></pre> <p>Description:</p> <p>Multiplies a constant by a matrix element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>float C</code> : Floating point value.</p> </li> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix C*A.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#divide-matrix-by-constant","title":"Divide matrix (by constant)","text":"<pre><code>Mat operator/(const Mat &amp;A, float C);\n</code></pre> <p>Description:</p> <p>Divides a matrix by a constant element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> <li> <p><code>float C</code> : Floating point value.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix A/C.</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#divide-matrix-element-wise_1","title":"Divide matrix (element-wise)","text":"<pre><code>Mat operator/(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Divides matrix A by matrix B element-wise.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : Input matrix A.</p> </li> <li> <p><code>const Mat &amp;B</code> : Input matrix B.</p> </li> </ul> <p>Returns:</p> <p>Mat - Result matrix C, where C[i,j] = A[i,j]/B[i,j].</p>"},{"location":"MATH/MATRIX/tiny-matrix-api/#equality-check","title":"Equality check","text":"<pre><code>bool operator==(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>Description:</p> <p>Checks if the specified matrices are equal.</p> <p>Parameters:</p> <ul> <li> <p><code>const Mat &amp;A</code> : First matrix.</p> </li> <li> <p><code>const Mat &amp;B</code> : Second matrix.</p> </li> </ul> <p>Returns:</p> <p>bool - true if equal, false otherwise.</p>"},{"location":"MATH/MATRIX/tiny-matrix-code/","title":"CODE","text":""},{"location":"MATH/MATRIX/tiny-matrix-code/#tiny_matrixhpp","title":"tiny_matrix.hpp","text":"<pre><code>/**\n * @file tiny_matrix.hpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @note This file is built on top of the mat.h file from the ESP-DSP library.\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// TinyMath\n#include \"tiny_math_config.h\"\n#include \"tiny_vec.h\"\n#include \"tiny_mat.h\"\n\n// Standard Libraries\n#include &lt;iostream&gt;\n#include &lt;stdint.h&gt;\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n// ESP32 DSP C++ Matrix library\n#include \"mat.h\"\n#endif\n\n/* STATEMENTS */\nnamespace tiny\n{\n    class Mat\n    {\n    public:\n        // ============================================================================\n        // Matrix Metadata\n        // ============================================================================\n        int row;         //&lt; number of rows\n        int col;         //&lt; number of columns\n        int pad;         //&lt; number of paddings between 2 rows\n        int stride;      //&lt; stride = (number of elements in a row) + padding\n        int element;     //&lt; number of elements = rows * cols\n        int memory;      //&lt; size of the data buffer = rows * stride\n        float *data;     //&lt; pointer to the data buffer\n        float *temp;     //&lt; pointer to the temporary data buffer\n        bool ext_buff;   //&lt; flag indicates that matrix use external buffer\n        bool sub_matrix; //&lt; flag indicates that matrix is a subset of another matrix\n\n        // ============================================================================\n        // Rectangular ROI Structure\n        // ============================================================================\n        /**\n         * @name Region of Interest (ROI) Structure\n         * @brief This is the structure for ROI\n         */\n        struct ROI\n        {\n            int pos_x;  ///&lt; starting column index\n            int pos_y;  ///&lt; starting row index\n            int width;  ///&lt; width of ROI (columns)\n            int height; ///&lt; height of ROI (rows)\n\n            ROI(int pos_x = 0, int pos_y = 0, int width = 0, int height = 0);\n            void resize_roi(int pos_x, int pos_y, int width, int height);\n            int area_roi(void) const;\n        };\n\n        // ============================================================================\n        // Printing Functions\n        // ============================================================================\n        void print_info() const;\n        void print_matrix(bool show_padding);\n\n        // ============================================================================\n        // Constructors &amp; Destructor\n        // ============================================================================\n        void alloc_mem(); // Allocate internal memory\n        Mat();\n        Mat(int rows, int cols);\n        Mat(int rows, int cols, int stride);\n        Mat(float *data, int rows, int cols);\n        Mat(float *data, int rows, int cols, int stride);\n        Mat(const Mat &amp;src);\n        ~Mat();\n\n        // ============================================================================\n        // Element Access\n        // ============================================================================\n        inline float &amp;operator()(int row, int col) { return data[row * stride + col]; }\n        inline const float &amp;operator()(int row, int col) const { return data[row * stride + col]; }\n\n        // ============================================================================\n        // Data Manipulation\n        // ============================================================================\n        tiny_error_t copy_paste(const Mat &amp;src, int row_pos, int col_pos);\n        tiny_error_t copy_head(const Mat &amp;src);\n        Mat view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const;\n        Mat view_roi(const Mat::ROI &amp;roi) const;\n        Mat copy_roi(int start_row, int start_col, int roi_rows, int roi_cols);\n        Mat copy_roi(const Mat::ROI &amp;roi);\n        Mat block(int start_row, int start_col, int block_rows, int block_cols);\n        void swap_rows(int row1, int row2);\n        void swap_cols(int col1, int col2);\n        void clear(void);\n\n        // ============================================================================\n        // Arithmetic Operators\n        // ============================================================================\n        Mat &amp;operator=(const Mat &amp;src);    // Copy assignment\n        Mat &amp;operator+=(const Mat &amp;A);     // Add matrix\n        Mat &amp;operator+=(float C);          // Add constant\n        Mat &amp;operator-=(const Mat &amp;A);     // Subtract matrix\n        Mat &amp;operator-=(float C);          // Subtract constant \n        Mat &amp;operator*=(const Mat &amp;A);     // Multiply matrix\n        Mat &amp;operator*=(float C);          // Multiply constant\n        Mat &amp;operator/=(const Mat &amp;B);     // Divide matrix\n        Mat &amp;operator/=(float C);          // Divide constant\n        Mat operator^(int C);              // Exponentiation\n\n        // ============================================================================\n        // Linear Algebra - Basic Operations\n        // ============================================================================\n        Mat transpose();                   // Transpose matrix\n        float determinant();               // Compute determinant (auto-selects method based on size)\n        float determinant_laplace();        // Compute determinant using Laplace expansion (O(n!), for small matrices)\n        float determinant_lu();            // Compute determinant using LU decomposition (O(n\u00b3), efficient for large matrices)\n        float determinant_gaussian();      // Compute determinant using Gaussian elimination (O(n\u00b3), efficient for large matrices)\n        Mat adjoint();                     // Compute adjoint matrix\n        Mat inverse_adjoint();            // Compute inverse using adjoint method\n        void normalize();                  // Normalize matrix\n        float norm() const;                // Compute matrix norm\n        float dotprod(const Mat &amp;A, const Mat &amp;B);  // Dot product\n\n        // ============================================================================\n        // Linear Algebra - Matrix Utilities\n        // ============================================================================\n        static Mat eye(int size);          // Create identity matrix\n        static Mat ones(int rows, int cols);  // Create matrix filled with ones\n        static Mat ones(int size);         // Create square matrix filled with ones\n        static Mat augment(const Mat &amp;A, const Mat &amp;B);  // Horizontal concatenation [A | B]\n        static Mat vstack(const Mat &amp;A, const Mat &amp;B);   // Vertical concatenation [A; B]\n\n        /**\n         * @brief Gram-Schmidt orthogonalization process\n         * @note Orthogonalizes a set of vectors using the Gram-Schmidt process\n         * @param vectors Input matrix where each column is a vector to be orthogonalized\n         * @param orthogonal_vectors Output matrix for orthogonalized vectors (each column is orthogonal)\n         * @param coefficients Output matrix for projection coefficients (R matrix in QR decomposition)\n         * @param tolerance Minimum norm threshold for linear independence check\n         * @return true if successful, false if input is invalid\n         */\n        static bool gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                               Mat &amp;coefficients, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Linear Algebra - Matrix Operations\n        // ============================================================================\n        Mat minor(int row, int col);       // Minor matrix (submatrix after removing row and col)\n        Mat cofactor(int row, int col);    // Cofactor matrix\n        Mat gaussian_eliminate() const;    // Gaussian elimination\n        Mat row_reduce_from_gaussian();   // Row reduction from Gaussian form\n        Mat inverse_gje();                 // Inverse using Gaussian-Jordan elimination\n\n        // ============================================================================\n        // Linear Algebra - Linear System Solving\n        // ============================================================================\n        Mat solve(const Mat &amp;A, const Mat &amp;b) const;  // Solve Ax = b using Gaussian elimination\n        Mat band_solve(Mat A, Mat b, int k);          // Solve banded system\n        Mat roots(Mat A, Mat y);                      // Alternative solve method\n\n        // ============================================================================\n        // Matrix Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct LUDecomposition;\n        struct CholeskyDecomposition;\n        struct QRDecomposition;\n        struct SVDDecomposition;\n\n        // Matrix property checks\n        bool is_symmetric(float tolerance = 1e-6f) const;\n        bool is_positive_definite(float tolerance = 1e-6f) const;\n\n        // Decomposition methods\n        LUDecomposition lu_decompose(bool use_pivoting = true) const;\n        CholeskyDecomposition cholesky_decompose() const;\n        QRDecomposition qr_decompose() const;\n        SVDDecomposition svd_decompose(int max_iter = 100, float tolerance = 1e-6f) const;\n\n        // Solve using decomposition (more efficient for multiple RHS)\n        static Mat solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b);\n        static Mat solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b);\n        static Mat solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b);  // Least squares solution\n\n        // Pseudo-inverse using SVD (for rank-deficient or non-square matrices)\n        static Mat pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Eigenvalue &amp; Eigenvector Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct EigenPair;\n        struct EigenDecomposition;\n\n        // Single eigenvalue methods (fast, for real-time applications)\n        EigenPair power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n        EigenPair inverse_power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n\n        // Complete eigendecomposition methods\n        EigenDecomposition eigendecompose_jacobi(float tolerance = 1e-6f, int max_iter = 100) const;\n        EigenDecomposition eigendecompose_qr(int max_iter = 100, float tolerance = 1e-6f) const;\n        EigenDecomposition eigendecompose(float tolerance = 1e-6f) const;  // Auto-select method\n\n    protected:\n\n    private:\n\n    };\n\n    // ============================================================================\n    // Matrix Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold LU decomposition results\n     * @note A = L * U, where L is lower triangular and U is upper triangular\n     */\n    struct Mat::LUDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix (with unit diagonal)\n        Mat U;                 ///&lt; Upper triangular matrix\n        Mat P;                 ///&lt; Permutation matrix (if pivoting used)\n        bool pivoted;          ///&lt; Whether pivoting was used\n        tiny_error_t status;   ///&lt; Computation status\n\n        LUDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold Cholesky decomposition results\n     * @note A = L * L^T, where L is lower triangular (for symmetric positive definite matrices)\n     */\n    struct Mat::CholeskyDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        CholeskyDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold QR decomposition results\n     * @note A = Q * R, where Q is orthogonal and R is upper triangular\n     */\n    struct Mat::QRDecomposition\n    {\n        Mat Q;                 ///&lt; Orthogonal matrix (Q^T * Q = I)\n        Mat R;                 ///&lt; Upper triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        QRDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold SVD decomposition results\n     * @note A = U * S * V^T, where U and V are orthogonal, S is diagonal (singular values)\n     */\n    struct Mat::SVDDecomposition\n    {\n        Mat U;                 ///&lt; Left singular vectors (orthogonal matrix)\n        Mat S;                 ///&lt; Singular values (diagonal matrix or vector)\n        Mat V;                 ///&lt; Right singular vectors (orthogonal matrix, V^T)\n        int rank;              ///&lt; Numerical rank of the matrix\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        SVDDecomposition();\n    };\n\n    // ============================================================================\n    // Eigenvalue &amp; Eigenvector Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold a single eigenvalue-eigenvector pair\n     * @note Used primarily for power iteration method\n     */\n    struct Mat::EigenPair\n    {\n        float eigenvalue;      ///&lt; Eigenvalue (real part)\n        Mat eigenvector;       ///&lt; Corresponding eigenvector (column vector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenPair();\n    };\n\n    /**\n     * @brief Structure to hold complete eigenvalue decomposition results\n     * @note Contains all eigenvalues and eigenvectors\n     */\n    struct Mat::EigenDecomposition\n    {\n        Mat eigenvalues;       ///&lt; Eigenvalues (diagonal matrix or vector)\n        Mat eigenvectors;      ///&lt; Eigenvector matrix (each column is an eigenvector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenDecomposition();\n    };\n\n    // ============================================================================\n    // Stream Operators\n    // ============================================================================\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m);\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi);\n    std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m);\n\n    // ============================================================================\n    // Global Arithmetic Operators\n    // ============================================================================\n    Mat operator+(const Mat &amp;A, const Mat &amp;B);\n    Mat operator+(const Mat &amp;A, float C);\n    Mat operator-(const Mat &amp;A, const Mat &amp;B);\n    Mat operator-(const Mat &amp;A, float C);\n    Mat operator*(const Mat &amp;A, const Mat &amp;B);\n    Mat operator*(const Mat &amp;A, float C);\n    Mat operator*(float C, const Mat &amp;A);\n    Mat operator/(const Mat &amp;A, float C);\n    Mat operator/(const Mat &amp;A, const Mat &amp;B);\n    bool operator==(const Mat &amp;A, const Mat &amp;B);\n\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-code/#tiny_matrixcpp","title":"tiny_matrix.cpp","text":"<pre><code>/**\n * @file tiny_matrix.cpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the source file for the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n// TinyMath\n#include \"tiny_matrix.hpp\"\n\n// Standard Libraries\n#include &lt;cstring&gt;\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n#include &lt;cmath&gt;\n#include &lt;cinttypes&gt;\n#include &lt;iomanip&gt;\n#include &lt;vector&gt;\n\n/* LIBRARIE CONTENTS */\nnamespace tiny\n{\n    // ============================================================================\n    // Rectangular ROI Structure\n    // ============================================================================\n    /**\n     * @brief Construct a new Mat:: R O I:: R O I object\n     * \n     * @param pos_x \n     * @param pos_y \n     * @param width \n     * @param height \n     */\n    Mat::ROI::ROI(int pos_x, int pos_y, int width, int height)\n    {\n        this-&gt;pos_x = pos_x;\n        this-&gt;pos_y = pos_y;\n        this-&gt;width = width;\n        this-&gt;height = height;\n    }\n\n    /**\n     * @brief resize the ROI structure\n     * \n     * @param pos_x starting column\n     * @param pos_y starting row\n     * @param width number of columns\n     * @param height number of rows\n     */\n    void Mat::ROI::resize_roi(int pos_x, int pos_y, int width, int height)\n    {\n        this-&gt;pos_x = pos_x;\n        this-&gt;pos_y = pos_y;\n        this-&gt;width = width;\n        this-&gt;height = height;\n    }\n\n    /**\n     * @brief calculate the area of the ROI structure - how many elements covered\n     * \n     * @return int \n     */\n    int Mat::ROI::area_roi(void) const\n    {\n        return this-&gt;width * this-&gt;height;\n    }\n\n    // ============================================================================\n    // Printing Functions\n    // ============================================================================\n    /**\n     * @name Mat::PrintHead()\n     * @brief Print the header of the matrix.\n     */\n    void Mat::print_info() const\n    {\n        std::cout &lt;&lt; \"Matrix Info &gt;&gt;&gt;\\n\";\n\n        // Basic matrix metadata\n        std::cout &lt;&lt; \"rows            \" &lt;&lt; this-&gt;row &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"cols            \" &lt;&lt; this-&gt;col &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"elements        \" &lt;&lt; this-&gt;element;\n\n        // Check if elements match rows * cols\n        if (this-&gt;element != this-&gt;row * this-&gt;col)\n        {\n            std::cout &lt;&lt; \"   [Warning] Mismatch! Expected: \" &lt;&lt; (this-&gt;row * this-&gt;col);\n        }\n        std::cout &lt;&lt; \"\\n\";\n\n        std::cout &lt;&lt; \"paddings        \" &lt;&lt; this-&gt;pad &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"stride          \" &lt;&lt; this-&gt;stride &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"memory          \" &lt;&lt; this-&gt;memory &lt;&lt; \"\\n\";\n\n        // Pointer information\n        std::cout &lt;&lt; \"data pointer    \" &lt;&lt; static_cast&lt;const void *&gt;(this-&gt;data) &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"temp pointer    \" &lt;&lt; static_cast&lt;const void *&gt;(this-&gt;temp) &lt;&lt; \"\\n\";\n\n        // Flags information\n        std::cout &lt;&lt; \"ext_buff        \" &lt;&lt; this-&gt;ext_buff;\n        if (this-&gt;ext_buff)\n        {\n            std::cout &lt;&lt; \"   (External buffer or View)\";\n        }\n        std::cout &lt;&lt; \"\\n\";\n\n        std::cout &lt;&lt; \"sub_matrix      \" &lt;&lt; this-&gt;sub_matrix;\n        if (this-&gt;sub_matrix)\n        {\n            std::cout &lt;&lt; \"   (This is a Sub-Matrix View)\";\n        }\n        std::cout &lt;&lt; \"\\n\";\n\n        // State warnings\n        if (this-&gt;sub_matrix &amp;&amp; !this-&gt;ext_buff)\n        {\n            std::cout &lt;&lt; \"[Warning] Sub-matrix is marked but ext_buff is false! Potential logic error.\\n\";\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cout &lt;&lt; \"[Info] No data buffer assigned to this matrix.\\n\";\n        }\n\n        std::cout &lt;&lt; \"&lt;&lt;&lt; Matrix Info\\n\";\n    }\n\n    /**\n     * @name Mat::print_matrix()\n     * @brief Print the matrix elements.\n     *\n     * @param show_padding If true, print the padding elements as well.\n     */\n    void Mat::print_matrix(bool show_padding)\n    {\n        if (this-&gt;data == nullptr)\n        {\n            std::cout &lt;&lt; \"[Error] Cannot print matrix: data pointer is null.\\n\";\n            return;\n        }\n\n        std::cout &lt;&lt; \"Matrix Elements &gt;&gt;&gt;\\n\";\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            // print the non-padding elements\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                std::cout &lt;&lt; std::setw(12) &lt;&lt; this-&gt;data[i * this-&gt;stride + j] &lt;&lt; \" \";\n            }\n\n            // if padding is enabled, print the padding elements\n            if (show_padding)\n            {\n                // print a separator first\n                std::cout &lt;&lt; \"      |\";\n\n                // print the padding elements\n                for (int j = this-&gt;col; j &lt; this-&gt;stride; ++j)\n                {\n                    if (j == this-&gt;col)\n                    {\n                        std::cout &lt;&lt; std::setw(7) &lt;&lt; this-&gt;data[i * this-&gt;stride + j] &lt;&lt; \" \";\n                    }\n                    else\n                    {\n                        // print the padding elements\n                        std::cout &lt;&lt; std::setw(12) &lt;&lt; this-&gt;data[i * this-&gt;stride + j] &lt;&lt; \" \";\n                    }\n                }\n            }\n\n            // print a new line after each row\n            std::cout &lt;&lt; \"\\n\";\n        }\n\n        std::cout &lt;&lt; \"&lt;&lt;&lt; Matrix Elements\\n\";\n        std::cout &lt;&lt; std::endl;\n    }\n\n    // ============================================================================\n    // Constructors &amp; Destructor\n    // ============================================================================\n    // memory allocation\n    /**\n     * @name Mat::allocate()\n     * @brief Allocate memory for the matrix according to the memory required.\n     */\n    void Mat::alloc_mem()\n    {\n        this-&gt;ext_buff = false;\n        this-&gt;memory = this-&gt;row * this-&gt;stride;\n        this-&gt;data = new float[this-&gt;memory];\n    }\n\n    /**\n     * @name Mat::Mat()\n     * @brief Constructor - default constructor: create a 1x1 matrix with only a zero element.\n     */\n    Mat::Mat()\n    {\n        this-&gt;row = 1;\n        this-&gt;col = 1;\n        this-&gt;pad = 0;\n        this-&gt;stride = 1;\n        this-&gt;element = 1;\n        this-&gt;memory = 1;\n        this-&gt;data = nullptr;\n        this-&gt;temp = nullptr;\n        this-&gt;ext_buff = false;\n        this-&gt;sub_matrix = false;\n        alloc_mem();\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Memory allocation failed in alloc_mem()\\n\";\n            // Memory allocation failed, object is in invalid state (data = nullptr)\n            // Caller should check data pointer before using the matrix\n            return;\n        }\n        std::memset(this-&gt;data, 0, this-&gt;memory * sizeof(float));\n    }\n\n    /**\n     * @name Mat::Mat(int rows, int cols)\n     * @brief Constructor - create a matrix with the specified number of rows and columns.\n     *\n     * @param rows Number of rows\n     * @param cols Number of columns\n     */\n    Mat::Mat(int rows, int cols)\n    {\n        this-&gt;row = rows;\n        this-&gt;col = cols;\n        this-&gt;pad = 0;\n        this-&gt;stride = cols;\n        this-&gt;element = rows * cols;\n        this-&gt;memory = rows * cols;\n        this-&gt;data = nullptr;\n        this-&gt;temp = nullptr;\n        this-&gt;ext_buff = false;\n        this-&gt;sub_matrix = false;\n        alloc_mem();\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Memory allocation failed in alloc_mem()\\n\";\n            // Memory allocation failed, object is in invalid state (data = nullptr)\n            // Caller should check data pointer before using the matrix\n            return;\n        }\n        std::memset(this-&gt;data, 0, this-&gt;memory * sizeof(float));\n    }\n    /**\n     * @name Mat::Mat(int rows, int cols, int stride)\n     * @brief Constructor - create a matrix with the specified number of rows, columns and stride.\n     *\n     * @param rows Number of rows\n     * @param cols Number of columns\n     * @param stride Stride (number of elements in a row)\n     */\n    Mat::Mat(int rows, int cols, int stride)\n    {\n        this-&gt;row = rows;\n        this-&gt;col = cols;\n        this-&gt;pad = stride - cols;\n        this-&gt;stride = stride;\n        this-&gt;element = rows * cols;\n        this-&gt;memory = rows * stride;\n        this-&gt;data = nullptr;\n        this-&gt;temp = nullptr;\n        this-&gt;ext_buff = false;\n        this-&gt;sub_matrix = false;\n        alloc_mem();\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Memory allocation failed in alloc_mem()\\n\";\n            // Memory allocation failed, object is in invalid state (data = nullptr)\n            // Caller should check data pointer before using the matrix\n            return;\n        }\n        std::memset(this-&gt;data, 0, this-&gt;memory * sizeof(float));\n    }\n\n    /**\n     * @name Mat::Mat(float *data, int rows, int cols)\n     * @brief Constructor - create a matrix with the specified number of rows, columns and external data.\n     *\n     * @param data Pointer to external data buffer\n     * @param rows Number of rows\n     * @param cols Number of columns\n     */\n    Mat::Mat(float *data, int rows, int cols)\n    {\n        this-&gt;row = rows;\n        this-&gt;col = cols;\n        this-&gt;pad = 0;\n        this-&gt;stride = cols;\n        this-&gt;element = rows * cols;\n        this-&gt;memory = rows * cols; // for external data, this item is actually not used\n        this-&gt;data = data;\n        this-&gt;temp = nullptr;\n        this-&gt;ext_buff = true;\n        this-&gt;sub_matrix = false;\n    }\n\n    /**\n     * @name Mat::Mat(float *data, int rows, int cols, int stride)\n     * @brief Constructor - create a matrix with the specified number of rows, columns and external data.\n     *\n     * @param data Pointer to external data buffer\n     * @param rows Number of rows\n     * @param cols Number of columns\n     * @param stride Stride (number of elements in a row)\n     */\n    Mat::Mat(float *data, int rows, int cols, int stride)\n    {\n        this-&gt;row = rows;\n        this-&gt;col = cols;\n        this-&gt;pad = stride - cols;\n        this-&gt;stride = stride;\n        this-&gt;element = rows * cols;\n        this-&gt;memory = rows * stride; // for external data, this item is actually not used\n        this-&gt;data = data;\n        this-&gt;temp = nullptr;\n        this-&gt;ext_buff = true;\n        this-&gt;sub_matrix = false;\n    }\n\n    /**\n     * @name Mat::Mat(const Mat &amp;src)\n     * @brief Copy constructor - create a matrix with the same properties as the source matrix.\n     *\n     * @param src Source matrix\n     */\n    Mat::Mat(const Mat &amp;src)\n    {\n        this-&gt;row = src.row;\n        this-&gt;col = src.col;\n        this-&gt;pad = src.pad;\n        this-&gt;stride = src.stride;\n        this-&gt;element = src.element;\n        this-&gt;memory = src.memory;\n\n        if (src.sub_matrix &amp;&amp; src.ext_buff)\n        {\n            // if the source is a view (submatrix), do shallow copy\n            this-&gt;data = src.data;\n            this-&gt;temp = nullptr;\n            this-&gt;ext_buff = true;\n            this-&gt;sub_matrix = true;\n        }\n        else\n        {\n            // otherwise do deep copy\n            this-&gt;data = nullptr;\n            this-&gt;temp = nullptr;\n            this-&gt;ext_buff = false;\n            this-&gt;sub_matrix = false;\n\n            if (src.data != nullptr)\n            {\n                alloc_mem();\n                if (this-&gt;data == nullptr)\n                {\n                    std::cerr &lt;&lt; \"[Error] Memory allocation failed in alloc_mem()\\n\";\n                    // Memory allocation failed, object is in invalid state (data = nullptr)\n                    // Caller should check data pointer before using the matrix\n                    return;\n                }\n                std::memcpy(this-&gt;data, src.data, this-&gt;memory * sizeof(float));\n            }\n        }\n    }\n\n    /**\n     * @name ~Mat()\n     * @brief Destructor - free the memory allocated for the matrix.\n     */\n    Mat::~Mat()\n    {\n        if (!this-&gt;ext_buff &amp;&amp; this-&gt;data)\n        {\n            delete[] this-&gt;data;\n        }\n        if (this-&gt;temp)\n        {\n            delete[] this-&gt;temp;\n        }\n    }\n\n    // ============================================================================\n    // Element Access\n    // ============================================================================\n    // Already defined by inline functions in the header file\n\n    // ============================================================================\n    // Data Manipulation\n    // ============================================================================\n\n    /**\n     * @name Mat::copy_paste(const Mat &amp;src, int row_pos, int col_pos)\n     * @brief Copy the elements of the source matrix into the destination matrix. The dimension of the current matrix must be larger than the source matrix.\n     * @brief This one does not share memory with the source matrix.\n     *\n     * @param src Source matrix\n     * @param row_pos Start row position of the destination matrix\n     * @param col_pos Start column position of the destination matrix\n     */\n    tiny_error_t Mat::copy_paste(const Mat &amp;src, int row_pos, int col_pos)\n    {\n        if ((row_pos + src.row) &gt; this-&gt;row)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid row position \" &lt;&lt; std::endl;\n            return TINY_ERR_INVALID_ARG;\n        }\n        if ((col_pos + src.col) &gt; this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid column position \" &lt;&lt; std::endl;\n            return TINY_ERR_INVALID_ARG;\n        }\n        for (size_t r = 0; r &lt; src.row; r++)\n        {\n            memcpy(&amp;this-&gt;data[(r + row_pos) * this-&gt;stride + col_pos], &amp;src.data[r * src.stride], src.col * sizeof(float));\n        }\n\n        return TINY_OK;\n    }\n\n    /**\n     * @name Mat::copy_head(const Mat &amp;src)\n     * @brief Copy the header of the source matrix into the destination matrix. The data pointer is shared.\n     *\n     * @param src Source matrix\n     */\n    tiny_error_t Mat::copy_head(const Mat &amp;src)\n    {\n        if (!this-&gt;ext_buff)\n        {\n            delete[] this-&gt;data;\n        }\n        this-&gt;row = src.row;\n        this-&gt;col = src.col;\n        this-&gt;element = src.element;\n        this-&gt;pad = src.pad;\n        this-&gt;stride = src.stride;\n        this-&gt;memory = src.memory;\n        this-&gt;data = src.data;\n        this-&gt;temp = src.temp;\n        this-&gt;ext_buff = src.ext_buff;\n        this-&gt;sub_matrix = src.sub_matrix;\n\n        return TINY_OK;\n    }\n\n    /**\n     * @name Mat::view_roi(int start_row, int start_col, int roi_rows, int roi_cols)\n     * @brief Make a shallow copy of ROI matrix. | Make a view of the ROI matrix. Low level function. Unlike ESP-DSP, it is not allowed to setup stride here, stride is automatically calculated inside the function.\n     *\n     * @param start_row Start row position of source matrix to copy\n     * @param start_col Start column position of source matrix to copy\n     * @param roi_rows Size of row elements of source matrix to copy\n     * @param roi_cols Size of column elements of source matrix to copy\n     *\n     * @todo the pointer address is changing every time access, but the result is correct.\n     *\n     * @return result matrix size row_size x col_size\n     */\n    Mat Mat::view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const\n    {\n        if ((start_row + roi_rows) &gt; this-&gt;row || (start_col + roi_cols) &gt; this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Invalid ROI request.\\n\";\n            return Mat();\n        }\n\n        Mat result;\n        result.row = roi_rows;\n        result.col = roi_cols;\n        result.stride = this-&gt;stride;\n        result.pad = this-&gt;stride - roi_cols;\n        result.element = roi_rows * roi_cols;\n        result.memory = roi_rows * this-&gt;stride;\n        result.data = this-&gt;data + (start_row * this-&gt;stride + start_col);\n        result.temp = nullptr;\n        result.ext_buff = true;\n        result.sub_matrix = true;\n\n        return result;\n    }\n\n    /**\n     * @name Mat::view_roi(const Mat::ROI &amp;roi)\n     * @brief Make a shallow copy of ROI matrix. | Make a view of the ROI matrix. Using ROI structure.\n     *\n     * @param roi Rectangular area of interest\n     *\n     * @return result matrix size row_size x col_size\n     */\n    Mat Mat::view_roi(const Mat::ROI &amp;roi) const\n    {\n        return view_roi(roi.pos_y, roi.pos_x, roi.height, roi.width);\n    }\n\n    /**\n     * @name Mat::copy_roi(int start_row, int start_col, int height, int width)\n     * @brief Make a deep copy of matrix. Copared to view_roi(), this one is a deep copy, not sharing memory with the source matrix.\n     *\n     * @param start_row Start row position of source matrix to copy\n     * @param start_col Start column position of source matrix to copy\n     * @param height Size of row elements of source matrix to copy\n     * @param width Size of column elements of source matrix to copy\n     *\n     * @return result matrix size row_size x col_size\n     */\n    Mat Mat::copy_roi(int start_row, int start_col, int height, int width)\n    {\n        if ((start_row + height) &gt; this-&gt;row)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid row position \" &lt;&lt; std::endl;\n            return Mat();\n        }\n        if ((start_col + width) &gt; this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid columnn position \" &lt;&lt; std::endl;\n            return Mat();\n        }\n\n        // initiate the result matrix\n        Mat result(height, width);\n\n        // deep copy the data from the source matrix\n        for (size_t r = 0; r &lt; result.row; r++)\n        {\n            memcpy(&amp;result.data[r * result.stride], &amp;this-&gt;data[(r + start_row) * this-&gt;stride + start_col], result.col * sizeof(float));\n        }\n\n        // return result;\n        return result;\n    }\n\n    /**\n     * @name Mat::copy_roi(const Mat::ROI &amp;roi)\n     * @brief Make a deep copy of matrix. Using ROI structure. Copared to view_roi(), this one is a deep copy, not sharing memory with the source matrix.\n     *\n     * @param roi Rectangular area of interest\n     *\n     * @return result matrix size row_size x col_size\n     */\n    Mat Mat::copy_roi(const Mat::ROI &amp;roi)\n    {\n        return (copy_roi(roi.pos_y, roi.pos_x, roi.height, roi.width));\n    }\n\n    /**\n     * @name Mat::block(int start_row, int start_col, int block_rows, int block_cols)\n     * @brief Get a block of matrix.\n     *\n     * @param start_row\n     * @param start_col\n     * @param block_rows\n     * @param block_cols\n     * @return Mat\n     */\n    Mat Mat::block(int start_row, int start_col, int block_rows, int block_cols)\n    {\n        // Boundary check\n        if (start_row &lt; 0 || start_col &lt; 0 || block_rows &lt;= 0 || block_cols &lt;= 0)\n        {\n            std::cerr &lt;&lt; \"[Error] Invalid block parameters: negative start position or non-positive block size.\\n\";\n            return Mat();\n        }\n        if ((start_row + block_rows) &gt; this-&gt;row || (start_col + block_cols) &gt; this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Block exceeds matrix boundaries.\\n\";\n            return Mat();\n        }\n\n        Mat result(block_rows, block_cols);\n        for (int i = 0; i &lt; block_rows; ++i)\n        {\n            for (int j = 0; j &lt; block_cols; ++j)\n            {\n                result(i, j) = (*this)(start_row + i, start_col + j);\n            }\n        }\n        return result;\n    }\n\n    /**\n     * @name Mat::swap_rows(int row1, int row2)\n     * @brief Swap two rows of the matrix.\n     *\n     * @param row1 The index of the first row to swap\n     * @param row2 The index of the second row to swap\n     */\n    void Mat::swap_rows(int row1, int row2)\n    {\n        if (row1 &lt; 0 || row1 &gt;= this-&gt;row || row2 &lt; 0 || row2 &gt;= this-&gt;row)\n        {\n            std::cerr &lt;&lt; \"Error: row index out of range\" &lt;&lt; std::endl;\n            return;\n        }\n\n        float *temp_row = new float[this-&gt;col];\n        memcpy(temp_row, &amp;this-&gt;data[row1 * this-&gt;stride], this-&gt;col * sizeof(float));\n        memcpy(&amp;this-&gt;data[row1 * this-&gt;stride], &amp;this-&gt;data[row2 * this-&gt;stride], this-&gt;col * sizeof(float));\n        memcpy(&amp;this-&gt;data[row2 * this-&gt;stride], temp_row, this-&gt;col * sizeof(float));\n        delete[] temp_row;\n    }\n\n    /**\n     * @name Mat::swap_cols(int col1, int col2)\n     * @brief Swap two columns of the matrix.\n     * @note Useful for column pivoting in algorithms like Gaussian elimination with column pivoting.\n     *\n     * @param col1 The index of the first column to swap\n     * @param col2 The index of the second column to swap\n     */\n    void Mat::swap_cols(int col1, int col2)\n    {\n        if (col1 &lt; 0 || col1 &gt;= this-&gt;col || col2 &lt; 0 || col2 &gt;= this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"Error: column index out of range\" &lt;&lt; std::endl;\n            return;\n        }\n\n        // Swap columns element by element (considering stride)\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            float temp = (*this)(i, col1);\n            (*this)(i, col1) = (*this)(i, col2);\n            (*this)(i, col2) = temp;\n        }\n    }\n\n    /**\n     * @name Mat::clear()\n     * @brief Clear the matrix by setting all elements to zero.\n     */\n    void Mat::clear(void)\n    {\n        for (int row = 0; row &lt; this-&gt;row; row++)\n        {\n            memset(this-&gt;data + (row * this-&gt;stride), 0, this-&gt;col * sizeof(float));\n        }\n    }\n\n    // ============================================================================\n    // Arithmetic Operators\n    // ============================================================================\n    /**\n     * @name &amp;Mat::operator=(const Mat &amp;src)\n     * @brief Copy assignment operator - copy the elements of the source matrix into the destination matrix. Compared to the copy constructor, this one is used for existing matrix to copy the elements. The copy constructor is used for the first time to create a new matrix and copy the elements at the same time.\n     *\n     * @param src\n     * @return Mat&amp;\n     */\n    Mat &amp;Mat::operator=(const Mat &amp;src)\n    {\n        // 1. Self-assignment check\n        if (this == &amp;src)\n        {\n            return *this;\n        }\n\n        // 2. Forbid assignment to sub-matrix views\n        if (this-&gt;sub_matrix)\n        {\n            std::cerr &lt;&lt; \"[Error] Assignment to a sub-matrix is not allowed.\\n\";\n            return *this;\n        }\n\n        // 3. If dimensions differ, reallocate memory\n        if (this-&gt;row != src.row || this-&gt;col != src.col)\n        {\n            if (!this-&gt;ext_buff &amp;&amp; this-&gt;data != nullptr)\n            {\n                delete[] this-&gt;data;\n            }\n\n            // Update dimensions and memory info\n            this-&gt;row = src.row;\n            this-&gt;col = src.col;\n            this-&gt;stride = src.col; // Follow source's logical stride\n            this-&gt;pad = 0;\n            this-&gt;element = this-&gt;row * this-&gt;col;\n            this-&gt;memory = this-&gt;row * this-&gt;stride;\n\n            this-&gt;ext_buff = false;\n            this-&gt;sub_matrix = false;\n\n            alloc_mem();\n        }\n\n        // 4. Data copy (row-wise)\n        for (int r = 0; r &lt; this-&gt;row; ++r)\n        {\n            std::memcpy(this-&gt;data + r * this-&gt;stride, src.data + r * src.stride, this-&gt;col * sizeof(float));\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator+=(const Mat &amp;A)\n     * @brief Element-wise addition of another matrix to this matrix.\n     *\n     * @param A The matrix to add\n     * @return Mat&amp; Reference to the current matrix\n     */\n    /**\n     * @name Mat::operator+=(const Mat &amp;A)\n     * @brief Element-wise addition of another matrix to this matrix.\n     *\n     * @param A The matrix to add\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator+=(const Mat &amp;A)\n    {\n        // 1. Dimension check\n        if ((this-&gt;row != A.row) || (this-&gt;col != A.col))\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix addition failed: Dimension mismatch (\"\n                      &lt;&lt; this-&gt;row &lt;&lt; \"x\" &lt;&lt; this-&gt;col &lt;&lt; \" vs \"\n                      &lt;&lt; A.row &lt;&lt; \"x\" &lt;&lt; A.col &lt;&lt; \")\\n\";\n            return *this;\n        }\n\n        // 2. Determine if padding handling is needed\n        bool need_padding_handling = (this-&gt;pad &gt; 0) || (A.pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n            // Padding-aware addition\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_add_f32(this-&gt;data, A.data, this-&gt;data,\n                         this-&gt;row, this-&gt;col,\n                         this-&gt;pad, A.pad, this-&gt;pad,\n                         1, 1, 1);\n#else\n            tiny_mat_add_f32(this-&gt;data, A.data, this-&gt;data,\n                             this-&gt;row, this-&gt;col,\n                             this-&gt;pad, A.pad, this-&gt;pad,\n                             1, 1, 1);\n#endif\n        }\n        else\n        {\n            // Vectorized addition for contiguous memory\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dsps_add_f32(this-&gt;data, A.data, this-&gt;data, this-&gt;memory, 1, 1, 1);\n#else\n            tiny_vec_add_f32(this-&gt;data, A.data, this-&gt;data, this-&gt;memory, 1, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator+=(float C)\n     * @brief Element-wise addition of a constant to this matrix.\n     *\n     * @param C The constant to add\n     */\n    /**\n     * @name Mat::operator+=(float C)\n     * @brief Element-wise addition of a constant to this matrix.\n     *\n     * @param C The constant to add\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator+=(float C)\n    {\n        // check whether padding is presented\n        bool need_padding_handling = (this-&gt;pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n            // Padding-aware constant addition\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_addc_f32(this-&gt;data, this-&gt;data, C,\n                          this-&gt;row, this-&gt;col,\n                          this-&gt;pad, this-&gt;pad,\n                          1, 1);\n#else\n            tiny_mat_addc_f32(this-&gt;data, this-&gt;data, C,\n                              this-&gt;row, this-&gt;col,\n                              this-&gt;pad, this-&gt;pad,\n                              1, 1);\n#endif\n        }\n        else\n        {\n            // Vectorized constant addition\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dsps_addc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, C, 1, 1);\n#else\n            tiny_vec_addc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, C, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator-=(const Mat &amp;A)\n     * @brief Element-wise subtraction of another matrix from this matrix.\n     *\n     * @param A The matrix to subtract\n     * @return Mat&amp; Reference to the current matrix\n     */\n    /**\n     * @name Mat::operator-=(const Mat &amp;A)\n     * @brief Element-wise subtraction of another matrix from this matrix.\n     *\n     * @param A The matrix to subtract\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator-=(const Mat &amp;A)\n    {\n        // 1. Dimension check\n        if ((this-&gt;row != A.row) || (this-&gt;col != A.col))\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix subtraction failed: Dimension mismatch (\"\n                      &lt;&lt; this-&gt;row &lt;&lt; \"x\" &lt;&lt; this-&gt;col &lt;&lt; \" vs \"\n                      &lt;&lt; A.row &lt;&lt; \"x\" &lt;&lt; A.col &lt;&lt; \")\\n\";\n            return *this;\n        }\n\n        // 2. Determine if padding handling is needed\n        bool need_padding_handling = (this-&gt;pad &gt; 0) || (A.pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n            // Padding-aware subtraction\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_sub_f32(this-&gt;data, A.data, this-&gt;data,\n                         this-&gt;row, this-&gt;col,\n                         this-&gt;pad, A.pad, this-&gt;pad,\n                         1, 1, 1);\n#else\n            tiny_mat_sub_f32(this-&gt;data, A.data, this-&gt;data,\n                             this-&gt;row, this-&gt;col,\n                             this-&gt;pad, A.pad, this-&gt;pad,\n                             1, 1, 1);\n#endif\n        }\n        else\n        {\n            // Vectorized subtraction for contiguous memory\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dsps_sub_f32(this-&gt;data, A.data, this-&gt;data, this-&gt;memory, 1, 1, 1);\n#else\n            tiny_vec_sub_f32(this-&gt;data, A.data, this-&gt;data, this-&gt;memory, 1, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator-=(float C)\n     * @brief Element-wise subtraction of a constant from this matrix.\n     *\n     * @param C The constant to subtract\n     */\n    /**\n     * @name Mat::operator-=(float C)\n     * @brief Element-wise subtraction of a constant from this matrix.\n     *\n     * @param C The constant to subtract\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator-=(float C)\n    {\n        bool need_padding_handling = (this-&gt;pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n            // Padding-aware constant subtraction\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            // Note: ESP32 DSP does not provide dspm_subc_f32, using dspm_addc_f32 with -C\n            dspm_addc_f32(this-&gt;data, this-&gt;data, -C,\n                          this-&gt;row, this-&gt;col,\n                          this-&gt;pad, this-&gt;pad,\n                          1, 1);\n#else\n            tiny_mat_subc_f32(this-&gt;data, this-&gt;data, C,\n                              this-&gt;row, this-&gt;col,\n                              this-&gt;pad, this-&gt;pad,\n                              1, 1);\n#endif\n        }\n        else\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            // Note: ESP32 DSP does not provide dsps_subc_f32, using dsps_addc_f32 with -C\n            dsps_addc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, -C, 1, 1);\n#else\n            tiny_vec_subc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, C, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator*=(const Mat &amp;m)\n     * @brief Matrix multiplication: this = this * m\n     *\n     * @param m The matrix to multiply with\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator*=(const Mat &amp;m)\n    {\n        // 1. Dimension check\n        if (this-&gt;col != m.row)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix multiplication failed: incompatible dimensions (\"\n                      &lt;&lt; this-&gt;row &lt;&lt; \"x\" &lt;&lt; this-&gt;col &lt;&lt; \" * \"\n                      &lt;&lt; m.row &lt;&lt; \"x\" &lt;&lt; m.col &lt;&lt; \")\\n\";\n            return *this;\n        }\n\n        // 2. Prepare temp matrix (incase overwriting the original data)\n        Mat temp = this-&gt;copy_roi(0, 0, this-&gt;row, this-&gt;col);\n\n        // 3. check whether padding is present in either matrix\n        bool need_padding_handling = (this-&gt;pad &gt; 0) || (m.pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mult_ex_f32(temp.data, m.data, this-&gt;data, temp.row, temp.col, m.col, temp.pad, m.pad, this-&gt;pad);\n#else\n            tiny_mat_mult_ex_f32(temp.data, m.data, this-&gt;data, temp.row, temp.col, m.col, temp.pad, m.pad, this-&gt;pad);\n#endif\n        }\n        else\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mult_f32(temp.data, m.data, this-&gt;data, temp.row, temp.col, m.col);\n#else\n            tiny_mat_mult_f32(temp.data, m.data, this-&gt;data, temp.row, temp.col, m.col);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator*=(float num)\n     * @brief Element-wise multiplication by a constant\n     *\n     * @param num The constant multiplier\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator*=(float num)\n    {\n        // check whether padding is present\n        bool need_padding_handling = (this-&gt;pad &gt; 0);\n\n        if (need_padding_handling)\n        {\n            // Padding-aware multiplication\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mulc_f32(this-&gt;data, this-&gt;data, num,\n                          this-&gt;row, this-&gt;col,\n                          this-&gt;pad, this-&gt;pad,\n                          1, 1);\n#else\n            tiny_mat_multc_f32(this-&gt;data, this-&gt;data, num, this-&gt;row, this-&gt;col, this-&gt;pad, this-&gt;pad, 1, 1);\n#endif\n        }\n        else\n        {\n            // No padding, use vectorized multiplication\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dsps_mulc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, num, 1, 1);\n#else\n            tiny_vec_mulc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, num, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator/=(const Mat &amp;B)\n     * @brief Element-wise division: this = this / B\n     *\n     * @param B The matrix divisor\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator/=(const Mat &amp;B)\n    {\n        // 1. Dimension check\n        if ((this-&gt;row != B.row) || (this-&gt;col != B.col))\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix division failed: Dimension mismatch (\"\n                      &lt;&lt; this-&gt;row &lt;&lt; \"x\" &lt;&lt; this-&gt;col &lt;&lt; \" vs \"\n                      &lt;&lt; B.row &lt;&lt; \"x\" &lt;&lt; B.col &lt;&lt; \")\\n\";\n            return *this;\n        }\n\n        // 2. Zero division check\n        bool zero_found = false;\n        const float epsilon = 1e-9f;\n        for (int i = 0; i &lt; B.row; ++i)\n        {\n            for (int j = 0; j &lt; B.col; ++j)\n            {\n                if (fabs(B(i, j)) &lt; epsilon)\n                {\n                    zero_found = true;\n                    break;\n                }\n            }\n            if (zero_found)\n                break;\n        }\n\n        if (zero_found)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix division failed: Division by zero detected.\\n\";\n            return *this;\n        }\n\n        // 3. Element-wise division\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                (*this)(i, j) /= B(i, j);\n            }\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator/=(float num)\n     * @brief Element-wise division of this matrix by a constant.\n     *\n     * @param num The constant divisor\n     * @return Mat&amp; Reference to the current matrix\n     */\n    Mat &amp;Mat::operator/=(float num)\n    {\n        // 1. Check division by zero\n        if (num == 0.0f)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix division by zero is undefined.\\n\";\n            return *this;\n        }\n\n        // 2. Determine if padding handling is needed\n        bool need_padding_handling = (this-&gt;pad &gt; 0);\n\n        float inv_num = 1.0f / num;\n\n        if (need_padding_handling)\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mulc_f32(this-&gt;data, this-&gt;data, inv_num,\n                          this-&gt;row, this-&gt;col,\n                          this-&gt;pad, this-&gt;pad,\n                          1, 1);\n#else\n            tiny_mat_multc_f32(this-&gt;data, this-&gt;data, inv_num,\n                              this-&gt;row, this-&gt;col,\n                              this-&gt;pad, this-&gt;pad,\n                              1, 1);\n#endif\n        }\n        else\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dsps_mulc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, inv_num, 1, 1);\n#else\n            tiny_vec_mulc_f32(this-&gt;data, this-&gt;data, this-&gt;memory, inv_num, 1, 1);\n#endif\n        }\n\n        return *this;\n    }\n\n    /**\n     * @name Mat::operator^(int num)\n     * @brief Element-wise integer exponentiation. Returns a new matrix where each element is raised to the given power.\n     *\n     * @param num The exponent (integer)\n     * @return Mat New matrix after exponentiation\n     */\n    Mat Mat::operator^(int num)\n    {\n        // Handle special cases\n        if (num == 0)\n        {\n            // Any number to the power of 0 is 1\n            Mat result(this-&gt;row, this-&gt;col, this-&gt;stride);\n            for (int i = 0; i &lt; this-&gt;row; ++i)\n            {\n                for (int j = 0; j &lt; this-&gt;col; ++j)\n                {\n                    result(i, j) = 1.0f;\n                }\n            }\n            return result;\n        }\n\n        if (num == 1)\n        {\n            // Return a copy of current matrix\n            return Mat(*this);\n        }\n\n        if (num &lt; 0)\n        {\n            std::cerr &lt;&lt; \"[Error] Negative exponent not supported in operator^.\\n\";\n            return Mat(*this); // Return a copy without modification\n        }\n\n        // General case: positive exponent &gt; 1\n        Mat result(this-&gt;row, this-&gt;col, this-&gt;stride);\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                float base = (*this)(i, j);\n                float value = 1.0f;\n                for (int k = 0; k &lt; num; ++k)\n                {\n                    value *= base;\n                }\n                result(i, j) = value;\n            }\n        }\n\n        return result;\n    }\n\n    // ============================================================================\n    // Linear Algebra - Basic Operations\n    // ============================================================================\n    /**\n     * @name Mat::transpose\n     * @brief Transpose the matrix.\n     *\n     * @return Transposed matrix\n     */\n    Mat Mat::transpose()\n    {\n        Mat result(this-&gt;col, this-&gt;row);\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                result(j, i) = this-&gt;data[i * this-&gt;stride + j];\n            }\n        }\n        return result;\n    }\n\n    /**\n     * @name Mat::determinant()\n     * @brief Compute the determinant of the matrix (auto-selects method based on size).\n     * @note For small matrices (n &lt;= 4), uses Laplace expansion.\n     *       For larger matrices, uses LU decomposition (O(n\u00b3)) for better efficiency.\n     *\n     * @return float The determinant value\n     */\n    float Mat::determinant()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Determinant requires a square matrix.\\n\";\n            return 0.0f;\n        }\n\n        int n = this-&gt;row;\n\n        // For small matrices, use Laplace expansion (more accurate for small sizes)\n        if (n &lt;= 4)\n        {\n            return this-&gt;determinant_laplace();\n        }\n\n        // For larger matrices, use LU decomposition (much faster, O(n\u00b3) vs O(n!))\n        return this-&gt;determinant_lu();\n    }\n\n    /**\n     * @name Mat::determinant_laplace()\n     * @brief Compute the determinant using Laplace expansion (cofactor expansion).\n     * @note Time complexity: O(n!) - suitable only for small matrices (n &lt;= 4).\n     *       Uses recursive method with first row expansion.\n     *\n     * @return float The determinant value\n     */\n    float Mat::determinant_laplace()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Determinant requires a square matrix.\\n\";\n            return 0.0f;\n        }\n\n        int n = this-&gt;row;\n        if (n == 1)\n        {\n            return this-&gt;data[0];\n        }\n        if (n == 2)\n        {\n            return this-&gt;data[0] * this-&gt;data[this-&gt;stride + 1] - this-&gt;data[1] * this-&gt;data[this-&gt;stride];\n        }\n\n        float det = 0.0f;\n        for (int j = 0; j &lt; n; ++j)\n        {\n            Mat minor_mat = this-&gt;minor(0, j);\n            float cofactor_val = ((j % 2 == 0) ? 1.0f : -1.0f) * minor_mat.determinant_laplace();\n            det += this-&gt;data[j] * cofactor_val;\n        }\n        return det;\n    }\n\n    /**\n     * @name Mat::determinant_lu()\n     * @brief Compute the determinant using LU decomposition.\n     * @note Time complexity: O(n\u00b3) - efficient for large matrices.\n     *       Formula: det(A) = det(P) * det(L) * det(U) = det(P) * 1 * (product of U diagonal)\n     *       where det(P) = (-1)^(number of row swaps)\n     *\n     * @return float The determinant value\n     */\n    float Mat::determinant_lu()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Determinant requires a square matrix.\\n\";\n            return 0.0f;\n        }\n\n        // Perform LU decomposition\n        LUDecomposition lu = this-&gt;lu_decompose(true);  // Use pivoting for numerical stability\n\n        if (lu.status != TINY_OK)\n        {\n            // Matrix is singular or near-singular\n            return 0.0f;\n        }\n\n        int n = this-&gt;row;\n\n        // Compute det(P): permutation matrix determinant = (-1)^(permutation signature)\n        float det_P = 1.0f;\n        if (lu.pivoted)\n        {\n            // Compute permutation signature by finding cycles in P\n            // P is a permutation matrix, so each row/column has exactly one 1\n            // We can compute the sign by decomposing into transpositions\n            std::vector&lt;bool&gt; visited(n, false);\n            int cycle_count = 0;\n\n            for (int i = 0; i &lt; n; ++i)\n            {\n                if (visited[i]) continue;\n\n                // Find the cycle starting at i\n                int current = i;\n                int cycle_length = 0;\n                while (!visited[current])\n                {\n                    visited[current] = true;\n                    cycle_length++;\n\n                    // Find where P maps current row\n                    for (int j = 0; j &lt; n; ++j)\n                    {\n                        if (fabsf(lu.P(current, j) - 1.0f) &lt; 1e-6f)\n                        {\n                            current = j;\n                            break;\n                        }\n                    }\n                }\n\n                // A cycle of length k contributes (k-1) transpositions\n                if (cycle_length &gt; 1)\n                {\n                    cycle_count += (cycle_length - 1);\n                }\n            }\n\n            // det(P) = (-1)^(number of transpositions)\n            det_P = (cycle_count % 2 == 0) ? 1.0f : -1.0f;\n        }\n\n        // Compute det(L): lower triangular with unit diagonal = 1\n        float det_L = 1.0f;  // L has unit diagonal, so det(L) = 1\n\n        // Compute det(U): product of diagonal elements\n        float det_U = 1.0f;\n        for (int i = 0; i &lt; n; ++i)\n        {\n            det_U *= lu.U(i, i);\n        }\n\n        // det(A) = det(P) * det(L) * det(U)\n        return det_P * det_L * det_U;\n    }\n\n    /**\n     * @name Mat::determinant_gaussian()\n     * @brief Compute the determinant using Gaussian elimination.\n     * @note Time complexity: O(n\u00b3) - efficient for large matrices.\n     *       Converts matrix to upper triangular form, then multiplies diagonal elements.\n     *       Tracks row swaps to account for sign changes.\n     *\n     * @return float The determinant value\n     */\n    float Mat::determinant_gaussian()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Determinant requires a square matrix.\\n\";\n            return 0.0f;\n        }\n\n        int n = this-&gt;row;\n        Mat A = Mat(*this);  // Working copy\n        int swap_count = 0;  // Track number of row swaps\n\n        // Gaussian elimination to upper triangular form\n        for (int k = 0; k &lt; n - 1; ++k)\n        {\n            // Partial pivoting: find row with largest element in column k\n            int max_row = k;\n            float max_val = fabsf(A(k, k));\n            for (int i = k + 1; i &lt; n; ++i)\n            {\n                if (fabsf(A(i, k)) &gt; max_val)\n                {\n                    max_val = fabsf(A(i, k));\n                    max_row = i;\n                }\n            }\n\n            // Swap rows if necessary\n            if (max_row != k)\n            {\n                A.swap_rows(k, max_row);\n                swap_count++;\n            }\n\n            // Check for singular matrix\n            if (fabsf(A(k, k)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                // Matrix is singular\n                return 0.0f;\n            }\n\n            // Eliminate below diagonal\n            for (int i = k + 1; i &lt; n; ++i)\n            {\n                float factor = A(i, k) / A(k, k);\n                for (int j = k; j &lt; n; ++j)\n                {\n                    A(i, j) -= factor * A(k, j);\n                }\n            }\n        }\n\n        // Compute determinant: product of diagonal elements\n        float det = 1.0f;\n        for (int i = 0; i &lt; n; ++i)\n        {\n            det *= A(i, i);\n        }\n\n        // Account for row swaps: each swap multiplies determinant by -1\n        if (swap_count % 2 == 1)\n        {\n            det = -det;\n        }\n\n        return det;\n    }\n\n    /**\n     * @name Mat::adjoint()\n     * @brief Compute the adjoint (adjugate) matrix.\n     *\n     * @return Mat The adjoint matrix\n     */\n    Mat Mat::adjoint()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Adjoint requires a square matrix.\\n\";\n            return Mat();\n        }\n\n        int n = this-&gt;row;\n        Mat result(n, n);\n\n        for (int i = 0; i &lt; n; ++i)\n        {\n            for (int j = 0; j &lt; n; ++j)\n            {\n                Mat cofactor_mat = this-&gt;cofactor(i, j);\n                result(j, i) = cofactor_mat.determinant(); // Note: transpose (j,i) not (i,j)\n            }\n        }\n\n        return result;\n    }\n\n    /**\n     * @name Mat::normalize()\n     * @brief Normalize the matrix by dividing each element by the matrix norm.\n     */\n    void Mat::normalize()\n    {\n        float n = this-&gt;norm();\n        if (n &gt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            for (int i = 0; i &lt; this-&gt;row; ++i)\n            {\n                for (int j = 0; j &lt; this-&gt;col; ++j)\n                {\n                    (*this)(i, j) /= n;\n                }\n            }\n        }\n    }\n\n    /**\n     * @name Mat::norm()\n     * @brief Compute the Frobenius norm of the matrix.\n     *\n     * @return float The matrix norm\n     */\n    float Mat::norm() const\n    {\n        float sum_sq = 0.0f;\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                float val = (*this)(i, j);\n                sum_sq += val * val;\n            }\n        }\n        return sqrtf(sum_sq);\n    }\n\n    /**\n     * @name Mat::inverse_adjoint()\n     * @brief Compute the inverse matrix using the adjoint method.\n     *\n     * @return Mat The inverse matrix\n     */\n    Mat Mat::inverse_adjoint()\n    {\n        float det = this-&gt;determinant();\n        if (fabsf(det) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix is singular, cannot compute inverse.\\n\";\n            return Mat();\n        }\n\n        Mat adj = this-&gt;adjoint();\n        return adj * (1.0f / det);\n    }\n\n    /**\n     * @name Mat::dotprod(const Mat &amp;A, const Mat &amp;B)\n     * @brief Compute the dot product of two matrices (element-wise multiplication and sum).\n     *\n     * @param A First matrix\n     * @param B Second matrix\n     * @return float Dot product value\n     */\n    float Mat::dotprod(const Mat &amp;A, const Mat &amp;B)\n    {\n        if (A.row != B.row || A.col != B.col)\n        {\n            std::cerr &lt;&lt; \"[Error] Dot product requires matrices of the same size.\\n\";\n            return 0.0f;\n        }\n\n        float result = 0.0f;\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            for (int j = 0; j &lt; A.col; ++j)\n            {\n                result += A(i, j) * B(i, j);\n            }\n        }\n        return result;\n    }\n\n    // ============================================================================\n    // Linear Algebra - Matrix Utilities\n    // ============================================================================\n    /**\n     * @name Mat::eye(int size)\n     * @brief Create an identity matrix of specified size.\n     *\n     * @param size Size of the square identity matrix\n     * @return Mat Identity matrix\n     */\n    Mat Mat::eye(int size)\n    {\n        Mat identity(size, size);\n        for (int i = 0; i &lt; size; ++i)\n        {\n            for (int j = 0; j &lt; size; ++j)\n            {\n                identity(i, j) = (i == j) ? 1.0f : 0.0f;\n            }\n        }\n        return identity;\n    }\n\n    /**\n     * @name Mat::ones(int rows, int cols)\n     * @brief Create a matrix filled with ones.\n     *\n     * @param rows Number of rows\n     * @param cols Number of columns\n     * @return Mat Matrix filled with ones\n     */\n    Mat Mat::ones(int rows, int cols)\n    {\n        Mat result(rows, cols);\n        for (int i = 0; i &lt; rows; ++i)\n        {\n            for (int j = 0; j &lt; cols; ++j)\n            {\n                result(i, j) = 1.0f;\n            }\n        }\n        return result;\n    }\n\n    /**\n     * @name Mat::ones(int size)\n     * @brief Create a square matrix filled with ones.\n     *\n     * @param size Size of the square matrix (rows = cols)\n     * @return Mat Square matrix [size x size] with all elements = 1\n     */\n    Mat Mat::ones(int size)\n    {\n        return Mat::ones(size, size);\n    }\n\n    /**\n     * @name Mat::augment(const Mat &amp;A, const Mat &amp;B)\n     * @brief Augment two matrices horizontally [A | B].\n     *\n     * @param A Left matrix\n     * @param B Right matrix\n     * @return Mat Augmented matrix [A B]\n     */\n    Mat Mat::augment(const Mat &amp;A, const Mat &amp;B)\n    {\n        // 1. Check if row counts match\n        if (A.row != B.row)\n        {\n            std::cerr &lt;&lt; \"[Error] Cannot augment matrices: Row counts do not match (\"\n                      &lt;&lt; A.row &lt;&lt; \" vs \" &lt;&lt; B.row &lt;&lt; \")\\n\";\n            return Mat();\n        }\n\n        // 2. Create new matrix with combined columns\n        Mat AB(A.row, A.col + B.col);\n\n        // 3. Copy data from A and B\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            // Copy A\n            for (int j = 0; j &lt; A.col; ++j)\n            {\n                AB(i, j) = A(i, j);\n            }\n            // Copy B\n            for (int j = 0; j &lt; B.col; ++j)\n            {\n                AB(i, A.col + j) = B(i, j);\n            }\n        }\n\n        return AB;\n    }\n\n    /**\n     * @name Mat::vstack(const Mat &amp;A, const Mat &amp;B)\n     * @brief Vertically stack two matrices [A; B].\n     *\n     * @param A Top matrix\n     * @param B Bottom matrix\n     * @return Mat Vertically stacked matrix [A; B]\n     */\n    Mat Mat::vstack(const Mat &amp;A, const Mat &amp;B)\n    {\n        // 1. Check if column counts match\n        if (A.col != B.col)\n        {\n            std::cerr &lt;&lt; \"[Error] Cannot vstack matrices: Column counts do not match (\"\n                      &lt;&lt; A.col &lt;&lt; \" vs \" &lt;&lt; B.col &lt;&lt; \")\\n\";\n            return Mat();\n        }\n\n        // 2. Create new matrix with combined rows\n        Mat AB(A.row + B.row, A.col);\n\n        // 3. Copy data from A and B\n        // Copy A (top rows)\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            for (int j = 0; j &lt; A.col; ++j)\n            {\n                AB(i, j) = A(i, j);\n            }\n        }\n        // Copy B (bottom rows)\n        for (int i = 0; i &lt; B.row; ++i)\n        {\n            for (int j = 0; j &lt; B.col; ++j)\n            {\n                AB(A.row + i, j) = B(i, j);\n            }\n        }\n\n        return AB;\n    }\n\n    /**\n     * @name Mat::gram_schmidt_orthogonalize()\n     * @brief Orthogonalize a set of vectors using the Gram-Schmidt process\n     * @note This is a general-purpose orthogonalization function that can be reused\n     *       for QR decomposition and other applications requiring orthogonal bases\n     * \n     * @param vectors Input matrix where each column is a vector to be orthogonalized\n     * @param orthogonal_vectors Output matrix for orthogonalized vectors (each column is orthogonal)\n     * @param coefficients Output matrix for projection coefficients (upper triangular, like R in QR)\n     * @param tolerance Minimum norm threshold for linear independence check\n     * @return true if successful, false if input is invalid\n     */\n    bool Mat::gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                         Mat &amp;coefficients, float tolerance)\n    {\n        // Validation\n        if (vectors.data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] gram_schmidt_orthogonalize: Input matrix is null.\\n\";\n            return false;\n        }\n\n        int m = vectors.row;  // Dimension of vectors\n        int n = vectors.col;  // Number of vectors\n\n        if (m == 0 || n == 0)\n        {\n            std::cerr &lt;&lt; \"[Error] gram_schmidt_orthogonalize: Invalid dimensions.\\n\";\n            return false;\n        }\n\n        // Initialize output matrices\n        orthogonal_vectors = Mat(m, n);\n        coefficients = Mat(n, n);  // Upper triangular matrix for coefficients\n        coefficients.clear();  // Initialize to zero\n\n        // Modified Gram-Schmidt process (more numerically stable than classical GS)\n        // Also includes re-orthogonalization for better numerical stability\n        for (int j = 0; j &lt; n; ++j)\n        {\n            // Copy j-th column of input vectors to working vector\n            for (int i = 0; i &lt; m; ++i)\n            {\n                orthogonal_vectors(i, j) = vectors(i, j);\n            }\n\n            // Modified Gram-Schmidt: orthogonalize against previous columns\n            // Use a more stable approach: subtract projection immediately\n            for (int k = 0; k &lt; j; ++k)\n            {\n                // Compute dot product: coefficient(k,j) = Q(:,k)^T * Q(:,j)\n                float dot = 0.0f;\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    dot += orthogonal_vectors(i, k) * orthogonal_vectors(i, j);\n                }\n                coefficients(k, j) = dot;  // Store projection coefficient\n\n                // Subtract projection immediately: Q(:,j) = Q(:,j) - dot * Q(:,k)\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    orthogonal_vectors(i, j) -= dot * orthogonal_vectors(i, k);\n                }\n            }\n\n            // Re-orthogonalization: improve numerical stability by doing one more pass\n            // This helps reduce accumulated rounding errors (especially important for\n            // near-linearly-dependent vectors)\n            for (int k = 0; k &lt; j; ++k)\n            {\n                float dot = 0.0f;\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    dot += orthogonal_vectors(i, k) * orthogonal_vectors(i, j);\n                }\n                // Update coefficient with correction (small correction for numerical stability)\n                coefficients(k, j) += dot;\n                // Subtract residual projection to improve orthogonality\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    orthogonal_vectors(i, j) -= dot * orthogonal_vectors(i, k);\n                }\n            }\n\n            // Normalize: compute norm of orthogonalized vector\n            float norm = 0.0f;\n            for (int i = 0; i &lt; m; ++i)\n            {\n                norm += orthogonal_vectors(i, j) * orthogonal_vectors(i, j);\n            }\n            norm = sqrtf(norm);\n\n            // Use stricter tolerance for near-linear-dependent vectors\n            // If norm is very small, generate an orthogonal vector to complete the basis\n            if (norm &lt; tolerance || norm &lt; 1e-5f)  // Stricter check for numerical stability\n            {\n                // Vector is linearly dependent (or near-zero)\n                // Instead of setting to zero, generate an orthogonal vector to maintain Q's orthogonality\n                // Strategy: Start with a standard basis vector and orthogonalize it\n                coefficients(j, j) = 0.0f;  // Original vector has zero norm (linearly dependent)\n\n                // Try to find an orthogonal vector by starting with standard basis vectors\n                // and orthogonalizing them against previous columns\n                bool found_orthogonal = false;\n                for (int basis_idx = 0; basis_idx &lt; m &amp;&amp; !found_orthogonal; ++basis_idx)\n                {\n                    // Start with standard basis vector e_basis_idx\n                    for (int i = 0; i &lt; m; ++i)\n                    {\n                        orthogonal_vectors(i, j) = (i == basis_idx) ? 1.0f : 0.0f;\n                    }\n\n                    // Orthogonalize against previous columns\n                    for (int k = 0; k &lt; j; ++k)\n                    {\n                        float dot = 0.0f;\n                        for (int i = 0; i &lt; m; ++i)\n                        {\n                            dot += orthogonal_vectors(i, k) * orthogonal_vectors(i, j);\n                        }\n                        for (int i = 0; i &lt; m; ++i)\n                        {\n                            orthogonal_vectors(i, j) -= dot * orthogonal_vectors(i, k);\n                        }\n                    }\n\n                    // Check if we got a non-zero vector\n                    float new_norm = 0.0f;\n                    for (int i = 0; i &lt; m; ++i)\n                    {\n                        new_norm += orthogonal_vectors(i, j) * orthogonal_vectors(i, j);\n                    }\n                    new_norm = sqrtf(new_norm);\n\n                    if (new_norm &gt; 1e-5f)\n                    {\n                        // Found a valid orthogonal vector, normalize it\n                        // Note: coefficients(j, j) remains 0 (original vector was linearly dependent)\n                        // but Q(:, j) is now a normalized orthogonal vector\n                        for (int i = 0; i &lt; m; ++i)\n                        {\n                            orthogonal_vectors(i, j) /= new_norm;\n                        }\n                        found_orthogonal = true;\n                    }\n                }\n\n                // If still no orthogonal vector found, set to zero (shouldn't happen for full-rank cases)\n                if (!found_orthogonal)\n                {\n                    coefficients(j, j) = 0.0f;\n                    for (int i = 0; i &lt; m; ++i)\n                    {\n                        orthogonal_vectors(i, j) = 0.0f;\n                    }\n                }\n            }\n            else\n            {\n                coefficients(j, j) = norm;\n                // Normalize the orthogonalized vector\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    orthogonal_vectors(i, j) /= norm;\n                }\n            }\n        }\n\n        return true;\n    }\n\n    // ============================================================================\n    // Linear Algebra - Matrix Operations\n    // ============================================================================\n    /**\n     * @name Mat::minor(int target_row, int target_col)\n     * @brief Calculate the minor matrix by removing specified row and column.\n     * @note Minor is the submatrix obtained by removing one row and one column.\n     *\n     * @param target_row Row index to remove\n     * @param target_col Column index to remove\n     * @return Mat The (n-1)x(n-1) minor matrix\n     */\n    Mat Mat::minor(int target_row, int target_col)\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Minor requires square matrix.\\n\";\n            return Mat();\n        }\n\n        int n = this-&gt;row;\n        Mat result(n - 1, n - 1);\n\n        for (int i = 0, res_i = 0; i &lt; n; ++i)\n        {\n            if (i == target_row)\n                continue;\n\n            for (int j = 0, res_j = 0; j &lt; n; ++j)\n            {\n                if (j == target_col)\n                    continue;\n\n                result.data[res_i * result.stride + res_j] = this-&gt;data[i * this-&gt;stride + j];\n                res_j++;\n            }\n            res_i++;\n        }\n\n        return result;\n    }\n\n    /**\n     * @name Mat::cofactor(int target_row, int target_col)\n     * @brief Calculate the cofactor matrix (same as minor matrix).\n     * @note The cofactor matrix is the same as the minor matrix.\n     *       The sign (-1)^(i+j) is applied when computing the cofactor value,\n     *       not to the matrix elements themselves.\n     *       Cofactor value C_ij = (-1)^(i+j) * det(minor_matrix)\n     *\n     * @param target_row Row index to remove\n     * @param target_col Column index to remove\n     * @return Mat The (n-1)x(n-1) cofactor matrix (same as minor matrix)\n     */\n    Mat Mat::cofactor(int target_row, int target_col)\n    {\n        // Cofactor matrix is the same as minor matrix\n        // The sign is applied when computing cofactor values, not to matrix elements\n        return this-&gt;minor(target_row, target_col);\n    }\n\n    /**\n     * @name Mat::gaussian_eliminate\n     * @brief Perform Gaussian Elimination to convert matrix to Row Echelon Form (REF).\n     *\n     * @return Mat The upper triangular matrix (REF form)\n     */\n    Mat Mat::gaussian_eliminate() const\n    {\n        Mat result(*this); // Create a copy of the original matrix\n        int rows = result.row;\n        int cols = result.col;\n\n        int lead = 0; // Leading column tracker\n\n        for (int r = 0; r &lt; rows; ++r)\n        {\n            if (lead &gt;= cols)\n                break;\n\n            int i = r;\n\n            // Find pivot row (partial pivoting)\n            while (result(i, lead) == 0)\n            {\n                i++;\n                if (i == rows)\n                {\n                    i = r;\n                    lead++;\n                    if (lead == cols)\n                        return result; // Return the result matrix (upper triangular)\n                }\n            }\n\n            // Swap rows if pivot is not in current row\n            if (i != r)\n                result.swap_rows(i, r);\n\n            // Eliminate rows below\n            for (int j = r + 1; j &lt; rows; ++j)\n            {\n                if (result(j, lead) == 0)\n                    continue;\n\n                float factor = result(j, lead) / result(r, lead);\n                for (int k = lead; k &lt; cols; ++k)\n                {\n                    result(j, k) -= factor * result(r, k);\n\n                    // Numerical precision handling (set near-zero values to zero)\n                    if (fabs(result(j, k)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n                        result(j, k) = 0.0f;\n                }\n            }\n\n            lead++;\n        }\n\n        return result; // Return the upper triangular matrix\n    }\n\n    /**\n     * @name Mat::row_reduce_from_gaussian()\n     * @brief Convert a matrix (assumed in row echelon form) to Reduced Row Echelon Form (RREF).\n     *\n     * @return Mat The matrix in RREF form\n     */\n    Mat Mat::row_reduce_from_gaussian()\n    {\n        Mat R(*this); // Make a copy to preserve original matrix\n        int rows = R.row;\n        int cols = R.col;\n\n        int pivot_row = rows - 1;\n        int pivot_col = cols - 2;\n\n        while (pivot_row &gt;= 0)\n        {\n            // Locate pivot in current row\n            int current_pivot_col = -1;\n            for (int k = 0; k &lt; cols; ++k)\n            {\n                if (R(pivot_row, k) != 0)\n                {\n                    current_pivot_col = k;\n                    break;\n                }\n            }\n\n            if (current_pivot_col != -1)\n            {\n                // Normalize pivot row\n                float pivot_val = R(pivot_row, current_pivot_col);\n                for (int s = current_pivot_col; s &lt; cols; ++s)\n                {\n                    R(pivot_row, s) /= pivot_val;\n                    if (fabs(R(pivot_row, s)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n                    {\n                        R(pivot_row, s) = 0.0f;\n                    }\n                }\n\n                // Eliminate above pivot\n                for (int t = pivot_row - 1; t &gt;= 0; --t)\n                {\n                    float factor = R(t, current_pivot_col);\n                    for (int s = current_pivot_col; s &lt; cols; ++s)\n                    {\n                        R(t, s) -= factor * R(pivot_row, s);\n                        if (fabs(R(t, s)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n                        {\n                            R(t, s) = 0.0f;\n                        }\n                    }\n                }\n            }\n\n            pivot_row--;\n        }\n\n        return R;\n    }\n\n    /**\n     * @name Mat::inverse_gje()\n     * @brief Compute the inverse of a square matrix using Gauss-Jordan elimination.\n     *\n     * @return Mat The inverse matrix if invertible, otherwise returns empty matrix.\n     */\n    Mat Mat::inverse_gje()\n    {\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Inversion requires a square matrix.\\n\";\n            return Mat();\n        }\n\n        // Step 1: Create augmented matrix [A | I]\n        Mat I = Mat::eye(this-&gt;row);            // Identity matrix\n        Mat augmented = Mat::augment(*this, I); // Augment matrix A with I\n\n        // Step 2: Apply Gauss-Jordan elimination to get [I | A_inv]\n        Mat rref = augmented.gaussian_eliminate().row_reduce_from_gaussian();\n\n        // Check if the left half is the identity matrix\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                if (fabs(rref(i, j) - I(i, j)) &gt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n                {\n                    std::cerr &lt;&lt; \"[Error] Matrix is singular, cannot compute inverse.\\n\";\n                    return Mat();\n                }\n            }\n        }\n\n        // Step 3: Extract the right half as the inverse matrix\n        Mat result(this-&gt;row, this-&gt;col);\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = 0; j &lt; this-&gt;col; ++j)\n            {\n                result(i, j) = rref(i, j + this-&gt;col); // Extract the right part\n            }\n        }\n\n        return result;\n    }\n\n    /**\n     * @name Mat::solve\n     * @brief Solve the linear system Ax = b using Gaussian elimination.\n     *\n     * @param A Coefficient matrix (NxN)\n     * @param b Result vector (Nx1)\n     * @return Mat Solution vector (Nx1) containing the roots of the equation Ax = b\n     */\n    Mat Mat::solve(const Mat &amp;A, const Mat &amp;b) const\n    {\n        // Check if the matrix A is square\n        if (A.row != A.col)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix A must be square for solving.\\n\";\n            return Mat(); // Return empty matrix\n        }\n\n        // Check if A and b dimensions are compatible for solving\n        if (A.row != b.row || b.col != 1)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix dimensions do not match for solving.\\n\";\n            return Mat(); // Return empty matrix\n        }\n\n        // Create augmented matrix [A | b]\n        Mat augmentedMatrix(A.row, A.col + 1);\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            for (int j = 0; j &lt; A.col; ++j)\n            {\n                augmentedMatrix(i, j) = A(i, j); // Copy matrix A into augmented matrix\n            }\n            augmentedMatrix(i, A.col) = b(i, 0); // Copy vector b into augmented matrix\n        }\n\n        // Perform Gaussian elimination\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            // Find pivot and make sure it's non-zero\n            if (augmentedMatrix(i, i) == 0)\n            {\n                std::cerr &lt;&lt; \"[Error] Pivot is zero, matrix is singular.\\n\";\n                return Mat(); // Return empty matrix\n            }\n\n            // Normalize the pivot row\n            float pivot = augmentedMatrix(i, i);\n            for (int j = i; j &lt; augmentedMatrix.col; ++j)\n            {\n                augmentedMatrix(i, j) /= pivot; // Normalize the pivot row\n            }\n\n            // Eliminate the entries below the pivot\n            for (int j = i + 1; j &lt; A.row; ++j)\n            {\n                float factor = augmentedMatrix(j, i);\n                for (int k = i; k &lt; augmentedMatrix.col; ++k)\n                {\n                    augmentedMatrix(j, k) -= factor * augmentedMatrix(i, k);\n                }\n            }\n        }\n\n        // Back-substitution to find the solution\n        Mat solution(A.row, 1);\n        for (int i = A.row - 1; i &gt;= 0; --i)\n        {\n            float sum = augmentedMatrix(i, A.col);\n            for (int j = i + 1; j &lt; A.row; ++j)\n            {\n                sum -= augmentedMatrix(i, j) * solution(j, 0);\n            }\n            solution(i, 0) = sum;\n        }\n\n        return solution;\n    }\n\n    /**\n     * @name Mat::band_solve\n     * @brief Solve the system of equations Ax = b using optimized Gaussian elimination for banded matrices.\n     *\n     * @param A Coefficient matrix (NxN) - banded matrix\n     * @param b Result vector (Nx1)\n     * @param k Bandwidth of the matrix (the width of the non-zero bands)\n     * @return Mat Solution vector (Nx1) containing the roots of the equation Ax = b\n     */\n    Mat Mat::band_solve(Mat A, Mat b, int k)\n    {\n        // Dimension compatibility check\n        if (A.row != A.col) // Check if A is a square matrix\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix A must be square for solving.\\n\";\n            return Mat(); // Return an empty matrix in case of an error\n        }\n\n        if (A.row != b.row || b.col != 1) // Check if dimensions of A and b are compatible\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix dimensions are not compatible for solving.\\n\";\n            return Mat(); // Return an empty matrix in case of an error\n        }\n\n        int bandsBelow = (k - 1) / 2; // Number of bands below the main diagonal\n\n        // Perform forward elimination to reduce the matrix\n        for (int i = 0; i &lt; A.row; ++i)\n        {\n            if (A(i, i) == 0)\n            {\n                // Pivot 0 - error\n                std::cerr &lt;&lt; \"[Error] Zero pivot detected in bandSolve. Cannot proceed.\\n\";\n                Mat err_result(b.row, 1);\n                memset(err_result.data, 0, b.row * sizeof(float));\n                return err_result;\n            }\n\n            float a_ii = 1 / A(i, i); // Inverse of the pivot element\n\n            // Eliminate elements below the pivot in the current column\n            for (int j = i + 1; j &lt; A.row &amp;&amp; j &lt;= i + bandsBelow; ++j)\n            {\n                if (A(j, i) != 0)\n                {\n                    float factor = A(j, i) * a_ii;\n                    for (int col_idx = i; col_idx &lt; A.col; ++col_idx)\n                    {\n                        A(j, col_idx) -= A(i, col_idx) * factor; // Eliminate the element\n                    }\n                    b(j, 0) -= b(i, 0) * factor; // Update the result vector\n                    A(j, i) = 0;                 // Set the element to zero as it has been eliminated\n                }\n            }\n        }\n\n        // Back substitution to solve for x\n        Mat x(b.row, 1);\n        x(x.row - 1, 0) = b(x.row - 1, 0) / A(x.row - 1, x.row - 1); // Solve the last variable\n\n        for (int i = x.row - 2; i &gt;= 0; --i)\n        {\n            float sum = 0;\n            for (int j = i + 1; j &lt; x.row; ++j)\n            {\n                sum += A(i, j) * x(j, 0); // Sum of the known terms\n            }\n            x(i, 0) = (b(i, 0) - sum) / A(i, i); // Solve for the current variable\n        }\n\n        return x; // Return the solution vector\n    }\n\n    /**\n     * @name Mat::roots(Mat A, Mat y)\n     * @brief   Solve the matrix using a different method. Another implementation of the 'solve' function, no difference in principle.\n     *\n     * This method solves the linear system A * x = y using Gaussian elimination.\n     *\n     * @param[in] A: matrix [N]x[N] with input coefficients\n     * @param[in] y: vector [N]x[1] with result values\n     *\n     * @return\n     *      - matrix [N]x[1] with roots\n     */\n    Mat Mat::roots(Mat A, Mat y)\n    {\n        // Check if A is square\n        if (A.row != A.col)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix A must be square for solving.\\n\";\n            return Mat();\n        }\n\n        int n = A.row; // Number of rows and columns in A (A is square)\n\n        // Create augmented matrix [A | y]\n        Mat augmentedMatrix = Mat::augment(A, y);\n\n        // Perform Gaussian elimination\n        for (int j = 0; j &lt; n; j++)\n        {\n            // Normalize the pivot row (make pivot element equal to 1)\n            float pivot = augmentedMatrix(j, j);\n            if (pivot == 0)\n            {\n                std::cerr &lt;&lt; \"[Error] Pivot is zero, system may have no solution.\" &lt;&lt; std::endl;\n                return Mat(); // Return an empty matrix in case of an error\n            }\n\n            for (int k = 0; k &lt; augmentedMatrix.col; k++)\n            {\n                augmentedMatrix(j, k) /= pivot;\n            }\n\n            // Eliminate the column below the pivot (set other elements in the column to zero)\n            for (int i = j + 1; i &lt; n; i++)\n            {\n                float factor = augmentedMatrix(i, j);\n                for (int k = 0; k &lt; augmentedMatrix.col; k++)\n                {\n                    augmentedMatrix(i, k) -= factor * augmentedMatrix(j, k);\n                }\n            }\n        }\n\n        // Perform back-substitution\n        Mat result(n, 1);\n        for (int i = n - 1; i &gt;= 0; i--)\n        {\n            float sum = augmentedMatrix(i, n); // Right-hand side of the augmented matrix\n            for (int j = i + 1; j &lt; n; j++)\n            {\n                sum -= augmentedMatrix(i, j) * result(j, 0); // Subtract the known terms\n            }\n            result(i, 0) = sum; // Solve for the current variable\n        }\n\n        return result;\n    }\n\n    // ============================================================================\n    // Matrix Decomposition\n    // ============================================================================\n    /**\n     * @name Mat::LUDecomposition::LUDecomposition()\n     * @brief Default constructor for LUDecomposition structure\n     */\n    Mat::LUDecomposition::LUDecomposition()\n    {\n        pivoted = false;\n        status = TINY_OK;\n    }\n\n    /**\n     * @name Mat::CholeskyDecomposition::CholeskyDecomposition()\n     * @brief Default constructor for CholeskyDecomposition structure\n     */\n    Mat::CholeskyDecomposition::CholeskyDecomposition()\n    {\n        status = TINY_OK;\n    }\n\n    /**\n     * @name Mat::QRDecomposition::QRDecomposition()\n     * @brief Default constructor for QRDecomposition structure\n     */\n    Mat::QRDecomposition::QRDecomposition()\n    {\n        status = TINY_OK;\n    }\n\n    /**\n     * @name Mat::SVDDecomposition::SVDDecomposition()\n     * @brief Default constructor for SVDDecomposition structure\n     */\n    Mat::SVDDecomposition::SVDDecomposition()\n    {\n        rank = 0;\n        iterations = 0;\n        status = TINY_OK;\n    }\n\n    /**\n     * @name Mat::is_positive_definite()\n     * @brief Check if matrix is positive definite (for Cholesky decomposition)\n     * @note Uses Sylvester's criterion: all leading principal minors must be positive\n     * \n     * @param tolerance Tolerance for numerical checks\n     * @return true if matrix is positive definite, false otherwise\n     */\n    bool Mat::is_positive_definite(float tolerance) const\n    {\n        // Must be square\n        if (this-&gt;row != this-&gt;col)\n        {\n            return false;\n        }\n\n        // Must be symmetric\n        if (!this-&gt;is_symmetric(tolerance))\n        {\n            return false;\n        }\n\n        int n = this-&gt;row;\n\n        // Check Sylvester's criterion: all leading principal minors must be positive\n        // For efficiency, we check a few leading minors\n        for (int k = 1; k &lt;= n &amp;&amp; k &lt;= 5; ++k)  // Check first 5 minors\n        {\n            Mat submatrix(k, k);\n            for (int i = 0; i &lt; k; ++i)\n            {\n                for (int j = 0; j &lt; k; ++j)\n                {\n                    submatrix(i, j) = (*this)(i, j);\n                }\n            }\n\n            float det = submatrix.determinant();\n            if (det &lt;= tolerance)\n            {\n                return false;\n            }\n        }\n\n        // Additional check: all diagonal elements should be positive\n        for (int i = 0; i &lt; n; ++i)\n        {\n            if ((*this)(i, i) &lt;= tolerance)\n            {\n                return false;\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * @name Mat::lu_decompose()\n     * @brief Compute LU decomposition: A = L * U (with optional pivoting)\n     * @note Efficient for solving multiple systems with same coefficient matrix\n     * \n     * @param use_pivoting Whether to use partial pivoting (default: true)\n     * @return LUDecomposition containing L, U, P matrices and status\n     */\n    Mat::LUDecomposition Mat::lu_decompose(bool use_pivoting) const\n    {\n        LUDecomposition result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] LU decomposition requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int n = this-&gt;row;\n        Mat A = Mat(*this);  // Working copy\n        result.L = Mat::eye(n);  // Initialize L as identity\n        result.U = Mat(n, n);   // Initialize U\n        result.pivoted = use_pivoting;\n\n        if (use_pivoting)\n        {\n            result.P = Mat::eye(n);  // Initialize P as identity\n        }\n\n        // LU decomposition with partial pivoting\n        for (int k = 0; k &lt; n; ++k)\n        {\n            if (use_pivoting)\n            {\n                // Find pivot (largest element in column k, below diagonal)\n                int max_row = k;\n                float max_val = fabsf(A(k, k));\n                for (int i = k + 1; i &lt; n; ++i)\n                {\n                    float abs_val = fabsf(A(i, k));\n                    if (abs_val &gt; max_val)\n                    {\n                        max_val = abs_val;\n                        max_row = i;\n                    }\n                }\n\n                // Swap rows if necessary\n                if (max_row != k)\n                {\n                    A.swap_rows(k, max_row);\n                    result.P.swap_rows(k, max_row);\n                    // Also swap previously computed L rows (but only the multipliers)\n                    for (int j = 0; j &lt; k; ++j)\n                    {\n                        float temp = result.L(k, j);\n                        result.L(k, j) = result.L(max_row, j);\n                        result.L(max_row, j) = temp;\n                    }\n                }\n            }\n\n            // Check for singular matrix\n            if (fabsf(A(k, k)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] LU decomposition: Matrix is singular or near-singular.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            // Compute U (upper triangular part)\n            for (int j = k; j &lt; n; ++j)\n            {\n                result.U(k, j) = A(k, j);\n            }\n\n            // Compute L (lower triangular multipliers)\n            for (int i = k + 1; i &lt; n; ++i)\n            {\n                float multiplier = A(i, k) / A(k, k);\n                result.L(i, k) = multiplier;\n\n                // Update A for next iteration\n                for (int j = k + 1; j &lt; n; ++j)\n                {\n                    A(i, j) -= multiplier * A(k, j);\n                }\n            }\n        }\n\n        result.status = TINY_OK;\n        return result;\n    }\n\n    /**\n     * @name Mat::cholesky_decompose()\n     * @brief Compute Cholesky decomposition: A = L * L^T (for symmetric positive definite matrices)\n     * @note Faster than LU for SPD matrices, used in structural dynamics\n     * \n     * @return CholeskyDecomposition containing L matrix and status\n     */\n    Mat::CholeskyDecomposition Mat::cholesky_decompose() const\n    {\n        CholeskyDecomposition result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Cholesky decomposition requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        // Check if symmetric\n        if (!this-&gt;is_symmetric(1e-6f))\n        {\n            std::cerr &lt;&lt; \"[Error] Cholesky decomposition requires a symmetric matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        int n = this-&gt;row;\n        result.L = Mat(n, n);\n\n        // Cholesky decomposition: A = L * L^T\n        for (int i = 0; i &lt; n; ++i)\n        {\n            for (int j = 0; j &lt;= i; ++j)\n            {\n                float sum = 0.0f;\n\n                if (j == i)\n                {\n                    // Diagonal elements\n                    for (int k = 0; k &lt; j; ++k)\n                    {\n                        sum += result.L(j, k) * result.L(j, k);\n                    }\n                    float diag_val = (*this)(j, j) - sum;\n\n                    if (diag_val &lt;= 0.0f)\n                    {\n                        std::cerr &lt;&lt; \"[Error] Cholesky decomposition: Matrix is not positive definite.\\n\";\n                        result.status = TINY_ERR_MATH_INVALID_PARAM;\n                        return result;\n                    }\n\n                    result.L(j, j) = sqrtf(diag_val);\n                }\n                else\n                {\n                    // Off-diagonal elements\n                    for (int k = 0; k &lt; j; ++k)\n                    {\n                        sum += result.L(i, k) * result.L(j, k);\n                    }\n                    result.L(i, j) = ((*this)(i, j) - sum) / result.L(j, j);\n                }\n            }\n        }\n\n        result.status = TINY_OK;\n        return result;\n    }\n\n    /**\n     * @name Mat::qr_decompose()\n     * @brief Compute QR decomposition: A = Q * R (Q orthogonal, R upper triangular)\n     * @note Numerically stable, used for least squares and orthogonalization\n     * \n     * @return QRDecomposition containing Q and R matrices and status\n     */\n    Mat::QRDecomposition Mat::qr_decompose() const\n    {\n        QRDecomposition result;\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int m = this-&gt;row;\n        int n = this-&gt;col;\n        int min_dim = (m &lt; n) ? m : n;\n\n        // QR decomposition using Gram-Schmidt process\n        // Use the reusable gram_schmidt_orthogonalize function\n        Mat Q_ortho, R_coeff;\n        if (!Mat::gram_schmidt_orthogonalize(*this, Q_ortho, R_coeff, TINY_MATH_MIN_POSITIVE_INPUT_F32))\n        {\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        // Extract Q and R from the orthogonalization results\n        result.Q = Q_ortho;\n        result.R = Mat(m, n);\n\n        // Copy coefficients to R (upper triangular part)\n        for (int j = 0; j &lt; min_dim; ++j)\n        {\n            for (int k = 0; k &lt;= j; ++k)\n            {\n                result.R(k, j) = R_coeff(k, j);\n            }\n\n            // Compute remaining R elements: R(j,k) = Q(:,j)^T * A(:,k) for k &gt; j\n            for (int k = j + 1; k &lt; n; ++k)\n            {\n                float dot = 0.0f;\n                for (int i = 0; i &lt; m; ++i)\n                {\n                    dot += result.Q(i, j) * (*this)(i, k);\n                }\n                result.R(j, k) = dot;\n            }\n        }\n\n        result.status = TINY_OK;\n        return result;\n    }\n\n    /**\n     * @name Mat::svd_decompose()\n     * @brief Compute Singular Value Decomposition: A = U * S * V^T\n     * @note Most general decomposition, used for rank estimation, pseudo-inverse, dimension reduction\n     *       Uses iterative method (bidiagonalization + QR iteration)\n     * \n     * @param max_iter Maximum number of iterations (default: 100)\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @return SVDDecomposition containing U, S, V matrices and status\n     */\n    Mat::SVDDecomposition Mat::svd_decompose(int max_iter, float tolerance) const\n    {\n        SVDDecomposition result;\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int m = this-&gt;row;\n        int n = this-&gt;col;\n        int min_dim = (m &lt; n) ? m : n;\n\n        // For simplicity, we use a simplified SVD algorithm\n        // Full SVD implementation is complex, so we use an iterative approach\n        // based on eigendecomposition of A^T * A and A * A^T\n\n        // Compute A^T * A (n x n matrix)\n        Mat AtA(n, n);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            for (int j = 0; j &lt; n; ++j)\n            {\n                AtA(i, j) = 0.0f;\n                for (int k = 0; k &lt; m; ++k)\n                {\n                    AtA(i, j) += (*this)(k, i) * (*this)(k, j);\n                }\n            }\n        }\n\n        // Eigendecomposition of A^T * A to get V and singular values squared\n        Mat::EigenDecomposition eig_AtA = AtA.eigendecompose_jacobi(tolerance, max_iter);\n\n        if (eig_AtA.status != TINY_OK)\n        {\n            result.status = eig_AtA.status;\n            return result;\n        }\n\n        // Extract singular values (square root of eigenvalues of A^T * A)\n        result.S = Mat(min_dim, 1);\n        result.V = Mat(n, n);\n\n        // Sort eigenvalues in descending order and extract singular values\n        // For simplicity, we'll use the eigenvalues directly\n        int sv_count = 0;\n        for (int i = 0; i &lt; n &amp;&amp; sv_count &lt; min_dim; ++i)\n        {\n            float eigenval = eig_AtA.eigenvalues(i, 0);\n            if (eigenval &gt; tolerance)\n            {\n                result.S(sv_count, 0) = sqrtf(eigenval);\n                // Copy corresponding eigenvector to V\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    result.V(j, sv_count) = eig_AtA.eigenvectors(j, i);\n                }\n                sv_count++;\n            }\n        }\n\n        result.rank = sv_count;\n\n        // Compute U from A * V = U * S\n        result.U = Mat(m, min_dim);\n        for (int i = 0; i &lt; sv_count; ++i)\n        {\n            float sigma = result.S(i, 0);\n            if (sigma &gt; tolerance)\n            {\n                // U(:,i) = (A * V(:,i)) / sigma\n                for (int j = 0; j &lt; m; ++j)\n                {\n                    float sum = 0.0f;\n                    for (int k = 0; k &lt; n; ++k)\n                    {\n                        sum += (*this)(j, k) * result.V(k, i);\n                    }\n                    result.U(j, i) = sum / sigma;\n                }\n            }\n        }\n\n        result.iterations = eig_AtA.iterations;\n        result.status = TINY_OK;\n        return result;\n    }\n\n    /**\n     * @name Mat::solve_lu()\n     * @brief Solve linear system using LU decomposition (more efficient for multiple RHS)\n     * \n     * @param lu LU decomposition of coefficient matrix\n     * @param b Right-hand side vector\n     * @return Solution vector x such that A * x = b\n     */\n    Mat Mat::solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b)\n    {\n        if (lu.status != TINY_OK)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_lu: Invalid LU decomposition.\\n\";\n            return Mat();\n        }\n\n        int n = lu.L.row;\n        if (b.row != n || b.col != 1)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_lu: Dimension mismatch.\\n\";\n            return Mat();\n        }\n\n        // Apply permutation if pivoting was used\n        Mat b_perm = b;\n        if (lu.pivoted)\n        {\n            // b_perm = P * b\n            b_perm = Mat(n, 1);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    if (lu.P(i, j) &gt; 0.5f)  // P is permutation matrix\n                    {\n                        b_perm(i, 0) = b(j, 0);\n                        break;\n                    }\n                }\n            }\n        }\n\n        // Solve L * y = b_perm (forward substitution)\n        Mat y(n, 1);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            float sum = b_perm(i, 0);\n            for (int j = 0; j &lt; i; ++j)\n            {\n                sum -= lu.L(i, j) * y(j, 0);\n            }\n            y(i, 0) = sum;  // L has unit diagonal\n        }\n\n        // Solve U * x = y (backward substitution)\n        Mat x(n, 1);\n        for (int i = n - 1; i &gt;= 0; --i)\n        {\n            float sum = y(i, 0);\n            for (int j = i + 1; j &lt; n; ++j)\n            {\n                sum -= lu.U(i, j) * x(j, 0);\n            }\n            if (fabsf(lu.U(i, i)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] solve_lu: Singular matrix.\\n\";\n                return Mat();\n            }\n            x(i, 0) = sum / lu.U(i, i);\n        }\n\n        return x;\n    }\n\n    /**\n     * @name Mat::solve_cholesky()\n     * @brief Solve linear system using Cholesky decomposition (for SPD matrices)\n     * \n     * @param chol Cholesky decomposition of coefficient matrix\n     * @param b Right-hand side vector\n     * @return Solution vector x such that A * x = b\n     */\n    Mat Mat::solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b)\n    {\n        if (chol.status != TINY_OK)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_cholesky: Invalid Cholesky decomposition.\\n\";\n            return Mat();\n        }\n\n        int n = chol.L.row;\n        if (b.row != n || b.col != 1)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_cholesky: Dimension mismatch.\\n\";\n            return Mat();\n        }\n\n        // Solve L * y = b (forward substitution)\n        Mat y(n, 1);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            float sum = b(i, 0);\n            for (int j = 0; j &lt; i; ++j)\n            {\n                sum -= chol.L(i, j) * y(j, 0);\n            }\n            if (fabsf(chol.L(i, i)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] solve_cholesky: Singular matrix.\\n\";\n                return Mat();\n            }\n            y(i, 0) = sum / chol.L(i, i);\n        }\n\n        // Solve L^T * x = y (backward substitution)\n        Mat x(n, 1);\n        for (int i = n - 1; i &gt;= 0; --i)\n        {\n            float sum = y(i, 0);\n            for (int j = i + 1; j &lt; n; ++j)\n            {\n                sum -= chol.L(j, i) * x(j, 0);  // L^T(j,i) = L(i,j)\n            }\n            if (fabsf(chol.L(i, i)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] solve_cholesky: Singular matrix.\\n\";\n                return Mat();\n            }\n            x(i, 0) = sum / chol.L(i, i);\n        }\n\n        return x;\n    }\n\n    /**\n     * @name Mat::solve_qr()\n     * @brief Solve linear system using QR decomposition (least squares solution)\n     * \n     * @param qr QR decomposition of coefficient matrix\n     * @param b Right-hand side vector\n     * @return Least squares solution vector x such that ||A * x - b|| is minimized\n     */\n    Mat Mat::solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b)\n    {\n        if (qr.status != TINY_OK)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_qr: Invalid QR decomposition.\\n\";\n            return Mat();\n        }\n\n        int m = qr.Q.row;\n        int n = qr.R.col;\n\n        if (b.row != m || b.col != 1)\n        {\n            std::cerr &lt;&lt; \"[Error] solve_qr: Dimension mismatch.\\n\";\n            return Mat();\n        }\n\n        // Compute Q^T * b\n        Mat Qt_b(n, 1);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            Qt_b(i, 0) = 0.0f;\n            for (int j = 0; j &lt; m; ++j)\n            {\n                Qt_b(i, 0) += qr.Q(j, i) * b(j, 0);  // Q^T(i,j) = Q(j,i)\n            }\n        }\n\n        // Solve R * x = Q^T * b (backward substitution)\n        Mat x(n, 1);\n        int min_dim = (m &lt; n) ? m : n;\n        for (int i = min_dim - 1; i &gt;= 0; --i)\n        {\n            if (fabsf(qr.R(i, i)) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                // Skip zero diagonal (underdetermined system)\n                x(i, 0) = 0.0f;\n                continue;\n            }\n\n            float sum = Qt_b(i, 0);\n            for (int j = i + 1; j &lt; n; ++j)\n            {\n                sum -= qr.R(i, j) * x(j, 0);\n            }\n            x(i, 0) = sum / qr.R(i, i);\n        }\n\n        // Set remaining components to zero if n &gt; m\n        for (int i = min_dim; i &lt; n; ++i)\n        {\n            x(i, 0) = 0.0f;\n        }\n\n        return x;\n    }\n\n    /**\n     * @name Mat::pseudo_inverse()\n     * @brief Compute pseudo-inverse using SVD: A^+ = V * S^+ * U^T\n     * \n     * @param svd SVD decomposition of matrix A\n     * @param tolerance Tolerance for singular values (default: 1e-6)\n     * @return Pseudo-inverse matrix A^+\n     */\n    Mat Mat::pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance)\n    {\n        if (svd.status != TINY_OK)\n        {\n            std::cerr &lt;&lt; \"[Error] pseudo_inverse: Invalid SVD decomposition.\\n\";\n            return Mat();\n        }\n\n        int m = svd.U.row;\n        int n = svd.V.row;\n        int rank = svd.rank;\n\n        // Compute S^+ (pseudo-inverse of S)\n        Mat Sp(n, m);\n        for (int i = 0; i &lt; rank; ++i)\n        {\n            float sigma = svd.S(i, 0);\n            if (sigma &gt; tolerance)\n            {\n                Sp(i, i) = 1.0f / sigma;\n            }\n        }\n\n        // Compute A^+ = V * S^+ * U^T\n        // First compute V * S^+\n        Mat VS(n, m);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            for (int j = 0; j &lt; m; ++j)\n            {\n                VS(i, j) = 0.0f;\n                for (int k = 0; k &lt; rank; ++k)\n                {\n                    VS(i, j) += svd.V(i, k) * Sp(k, j);\n                }\n            }\n        }\n\n        // Then compute (V * S^+) * U^T\n        Mat A_plus(n, m);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            for (int j = 0; j &lt; m; ++j)\n            {\n                A_plus(i, j) = 0.0f;\n                for (int k = 0; k &lt; rank; ++k)\n                {\n                    A_plus(i, j) += VS(i, k) * svd.U(j, k);  // U^T(k,j) = U(j,k)\n                }\n            }\n        }\n\n        return A_plus;\n    }\n\n    // ============================================================================\n    // Eigenvalue &amp; Eigenvector Decomposition\n    // ============================================================================\n    /**\n     * @name Mat::EigenPair::EigenPair()\n     * @brief Default constructor for EigenPair structure\n     */\n    Mat::EigenPair::EigenPair() : eigenvalue(0.0f), iterations(0), status(TINY_OK)\n    {\n    }\n\n    /**\n     * @name Mat::EigenDecomposition::EigenDecomposition()\n     * @brief Default constructor for EigenDecomposition structure\n     */\n    Mat::EigenDecomposition::EigenDecomposition() : iterations(0), status(TINY_OK)\n    {\n    }\n\n    /**\n     * @name Mat::is_symmetric()\n     * @brief Check if the matrix is symmetric within a given tolerance.\n     * @note Essential for SHM applications where structural matrices are typically symmetric.\n     *\n     * @param tolerance Maximum allowed difference between A(i,j) and A(j,i)\n     * @return true if matrix is symmetric, false otherwise\n     */\n    bool Mat::is_symmetric(float tolerance) const\n    {\n        // Only square matrices can be symmetric\n        if (this-&gt;row != this-&gt;col)\n        {\n            return false;\n        }\n\n        // Check symmetry: A(i,j) should equal A(j,i) within tolerance\n        for (int i = 0; i &lt; this-&gt;row; ++i)\n        {\n            for (int j = i + 1; j &lt; this-&gt;col; ++j)\n            {\n                float diff = fabsf((*this)(i, j) - (*this)(j, i));\n                if (diff &gt; tolerance)\n                {\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * @name Mat::power_iteration()\n     * @brief Compute the dominant (largest magnitude) eigenvalue and eigenvector using power iteration.\n     * @note Fast method suitable for real-time SHM applications to quickly identify primary frequency.\n     *\n     * @param max_iter Maximum number of iterations (default: 1000)\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @return EigenPair containing the dominant eigenvalue, eigenvector, and status\n     */\n    Mat::EigenPair Mat::power_iteration(int max_iter, float tolerance) const\n    {\n        EigenPair result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Power iteration requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int n = this-&gt;row;\n\n        // Initialize eigenvector with better strategy to avoid convergence to smaller eigenvalues\n        // Strategy: Use sum of columns (or rows) to get a vector with components in all directions\n        result.eigenvector = Mat(n, 1);\n        float norm_sq = 0.0f;\n\n        // Method 1: Use sum of absolute values of columns (more robust)\n        for (int i = 0; i &lt; n; ++i)\n        {\n            float col_sum = 0.0f;\n            for (int j = 0; j &lt; n; ++j)\n            {\n                col_sum += fabsf((*this)(j, i));\n            }\n            result.eigenvector(i, 0) = col_sum + 1.0f; // Add 1 to avoid zero\n            norm_sq += result.eigenvector(i, 0) * result.eigenvector(i, 0);\n        }\n\n        // If all components are too similar, use a different initialization\n        if (norm_sq &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            // Fallback: use values based on index with some variation\n            for (int i = 0; i &lt; n; ++i)\n            {\n                result.eigenvector(i, 0) = 1.0f + 0.1f * static_cast&lt;float&gt;(i);\n                norm_sq += result.eigenvector(i, 0) * result.eigenvector(i, 0);\n            }\n        }\n\n        float inv_norm = 1.0f / sqrtf(norm_sq);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            result.eigenvector(i, 0) *= inv_norm;\n        }\n\n        // Power iteration loop\n        Mat temp_vec(n, 1);\n        float prev_eigenvalue = 0.0f;\n\n        for (int iter = 0; iter &lt; max_iter; ++iter)\n        {\n            // Compute A * v\n            for (int i = 0; i &lt; n; ++i)\n            {\n                temp_vec(i, 0) = 0.0f;\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    temp_vec(i, 0) += (*this)(i, j) * result.eigenvector(j, 0);\n                }\n            }\n\n            // Compute Rayleigh quotient: lambda = v^T * A * v / (v^T * v)\n            float numerator = 0.0f;\n            float denominator = 0.0f;\n            for (int i = 0; i &lt; n; ++i)\n            {\n                numerator += result.eigenvector(i, 0) * temp_vec(i, 0);\n                denominator += result.eigenvector(i, 0) * result.eigenvector(i, 0);\n            }\n\n            if (fabsf(denominator) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] Power iteration: eigenvector norm too small.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            result.eigenvalue = numerator / denominator;\n\n            // Normalize the new vector\n            float new_norm_sq = 0.0f;\n            for (int i = 0; i &lt; n; ++i)\n            {\n                new_norm_sq += temp_vec(i, 0) * temp_vec(i, 0);\n            }\n\n            if (new_norm_sq &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] Power iteration: computed vector norm too small.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            float new_inv_norm = 1.0f / sqrtf(new_norm_sq);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                result.eigenvector(i, 0) = temp_vec(i, 0) * new_inv_norm;\n            }\n\n            // Check convergence\n            if (iter &gt; 0)\n            {\n                float eigenvalue_change = fabsf(result.eigenvalue - prev_eigenvalue);\n                if (eigenvalue_change &lt; tolerance * fabsf(result.eigenvalue))\n                {\n                    result.iterations = iter + 1;\n                    result.status = TINY_OK;\n                    return result;\n                }\n            }\n\n            prev_eigenvalue = result.eigenvalue;\n        }\n\n        // Max iterations reached\n        result.iterations = max_iter;\n        result.status = TINY_ERR_NOT_FINISHED;\n        std::cerr &lt;&lt; \"[Warning] Power iteration did not converge within \" &lt;&lt; max_iter &lt;&lt; \" iterations.\\n\";\n        return result;\n    }\n\n    /**\n     * @name Mat::inverse_power_iteration()\n     * @brief Compute the smallest (minimum magnitude) eigenvalue and eigenvector using inverse power iteration.\n     * @note Critical for system identification - finds fundamental frequency/lowest mode in structural dynamics.\n     *       This method is essential for SHM applications where the smallest eigenvalue corresponds to the\n     *       fundamental frequency of the system.\n     *\n     * @param max_iter Maximum number of iterations (default: 1000)\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @return EigenPair containing the smallest eigenvalue, eigenvector, and status\n     * \n     * @details Algorithm:\n     *  1. Initialize normalized eigenvector v\n     *  2. Iterate: Solve A * y = v (equivalent to y = A^(-1) * v)\n     *  3. Normalize y to get new v\n     *  4. Compute eigenvalue estimate: lambda_min = 1 / (v^T * y)\n     *  5. Check convergence\n     * \n     * @note The matrix must be invertible (non-singular) for this method to work.\n     *       If the matrix is singular or near-singular, the method will fail gracefully.\n     */\n    Mat::EigenPair Mat::inverse_power_iteration(int max_iter, float tolerance) const\n    {\n        EigenPair result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Inverse power iteration requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int n = this-&gt;row;\n\n        // Check if matrix is singular by computing determinant (quick check)\n        // For efficiency, we'll check during the first solve operation instead\n\n        // Initialize eigenvector for inverse power iteration\n        // Strategy: Use a vector that is orthogonal to the dominant eigenvector direction\n        // For inverse power iteration, we want to converge to the smallest eigenvalue\n        // Use a simple initialization: [1, 1, ..., 1]^T normalized, which typically\n        // has components in all eigenvector directions\n        result.eigenvector = Mat(n, 1);\n        float norm_sq = 0.0f;\n\n        // Initialize with alternating signs to avoid alignment with dominant eigenvector\n        // This helps ensure we converge to the smallest eigenvalue\n        for (int i = 0; i &lt; n; ++i)\n        {\n            // Use alternating pattern: 1, -1, 1, -1, ... with small variations\n            result.eigenvector(i, 0) = (i % 2 == 0) ? 1.0f : -1.0f;\n            result.eigenvector(i, 0) += 0.1f * static_cast&lt;float&gt;(i) / static_cast&lt;float&gt;(n);\n            norm_sq += result.eigenvector(i, 0) * result.eigenvector(i, 0);\n        }\n\n        // Normalize\n        if (norm_sq &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            // Fallback: use uniform vector\n            for (int i = 0; i &lt; n; ++i)\n            {\n                result.eigenvector(i, 0) = 1.0f;\n            }\n            norm_sq = static_cast&lt;float&gt;(n);\n        }\n\n        float inv_norm = 1.0f / sqrtf(norm_sq);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            result.eigenvector(i, 0) *= inv_norm;\n        }\n\n        // Inverse power iteration loop\n        Mat temp_vec(n, 1);\n        float prev_eigenvalue = 0.0f;\n\n        for (int iter = 0; iter &lt; max_iter; ++iter)\n        {\n            // Solve A * y = v (equivalent to computing A^(-1) * v)\n            // This is the key difference from power iteration\n            temp_vec = solve(*this, result.eigenvector);\n\n            // Check if solve failed (matrix is singular or near-singular)\n            if (temp_vec.row == 0 || temp_vec.data == nullptr)\n            {\n                std::cerr &lt;&lt; \"[Error] Inverse power iteration: Matrix is singular or near-singular. \"\n                          &lt;&lt; \"Cannot solve linear system A * y = v.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            // Check if solution vector is valid (not all zeros or NaN)\n            bool valid_solution = false;\n            for (int i = 0; i &lt; n; ++i)\n            {\n                if (std::isnan(temp_vec(i, 0)) || std::isinf(temp_vec(i, 0)))\n                {\n                    std::cerr &lt;&lt; \"[Error] Inverse power iteration: Solution contains NaN or Inf.\\n\";\n                    result.status = TINY_ERR_MATH_INVALID_PARAM;\n                    return result;\n                }\n                if (fabsf(temp_vec(i, 0)) &gt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n                {\n                    valid_solution = true;\n                }\n            }\n\n            if (!valid_solution)\n            {\n                std::cerr &lt;&lt; \"[Error] Inverse power iteration: Solution vector is zero or too small.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            // Compute Rayleigh quotient for A directly: lambda = (v^T * A * v) / (v^T * v)\n            // This gives us the eigenvalue of A corresponding to eigenvector v\n            // Note: In inverse power iteration, we iterate on A^(-1), but we want the eigenvalue of A\n            // Since y = A^(-1) * v, we have A * y = v, so we can compute v^T * A * v = v^T * A * y\n            // But more directly, we compute A * v to get the eigenvalue\n\n            // Compute A * v\n            Mat Av(n, 1);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                Av(i, 0) = 0.0f;\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    Av(i, 0) += (*this)(i, j) * result.eigenvector(j, 0);\n                }\n            }\n\n            // Compute Rayleigh quotient: lambda = (v^T * A * v) / (v^T * v)\n            float numerator = 0.0f;\n            float denominator = 0.0f;\n            for (int i = 0; i &lt; n; ++i)\n            {\n                numerator += result.eigenvector(i, 0) * Av(i, 0);\n                denominator += result.eigenvector(i, 0) * result.eigenvector(i, 0);\n            }\n\n            if (fabsf(denominator) &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] Inverse power iteration: eigenvector norm too small.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            // Compute eigenvalue of A using Rayleigh quotient\n            result.eigenvalue = numerator / denominator;\n\n            // Normalize the new vector\n            float new_norm_sq = 0.0f;\n            for (int i = 0; i &lt; n; ++i)\n            {\n                new_norm_sq += temp_vec(i, 0) * temp_vec(i, 0);\n            }\n\n            if (new_norm_sq &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n            {\n                std::cerr &lt;&lt; \"[Error] Inverse power iteration: computed vector norm too small.\\n\";\n                result.status = TINY_ERR_MATH_INVALID_PARAM;\n                return result;\n            }\n\n            float new_inv_norm = 1.0f / sqrtf(new_norm_sq);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                result.eigenvector(i, 0) = temp_vec(i, 0) * new_inv_norm;\n            }\n\n            // Check convergence\n            if (iter &gt; 0)\n            {\n                float eigenvalue_change = fabsf(result.eigenvalue - prev_eigenvalue);\n                // Use relative tolerance for convergence check\n                float rel_tolerance = tolerance * fmaxf(fabsf(result.eigenvalue), 1.0f);\n                if (eigenvalue_change &lt; rel_tolerance)\n                {\n                    result.iterations = iter + 1;\n                    result.status = TINY_OK;\n                    return result;\n                }\n            }\n\n            prev_eigenvalue = result.eigenvalue;\n        }\n\n        // Max iterations reached\n        result.iterations = max_iter;\n        result.status = TINY_ERR_NOT_FINISHED;\n        std::cerr &lt;&lt; \"[Warning] Inverse power iteration did not converge within \" &lt;&lt; max_iter &lt;&lt; \" iterations.\\n\";\n        return result;\n    }\n\n    /**\n     * @name Mat::eigendecompose_jacobi()\n     * @brief Compute complete eigenvalue decomposition using Jacobi method for symmetric matrices.\n     * @note Robust and accurate method ideal for structural dynamics matrices in SHM.\n     *\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @param max_iter Maximum number of iterations (default: 100)\n     * @return EigenDecomposition containing all eigenvalues, eigenvectors, and status\n     */\n    Mat::EigenDecomposition Mat::eigendecompose_jacobi(float tolerance, int max_iter) const\n    {\n        EigenDecomposition result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Eigendecomposition requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        // Check if matrix is symmetric\n        if (!this-&gt;is_symmetric(tolerance * 10.0f))\n        {\n            std::cerr &lt;&lt; \"[Warning] Matrix is not symmetric. Jacobi method may not converge correctly.\\n\";\n        }\n\n        int n = this-&gt;row;\n\n        // Initialize: working copy of matrix, eigenvectors as identity\n        Mat A = Mat(*this); // Working copy (will become diagonal)\n        result.eigenvectors = Mat::eye(n);\n\n        // Jacobi iteration\n        for (int iter = 0; iter &lt; max_iter; ++iter)\n        {\n            // Find largest off-diagonal element\n            float max_off_diag = 0.0f;\n            int p = 0, q = 0;\n\n            for (int i = 0; i &lt; n; ++i)\n            {\n                for (int j = i + 1; j &lt; n; ++j)\n                {\n                    float abs_val = fabsf(A(i, j));\n                    if (abs_val &gt; max_off_diag)\n                    {\n                        max_off_diag = abs_val;\n                        p = i;\n                        q = j;\n                    }\n                }\n            }\n\n            // Check convergence\n            if (max_off_diag &lt; tolerance)\n            {\n                // Extract eigenvalues from diagonal\n                result.eigenvalues = Mat(n, 1);\n                for (int i = 0; i &lt; n; ++i)\n                {\n                    result.eigenvalues(i, 0) = A(i, i);\n                }\n                result.iterations = iter + 1;\n                result.status = TINY_OK;\n                return result;\n            }\n\n            // Compute rotation angle\n            float app = A(p, p);\n            float aqq = A(q, q);\n            float apq = A(p, q);\n\n            float tau = (aqq - app) / (2.0f * apq);\n            float t;\n            if (tau &gt;= 0.0f)\n            {\n                t = 1.0f / (tau + sqrtf(1.0f + tau * tau));\n            }\n            else\n            {\n                t = -1.0f / (-tau + sqrtf(1.0f + tau * tau));\n            }\n\n            float c = 1.0f / sqrtf(1.0f + t * t); // cosine\n            float s = t * c;                       // sine\n\n            // Apply Jacobi rotation to A\n            // Update rows p and q\n            for (int j = 0; j &lt; n; ++j)\n            {\n                if (j != p &amp;&amp; j != q)\n                {\n                    float apj = A(p, j);\n                    float aqj = A(q, j);\n                    A(p, j) = c * apj - s * aqj;\n                    A(q, j) = s * apj + c * aqj;\n                    A(j, p) = A(p, j); // Maintain symmetry\n                    A(j, q) = A(q, j);\n                }\n            }\n\n            // Update diagonal elements\n            float app_new = c * c * app - 2.0f * c * s * apq + s * s * aqq;\n            float aqq_new = s * s * app + 2.0f * c * s * apq + c * c * aqq;\n            A(p, p) = app_new;\n            A(q, q) = aqq_new;\n            A(p, q) = 0.0f;\n            A(q, p) = 0.0f;\n\n            // Update eigenvectors\n            for (int i = 0; i &lt; n; ++i)\n            {\n                float vip = result.eigenvectors(i, p);\n                float viq = result.eigenvectors(i, q);\n                result.eigenvectors(i, p) = c * vip - s * viq;\n                result.eigenvectors(i, q) = s * vip + c * viq;\n            }\n        }\n\n        // Extract eigenvalues from diagonal\n        result.eigenvalues = Mat(n, 1);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            result.eigenvalues(i, 0) = A(i, i);\n        }\n\n        result.iterations = max_iter;\n        result.status = TINY_ERR_NOT_FINISHED;\n        std::cerr &lt;&lt; \"[Warning] Jacobi method did not converge within \" &lt;&lt; max_iter &lt;&lt; \" iterations.\\n\";\n        return result;\n    }\n\n    /**\n     * @name Mat::eigendecompose_qr()\n     * @brief Compute complete eigenvalue decomposition using QR algorithm for general matrices.\n     * @note Supports non-symmetric matrices, but may have complex eigenvalues (only real part returned).\n     *\n     * @param max_iter Maximum number of QR iterations (default: 100)\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @return EigenDecomposition containing eigenvalues, eigenvectors, and status\n     */\n    Mat::EigenDecomposition Mat::eigendecompose_qr(int max_iter, float tolerance) const\n    {\n        EigenDecomposition result;\n\n        // Validation: must be square matrix\n        if (this-&gt;row != this-&gt;col)\n        {\n            std::cerr &lt;&lt; \"[Error] Eigendecomposition requires a square matrix.\\n\";\n            result.status = TINY_ERR_INVALID_ARG;\n            return result;\n        }\n\n        if (this-&gt;data == nullptr)\n        {\n            std::cerr &lt;&lt; \"[Error] Matrix data pointer is null.\\n\";\n            result.status = TINY_ERR_MATH_NULL_POINTER;\n            return result;\n        }\n\n        int n = this-&gt;row;\n\n        // Initialize: start with original matrix, eigenvectors as identity\n        Mat A = Mat(*this); // Working copy (will become upper triangular)\n        result.eigenvectors = Mat::eye(n);\n\n        // QR iteration with improved convergence checking\n        for (int iter = 0; iter &lt; max_iter; ++iter)\n        {\n            // Check convergence: check if matrix is upper triangular\n            // Use a more lenient tolerance for sub-diagonal elements\n            bool converged = true;\n            float max_off_diag = 0.0f;\n            for (int i = 1; i &lt; n; ++i)\n            {\n                for (int j = 0; j &lt; i; ++j)\n                {\n                    float abs_val = fabsf(A(i, j));\n                    if (abs_val &gt; max_off_diag)\n                        max_off_diag = abs_val;\n                    // Use relative tolerance: compare with diagonal elements\n                    float diag_scale = fmaxf(fabsf(A(i, i)), fabsf(A(j, j)));\n                    float rel_tolerance = tolerance * fmaxf(1.0f, diag_scale);\n                    if (abs_val &gt; rel_tolerance)\n                    {\n                        converged = false;\n                    }\n                }\n            }\n\n            if (converged)\n            {\n                // Extract eigenvalues from diagonal\n                result.eigenvalues = Mat(n, 1);\n                for (int i = 0; i &lt; n; ++i)\n                {\n                    result.eigenvalues(i, 0) = A(i, i);\n                }\n                result.iterations = iter + 1;\n                result.status = TINY_OK;\n                return result;\n            }\n\n            // Optional: Use shift to accelerate convergence (Wilkinson shift for last 2x2 block)\n            // For simplicity, we skip shift for now but can add it later if needed\n\n            // QR decomposition using Gram-Schmidt process\n            // Use the reusable gram_schmidt_orthogonalize function\n            Mat Q_ortho, R_coeff;\n            if (!Mat::gram_schmidt_orthogonalize(A, Q_ortho, R_coeff, TINY_MATH_MIN_POSITIVE_INPUT_F32))\n            {\n                result.status = TINY_ERR_MATH_NULL_POINTER;\n                return result;\n            }\n\n            Mat Q = Q_ortho;\n            Mat R(n, n);\n\n            // Copy coefficients to R (upper triangular part)\n            for (int j = 0; j &lt; n; ++j)\n            {\n                for (int k = 0; k &lt;= j; ++k)\n                {\n                    R(k, j) = R_coeff(k, j);\n                }\n\n                // Compute remaining R elements: R(j,k) = Q(:,j)^T * A(:,k) for k &gt; j\n                for (int k = j + 1; k &lt; n; ++k)\n                {\n                    float dot = 0.0f;\n                    for (int i = 0; i &lt; n; ++i)\n                    {\n                        dot += Q(i, j) * A(i, k);\n                    }\n                    R(j, k) = dot;\n                }\n            }\n\n            // Update A = R * Q\n            Mat A_new(n, n);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    A_new(i, j) = 0.0f;\n                    for (int k = 0; k &lt; n; ++k)\n                    {\n                        A_new(i, j) += R(i, k) * Q(k, j);\n                    }\n                }\n            }\n            A = A_new;\n\n            // Update eigenvectors: V = V * Q\n            Mat V_new(n, n);\n            for (int i = 0; i &lt; n; ++i)\n            {\n                for (int j = 0; j &lt; n; ++j)\n                {\n                    V_new(i, j) = 0.0f;\n                    for (int k = 0; k &lt; n; ++k)\n                    {\n                        V_new(i, j) += result.eigenvectors(i, k) * Q(k, j);\n                    }\n                }\n            }\n            result.eigenvectors = V_new;\n        }\n\n        // Extract eigenvalues from diagonal\n        result.eigenvalues = Mat(n, 1);\n        for (int i = 0; i &lt; n; ++i)\n        {\n            result.eigenvalues(i, 0) = A(i, i);\n        }\n\n        result.iterations = max_iter;\n        result.status = TINY_ERR_NOT_FINISHED;\n        std::cerr &lt;&lt; \"[Warning] QR algorithm did not converge within \" &lt;&lt; max_iter &lt;&lt; \" iterations.\\n\";\n        return result;\n    }\n\n    /**\n     * @name Mat::eigendecompose()\n     * @brief Automatic eigenvalue decomposition with method selection.\n     * @note Convenient interface for edge computing: uses Jacobi for symmetric matrices, QR for general.\n     *\n     * @param tolerance Convergence tolerance (default: 1e-6)\n     * @return EigenDecomposition containing eigenvalues, eigenvectors, and status\n     */\n    Mat::EigenDecomposition Mat::eigendecompose(float tolerance) const\n    {\n        // Check if matrix is symmetric\n        if (this-&gt;is_symmetric(tolerance * 10.0f))\n        {\n            // Use Jacobi method for symmetric matrices (more efficient and stable)\n            return this-&gt;eigendecompose_jacobi(tolerance, 100);\n        }\n        else\n        {\n            // Use QR algorithm for general matrices\n            return this-&gt;eigendecompose_qr(100, tolerance);\n        }\n    }\n\n    // ============================================================================\n    // Stream Operators\n    // ============================================================================\n    /**\n     * @name operator&lt;&lt;\n     * @brief Stream insertion operator for printing matrix to the output stream (e.g., std::cout).\n     *\n     * This function allows printing the contents of a matrix to an output stream.\n     * It prints each row of the matrix on a new line, with elements separated by spaces.\n     *\n     * @param os Output stream where the matrix will be printed (e.g., std::cout)\n     * @param m Matrix to be printed\n     *\n     * @return os The output stream after printing the matrix\n     */\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m)\n    {\n        if (m.data == nullptr)\n        {\n            os &lt;&lt; \"[Error] Cannot print matrix: data pointer is null.\\n\";\n            return os;\n        }\n\n        for (int i = 0; i &lt; m.row; ++i)\n        {\n            os &lt;&lt; m(i, 0);\n            for (int j = 1; j &lt; m.col; ++j)\n            {\n                os &lt;&lt; \" \" &lt;&lt; m(i, j);\n            }\n            os &lt;&lt; std::endl;\n        }\n        return os;\n    }\n\n    /**\n     * @name operator&lt;&lt;\n     * @brief Stream insertion operator for printing the Rectangular ROI structure to the output stream.\n     *\n     * This function prints the details of the ROI (Region of Interest) including the start row and column,\n     * and the width and height of the rectangular region.\n     *\n     * @param os Output stream where the ROI will be printed (e.g., std::cout)\n     * @param roi The ROI structure to be printed\n     *\n     * @return os The output stream after printing the ROI details\n     */\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi)\n    {\n        os &lt;&lt; \"row start \" &lt;&lt; roi.pos_y &lt;&lt; std::endl;\n        os &lt;&lt; \"col start \" &lt;&lt; roi.pos_x &lt;&lt; std::endl;\n        os &lt;&lt; \"row count \" &lt;&lt; roi.height &lt;&lt; std::endl;\n        os &lt;&lt; \"col count \" &lt;&lt; roi.width &lt;&lt; std::endl;\n\n        return os;\n    }\n\n    /**\n     * @name operator&gt;&gt;\n     * @brief Stream extraction operator for reading matrix from the input stream (e.g., std::cin).\n     *\n     * This function reads the contents of a matrix from an input stream.\n     * The matrix elements are read row by row, with elements separated by spaces or newlines.\n     *\n     * @param is Input stream from which the matrix will be read (e.g., std::cin)\n     * @param m Matrix to store the read data\n     *\n     * @return is The input stream after reading the matrix\n     */\n    std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m)\n    {\n        for (int i = 0; i &lt; m.row; ++i)\n        {\n            for (int j = 0; j &lt; m.col; ++j)\n            {\n                is &gt;&gt; m(i, j);\n            }\n        }\n        return is;\n    }\n\n    // ============================================================================\n    // Global Arithmetic Operators\n    // ============================================================================\n    /**\n     * + operator, sum of two matrices\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] B: Input matrix B\n     *\n     * @return\n     *     - result matrix A+B\n     */\n    Mat operator+(const Mat &amp;m1, const Mat &amp;m2)\n    {\n        if ((m1.row != m2.row) || (m1.col != m2.col))\n        {\n            std::cerr &lt;&lt; \"operator + Error: matrices do not have equal dimensions\" &lt;&lt; std::endl;\n            Mat err_ret;\n            return err_ret;\n        }\n\n        if (m1.sub_matrix || m2.sub_matrix)\n        {\n            Mat temp(m1.row, m1.col);\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_add_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m1.pad, m2.pad, temp.pad, 1, 1, 1);\n#else\n            tiny_mat_add_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m1.pad, m2.pad, temp.pad, 1, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m1);\n            return (temp += m2);\n        }\n    }\n\n    /**\n     * + operator, sum of matrix with constant\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] C: Input constant\n     *\n     * @return\n     *     - result matrix A+C\n     */\n    Mat operator+(const Mat &amp;m, float C)\n    {\n        if (m.sub_matrix)\n        {\n            Mat temp(m.row, m.col);\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_addc_f32(m.data, temp.data, C, m.row, m.col, m.pad, temp.pad, 1, 1);\n#else\n            tiny_mat_addc_f32(m.data, temp.data, C, m.row, m.col, m.pad, temp.pad, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m);\n            return (temp += C);\n        }\n    }\n\n    /**\n     * - operator, subtraction of two matrices\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] B: Input matrix B\n     *\n     * @return\n     *     - result matrix A-B\n     */\n    Mat operator-(const Mat &amp;m1, const Mat &amp;m2)\n    {\n        if ((m1.row != m2.row) || (m1.col != m2.col))\n        {\n            std::cerr &lt;&lt; \"operator - Error: matrices do not have equal dimensions\" &lt;&lt; std::endl;\n            Mat err_ret;\n            return err_ret;\n        }\n\n        if (m1.sub_matrix || m2.sub_matrix)\n        {\n            Mat temp(m1.row, m1.col);\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_sub_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m1.pad, m2.pad, temp.pad, 1, 1, 1);\n#else\n            tiny_mat_sub_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m1.pad, m2.pad, temp.pad, 1, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m1);\n            return (temp -= m2);\n        }\n    }\n\n\n    /**\n     * - operator, subtraction of matrix with constant\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] C: Input constant\n     *\n     * @return\n     *     - result matrix A-C\n     */\n    Mat operator-(const Mat &amp;m, float C)\n    {\n        if (m.sub_matrix)\n        {\n            Mat temp(m.row, m.col);\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_addc_f32(m.data, temp.data, -C, m.row, m.col, m.pad, temp.pad, 1, 1);\n#else\n            tiny_mat_addc_f32(m.data, temp.data, -C, m.row, m.col, m.pad, temp.pad, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m);\n            return (temp -= C);\n        }\n    }\n\n\n    /**\n     * * operator, multiplication of two matrices.\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] B: Input matrix B\n     *\n     * @return\n     *     - result matrix A*B\n     */\n    Mat operator*(const Mat &amp;m1, const Mat &amp;m2)\n    {\n        if (m1.col != m2.row)\n        {\n            std::cerr &lt;&lt; \"operator * Error: matrices do not have correct dimensions\" &lt;&lt; std::endl;\n            Mat err_ret;\n            return err_ret;\n        }\n        Mat temp(m1.row, m2.col);\n\n        if (m1.sub_matrix || m2.sub_matrix)\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mult_ex_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m2.col, m1.pad, m2.pad, temp.pad);\n#else\n            tiny_mat_mult_ex_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m2.col, m1.pad, m2.pad, temp.pad);\n#endif\n        }\n        else\n        {\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mult_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m2.col);\n#else\n            tiny_mat_mult_f32(m1.data, m2.data, temp.data, m1.row, m1.col, m2.col);\n#endif\n        }\n\n        return temp;\n    }\n\n    /**\n     * * operator, multiplication of matrix with constant\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] C: floating point value\n     *\n     * @return\n     *     - result matrix A*B\n     */\n    Mat operator*(const Mat &amp;m, float num)\n    {\n        if (m.sub_matrix)\n        {\n            Mat temp(m.row, m.col);\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mulc_f32(m.data, temp.data, num, m.row, m.col, m.pad, temp.pad, 1, 1);\n#else\n            tiny_mat_multc_f32(m.data, temp.data, num, m.row, m.col, m.pad, temp.pad, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m);\n            return (temp *= num);\n        }\n    }\n\n    /**\n     * * operator, multiplication of matrix with constant\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] C: floating point value\n     * @param[in] A: Input matrix A\n     *\n     * @return\n     *     - result matrix C*A\n     */\n    Mat operator*(float num, const Mat &amp;m)\n    {\n        return (m * num);\n    }\n\n    /**\n     * / operator, divide of matrix by constant\n     * The operator use DSP optimized implementation of multiplication.\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] C: floating point value\n     *\n     * @return\n     *     - result matrix A/C\n     */\n    Mat operator/(const Mat &amp;m, float num)\n    {\n        // Check division by zero\n        if (num == 0.0f)\n        {\n            std::cerr &lt;&lt; \"[Error] Division by zero in operator/.\\n\";\n            Mat err_ret;\n            return err_ret;\n        }\n\n        if (m.sub_matrix)\n        {\n            Mat temp(m.row, m.col);\n            float inv_num = 1.0f / num;\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n            dspm_mulc_f32(m.data, temp.data, inv_num, m.row, m.col, m.pad, temp.pad, 1, 1);\n#else\n            tiny_mat_multc_f32(m.data, temp.data, inv_num, m.row, m.col, m.pad, temp.pad, 1, 1);\n#endif\n            return temp;\n        }\n        else\n        {\n            Mat temp(m);\n            return (temp /= num);\n        }\n    }\n\n\n    /**\n     * / operator, divide matrix A by matrix B (element-wise)\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] B: Input matrix B\n     *\n     * @return\n     *     - result matrix C, where C[i,j] = A[i,j]/B[i,j]\n     */\n    Mat operator/(const Mat &amp;A, const Mat &amp;B)\n    {\n        if ((A.row != B.row) || (A.col != B.col))\n        {\n            std::cerr &lt;&lt; \"operator / Error: matrices do not have equal dimensions\" &lt;&lt; std::endl;\n            Mat err_ret;\n            return err_ret;\n        }\n\n        Mat temp(A.row, A.col);\n        for (int row = 0; row &lt; A.row; row++)\n        {\n            for (int col = 0; col &lt; A.col; col++)\n            {\n                temp(row, col) = A(row, col) / B(row, col);\n            }\n        }\n        return temp;\n    }\n\n\n    /**\n     * == operator, compare two matrices\n     *\n     * @param[in] A: Input matrix A\n     * @param[in] B: Input matrix B\n     *\n     * @return\n     *      - true if matrices are the same\n     *      - false if matrices are different\n     */\n    bool operator==(const Mat &amp;m1, const Mat &amp;m2)\n    {\n        if ((m1.col != m2.col) || (m1.row != m2.row))\n        {\n            return false;\n        }\n\n        const float epsilon = 1e-5f;\n        for (int row = 0; row &lt; m1.row; row++)\n        {\n            for (int col = 0; col &lt; m1.col; col++)\n            {\n                float diff = fabs(m1(row, col) - m2(row, col));\n                if (diff &gt; epsilon)\n                {\n                    std::cout &lt;&lt; \"operator == Error: \" &lt;&lt; row &lt;&lt; \" \" &lt;&lt; col &lt;&lt; \", m1.data=\" &lt;&lt; m1(row, col) &lt;&lt; \", m2.data=\" &lt;&lt; m2(row, col) &lt;&lt; \", diff=\" &lt;&lt; diff &lt;&lt; std::endl;\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/","title":"Tests","text":"<p>Tip</p> <p>The following test code and cases also serve as usage teaching examples.</p>"},{"location":"MATH/MATRIX/tiny-matrix-test/#tiny_matrix_testhpp","title":"tiny_matrix_test.hpp","text":"<pre><code>/**\n * @file tiny_matrix_test.hpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the test of the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_matrix.hpp\" // TinyMatrix Header\n\n/* STATEMENTS */\nvoid tiny_matrix_test();  // C-compatible test entry\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#tiny_matrix_testcpp","title":"tiny_matrix_test.cpp","text":"<pre><code>/**\n * @file tiny_matrix_test.cpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the source file for the test of the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @copyright Copyright (c) 2025\n *\n */\n\n/* DEPENDENCIES */\n#include \"tiny_matrix_test.hpp\" // TinyMatrix Test Header\n#include \"tiny_time.h\"           // For performance testing\n\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;cmath&gt;\n#include &lt;sstream&gt;  // For std::istringstream (used in stream operator tests)\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n#include \"esp_task_wdt.h\"       // For FreeRTOS task watchdog\n#endif\n\n/* PERFORMANCE TESTING MACROS */\n// Reduced matrix sizes and iterations to prevent watchdog timeout and memory issues\n#define PERFORMANCE_TEST_ITERATIONS 100        // Reduced from 1000 to prevent timeout\n#define PERFORMANCE_TEST_ITERATIONS_HEAVY 10   // Reduced from 100 for compute-intensive operations\n#define PERFORMANCE_TEST_WARMUP 3              // Reduced from 10\n\n// Macro for timing a single operation\n#define TIME_OPERATION(operation, description) \\\n    do { \\\n        TinyTimeMark_t t0 = tiny_get_running_time(); \\\n        operation; \\\n        TinyTimeMark_t t1 = tiny_get_running_time(); \\\n        double dt_us = (double)(t1 - t0); \\\n        std::cout &lt;&lt; \"[Performance] \" &lt;&lt; description &lt;&lt; \": \" &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; dt_us &lt;&lt; \" us\\n\"; \\\n    } while(0)\n\n// Helper function to feed watchdog (inline to avoid function call overhead in tight loops)\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n// Static flag to track if task has been added to watchdog\nstatic bool task_wdt_added = false;\n\ninline void ensure_task_wdt_added()\n{\n    if (!task_wdt_added)\n    {\n        esp_task_wdt_add(NULL);  // Add current task to watchdog (NULL = current task)\n        task_wdt_added = true;\n    }\n}\n\ninline void feed_watchdog()\n{\n    ensure_task_wdt_added();\n    esp_task_wdt_reset();\n}\n\ninline void feed_watchdog_if_needed(int iteration, int interval)\n{\n    if ((iteration + 1) % interval == 0)\n    {\n        feed_watchdog();\n    }\n}\n#else\ninline void feed_watchdog()\n{\n    // No-op for non-ESP32 platforms\n}\n\ninline void feed_watchdog_if_needed(int iteration, int interval)\n{\n    // No-op for non-ESP32 platforms\n    (void)iteration;\n    (void)interval;\n}\n#endif\n\n// Macro for timing repeated operations\n#define TIME_REPEATED_OPERATION(operation, iterations, description) \\\n    do { \\\n        /* Feed watchdog before starting */ \\\n        feed_watchdog(); \\\n        /* Warmup */ \\\n        for (int w = 0; w &lt; PERFORMANCE_TEST_WARMUP; ++w) { \\\n            feed_watchdog();  /* Feed watchdog before each operation */ \\\n            operation; \\\n            feed_watchdog();  /* Feed watchdog after each operation */ \\\n        } \\\n        /* Actual test */ \\\n        TinyTimeMark_t perf_t0 = tiny_get_running_time(); \\\n        for (int i = 0; i &lt; iterations; ++i) { \\\n            /* Feed watchdog every 10 iterations (increased interval to reduce overhead) */ \\\n            if (i % 10 == 0) feed_watchdog(); \\\n            operation; \\\n        } \\\n        feed_watchdog();  /* Final feed after loop */ \\\n        TinyTimeMark_t perf_t1 = tiny_get_running_time(); \\\n        double perf_dt_total_us = (double)(perf_t1 - perf_t0); \\\n        double perf_dt_avg_us = perf_dt_total_us / iterations; \\\n        std::cout &lt;&lt; \"[Performance] \" &lt;&lt; description &lt;&lt; \" (\" &lt;&lt; iterations &lt;&lt; \" iterations): \" \\\n                  &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; perf_dt_total_us &lt;&lt; \" us total, \" \\\n                  &lt;&lt; perf_dt_avg_us &lt;&lt; \" us avg\\n\"; \\\n    } while(0)\n\n// Helper function to check if two matrices are approximately equal\nbool matrices_approximately_equal(const tiny::Mat &amp;m1, const tiny::Mat &amp;m2, float epsilon = 1e-5f)\n{\n    if (m1.row != m2.row || m1.col != m2.col)\n        return false;\n\n    for (int i = 0; i &lt; m1.row; ++i)\n    {\n        for (int j = 0; j &lt; m1.col; ++j)\n        {\n            if (std::fabs(m1(i, j) - m2(i, j)) &gt; epsilon)\n                return false;\n        }\n    }\n    return true;\n}\n\n// ============================================================================\n// Group 1: Object Foundation - Constructor &amp; Destructor Tests\n// ============================================================================\n// Purpose: Test object creation and destruction - the foundation of all operations\nvoid test_constructor_destructor()\n{\n    std::cout &lt;&lt; \"\\n[Group 1: Object Foundation - Constructor &amp; Destructor Tests]\\n\";\n\n    // Test 1.1: default constructor\n    std::cout &lt;&lt; \"[Test 1.1] Default Constructor\\n\";\n    tiny::Mat mat1;\n    mat1.print_info();\n    mat1.print_matrix(true);\n\n    // Test 1.2: constructor with rows and cols, using internal allocation\n    std::cout &lt;&lt; \"[Test 1.2] Constructor with Rows and Cols\\n\";\n    tiny::Mat mat2(3, 4);\n    mat2.print_info();\n    mat2.print_matrix(true);\n\n    // Test 1.3: constructor with rows and cols, specifying stride, using internal allocation\n    std::cout &lt;&lt; \"[Test 1.3] Constructor with Rows, Cols and Stride\\n\";\n    tiny::Mat mat3(3, 4, 5);\n    mat3.print_info();\n    mat3.print_matrix(true);\n\n    // Test 1.4: constructor with external data\n    std::cout &lt;&lt; \"[Test 1.4] Constructor with External Data\\n\";\n    float data[12] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};\n    tiny::Mat mat4(data, 3, 4);\n    mat4.print_info();\n    mat4.print_matrix(true);\n\n    // Test 1.5: constructor with external data and stride\n    std::cout &lt;&lt; \"[Test 1.5] Constructor with External Data and Stride\\n\";\n    float data_stride[15] = {0, 1, 2, 3, 0, 4, 5, 6, 7, 0, 8, 9, 10, 11, 0};\n    tiny::Mat mat5(data_stride, 3, 4, 5);\n    mat5.print_info();\n    mat5.print_matrix(true);\n\n    // Test 1.6: copy constructor\n    std::cout &lt;&lt; \"[Test 1.6] Copy Constructor\\n\";\n    tiny::Mat mat6(mat5);\n    mat6.print_info();\n    mat6.print_matrix(true);\n}\n\n// ============================================================================\n// Group 2: Object Foundation - Element Access Tests\n// ============================================================================\n// Purpose: Test element access - fundamental operation for data manipulation\nvoid test_element_access()\n{\n    std::cout &lt;&lt; \"\\n[Group 2: Object Foundation - Element Access Tests]\\n\";\n    tiny::Mat mat(2, 3);\n\n    // Test 2.1: non-const access\n    std::cout &lt;&lt; \"[Test 2.1] Non-const Access\\n\";\n    mat(0, 0) = 1.1f;\n    mat(0, 1) = 2.2f;\n    mat(0, 2) = 3.3f;\n    mat(1, 0) = 4.4f;\n    mat(1, 1) = 5.5f;\n    mat(1, 2) = 6.6f;\n    mat.print_info();\n    mat.print_matrix(true);\n\n    // Test 2.2: const access\n    std::cout &lt;&lt; \"[Test 2.2] Const Access\\n\";\n    const tiny::Mat const_mat = mat;\n    std::cout &lt;&lt; \"const_mat(0, 0): \" &lt;&lt; const_mat(0, 0) &lt;&lt; \"\\n\";\n}\n\n// ============================================================================\n// Group 3: Object Foundation - Data Manipulation Tests (ROI Operations)\n// ============================================================================\n// Purpose: Test ROI operations - efficient data views and submatrix operations\nvoid test_roi_operations()\n{\n    std::cout &lt;&lt; \"\\n[Group 3: Object Foundation - Data Manipulation Tests (ROI Operations)]\\n\";\n\n    // Material Matrices\n    tiny::Mat matA(2, 3);\n    for (int i = 0; i &lt; 2; ++i)\n    {\n        for (int j = 0; j &lt; 3; ++j)\n        {\n            matA(i, j) = i * 3 + j + 1;\n            matA(i, j) = matA(i, j) / 10;\n        }\n    }\n\n    float data[15] = {0, 1, 2, 3, 0, 4, 5, 6, 7, 0, 8, 9, 10, 11, 0};\n    tiny::Mat matB(data, 3, 4, 5);\n\n    tiny::Mat matC;\n\n    std::cout &lt;&lt; \"[Material Matrices]\\n\";\n    std::cout &lt;&lt; \"matA:\\n\";\n    matA.print_info();\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"matB:\\n\";\n    matB.print_info();\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"matC:\\n\";\n    matC.print_info();\n    matC.print_matrix(true);\n\n    // Test 3.1: Copy ROI\n    std::cout &lt;&lt; \"[Test 3.1] Copy ROI - Over Range Case\\n\";\n    matB.copy_paste(matA, 1, 2);\n    std::cout &lt;&lt; \"matB after copy_paste matA at (1, 2):\\n\";\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"nothing changed.\\n\";\n\n    std::cout &lt;&lt; \"[Test 3.1] Copy ROI - Suitable Range Case\\n\";\n    matB.copy_paste(matA, 1, 1);\n    std::cout &lt;&lt; \"matB after copy_paste matA at (1, 1):\\n\";\n    matB.print_info();\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"successfully copied.\\n\";\n\n    // Test 3.2: Copy Head\n    std::cout &lt;&lt; \"[Test 3.2] Copy Head\\n\";\n    matC.copy_head(matB);\n    std::cout &lt;&lt; \"matC after copy_head matB:\\n\";\n    matC.print_info();\n    matC.print_matrix(true);\n\n    std::cout &lt;&lt; \"[Test 3.2] Copy Head - Memory Sharing Check\\n\"; // matB and matC share the same data pointer\n    matB(0, 0) = 99.99f;\n    std::cout &lt;&lt; \"matB(0, 0) = 99.99f\\n\";\n    std::cout &lt;&lt; \"matC:\\n\";\n    matC.print_info();\n    matC.print_matrix(true);\n\n    // Test 3.3: Get a View of ROI - low level function\n    std::cout &lt;&lt; \"[Test 3.3] Get a View of ROI - Low Level Function\\n\";\n    std::cout &lt;&lt; \"get a view of ROI with overrange dimensions - rows:\\n\";\n    tiny::Mat roi1 = matB.view_roi(1, 1, 3, 2); // note here, C++ will use the copy constructor, which will copy according to the case (submatrix - shallow copy | normal - deep copy)\n    std::cout &lt;&lt; \"get a view of ROI with overrange dimensions - cols:\\n\";\n    tiny::Mat roi2 = matB.view_roi(1, 1, 2, 4); // note here, C++ will use the copy constructor, which will copy according to the case (submatrix - shallow copy | normal - deep copy)\n    std::cout &lt;&lt; \"get a view of ROI with suitable dimensions:\\n\";\n    tiny::Mat roi3 = matB.view_roi(1, 1, 2, 2); // note here, C++ will use the copy constructor, which will copy according to the case (submatrix - shallow copy | normal - deep copy)\n    std::cout &lt;&lt; \"roi3:\\n\";\n    roi3.print_info();\n    roi3.print_matrix(true);\n\n    // Test 3.4: Get a View of ROI - using ROI structure\n    std::cout &lt;&lt; \"[Test 3.4] Get a View of ROI - Using ROI Structure\\n\";\n    tiny::Mat::ROI roi_struct(1, 1, 2, 2);\n    tiny::Mat roi4 = matB.view_roi(roi_struct);\n    roi4.print_info();\n    roi4.print_matrix(true);\n\n    // Test 3.5: Copy ROI - low level function\n    std::cout &lt;&lt; \"[Test 3.5] Copy ROI - Low Level Function\\n\";\n    tiny::Mat mat_deep_copy = matB.copy_roi(1, 1, 2, 2);\n    mat_deep_copy.print_info();\n    mat_deep_copy.print_matrix(true);\n\n    // Test 3.6: Copy ROI - using ROI structure\n    std::cout &lt;&lt; \"[Test 3.6] Copy ROI - Using ROI Structure\\n\";\n    TinyTimeMark_t tic1 = tiny_get_running_time();\n    tiny::Mat::ROI roi_struct2(1, 1, 2, 2);\n    tiny::Mat mat_deep_copy2 = matB.copy_roi(roi_struct2);\n    TinyTimeMark_t toc1 = tiny_get_running_time();\n    TinyTimeMark_t copy_roi_time = toc1 - tic1;\n    std::cout &lt;&lt; \"time for copy_roi using ROI structure: \" &lt;&lt; copy_roi_time &lt;&lt; \" ms\\n\";\n    mat_deep_copy2.print_info();\n    mat_deep_copy2.print_matrix(true);\n\n    // Test 3.7: Block\n    std::cout &lt;&lt; \"[Test 3.7] Block\\n\";\n    TinyTimeMark_t tic2 = tiny_get_running_time();\n    tiny::Mat mat_block = matB.block(1, 1, 2, 2);\n    TinyTimeMark_t toc2 = tiny_get_running_time();\n    TinyTimeMark_t block_roi_time = toc2 - tic2;\n    std::cout &lt;&lt; \"time for block: \" &lt;&lt; block_roi_time &lt;&lt; \" ms\\n\";\n    mat_block.print_info();\n    mat_block.print_matrix(true);\n\n    // Test 3.8: Swap Rows\n    std::cout &lt;&lt; \"[Test 3.8] Swap Rows\\n\";\n    std::cout &lt;&lt; \"matB before swap rows:\\n\";\n    matB.print_info();\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"matB after swap_rows(0, 2):\\n\";\n    matB.swap_rows(0, 2);\n    matB.print_info();\n    matB.print_matrix(true);\n\n    // Test 3.9: Swap Columns\n    std::cout &lt;&lt; \"[Test 3.9] Swap Columns\\n\";\n    std::cout &lt;&lt; \"matB before swap columns:\\n\";\n    matB.print_info();\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"matB after swap_cols(0, 2):\\n\";\n    matB.swap_cols(0, 2);\n    matB.print_info();\n    matB.print_matrix(true);\n\n    // Test 3.10: Clear\n    std::cout &lt;&lt; \"[Test 3.10] Clear\\n\";\n    std::cout &lt;&lt; \"matB before clear:\\n\";\n    matB.print_info();\n    matB.print_matrix(true);\n    std::cout &lt;&lt; \"matB after clear:\\n\";\n    matB.clear();\n    matB.print_info();\n    matB.print_matrix(true);\n}\n\n// ============================================================================\n// Group 4: Basic Operations - Arithmetic Operators Tests\n// ============================================================================\n// Purpose: Test basic arithmetic operations - foundation for numerical computations\n// Group 4.1: Assignment Operator\nvoid test_assignment_operator()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.1: Basic Operations - Assignment Operator Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.1.1] Assignment (Same Dimensions)\\n\";\n    tiny::Mat dst(2, 3), src(2, 3);\n    for (int i = 0; i &lt; 2; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            src(i, j) = static_cast&lt;float&gt;(i * 3 + j + 1);\n    dst = src;\n    dst.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.1.2] Assignment (Different Dimensions)\\n\";\n    tiny::Mat dst2(4, 2);\n    dst2 = src;\n    dst2.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.1.3] Assignment to Sub-Matrix (Expect Error)\\n\";\n    float data[15] = {0, 1, 2, 3, 0, 4, 5, 6, 7, 0, 8, 9, 10, 11, 0};\n    tiny::Mat base(data, 3, 4, 5);\n    tiny::Mat subView = base.view_roi(1, 1, 2, 2);\n    subView = src;\n    subView.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.1.4] Self-Assignment\\n\";\n    src = src;\n    src.print_matrix(true);\n}\n\n// Group 4.2: Matrix Addition\nvoid test_matrix_addition()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.2: Matrix Addition Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.2.1] Matrix Addition (Same Dimensions)\\n\";\n    tiny::Mat A(2, 3), B(2, 3);\n    for (int i = 0; i &lt; 2; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n        {\n            A(i, j) = static_cast&lt;float&gt;(i * 3 + j + 1);\n            B(i, j) = 1.0f;\n        }\n    A += B;\n    A.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.2.2] Sub-Matrix Addition\\n\";\n    float data[20] = {0,1,2,3,0,4,5,6,7,0,8,9,10,11,0,12,13,14,15,0};\n    tiny::Mat base(data, 4, 4, 5);\n    tiny::Mat subA = base.view_roi(1,1,2,2);\n    tiny::Mat subB = base.view_roi(1,1,2,2);\n    subA += subB;\n    subA.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.2.3] Full Matrix + Sub-Matrix Addition\\n\";\n    tiny::Mat full(2,2);\n    for(int i=0;i&lt;2;++i) for(int j=0;j&lt;2;++j) full(i,j)=2.0f;\n    full += subB;\n    full.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.2.4] Addition Dimension Mismatch (Expect Error)\\n\";\n    tiny::Mat wrongDim(3,3);\n    full += wrongDim;\n}\n\n// Group 4.3: Constant Addition\nvoid test_constant_addition()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.3: Constant Addition Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.3.1] Full Matrix + Constant\\n\";\n    tiny::Mat mat1(2,3);\n    for (int i = 0; i &lt; 2; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat1(i,j) = static_cast&lt;float&gt;(i*3 + j);\n    mat1 += 5.0f;\n    mat1.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.3.2] Sub-Matrix + Constant\\n\";\n    float data[20] = {0,1,2,3,0,4,5,6,7,0,8,9,10,11,0,12,13,14,15,0};\n    tiny::Mat base(data,4,4,5);\n    tiny::Mat sub = base.view_roi(1,1,2,2);\n    sub += 3.0f;\n    sub.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.3.3] Add Zero\\n\";\n    tiny::Mat mat2(2,2);\n    mat2(0,0)=1; mat2(0,1)=2; mat2(1,0)=3; mat2(1,1)=4;\n    mat2 += 0.0f;\n    mat2.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.3.4] Add Negative Constant\\n\";\n    tiny::Mat mat3(2,2);\n    mat3(0,0)=10; mat3(0,1)=20; mat3(1,0)=30; mat3(1,1)=40;\n    mat3 += -15.0f;\n    mat3.print_matrix(true);\n}\n\n// Group 4.4: Matrix Subtraction\nvoid test_matrix_subtraction()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.4: Matrix Subtraction Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.4.1] Matrix Subtraction\\n\";\n    tiny::Mat A(2,2), B(2,2);\n    A(0,0)=5; A(0,1)=7; A(1,0)=9; A(1,1)=11;\n    B(0,0)=1; B(0,1)=2; B(1,0)=3; B(1,1)=4;\n    A -= B;\n    A.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.4.2] Subtraction Dimension Mismatch (Expect Error)\\n\";\n    tiny::Mat wrong(3,3);\n    A -= wrong;\n}\n\n// Group 4.5: Constant Subtraction\nvoid test_constant_subtraction()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.5: Constant Subtraction Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.5.1] Full Matrix - Constant\\n\";\n    tiny::Mat mat(2,3);\n    for (int i=0;i&lt;2;++i) for(int j=0;j&lt;3;++j) mat(i,j) = i*3+j+1;\n    mat -= 2.0f;\n    mat.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.5.2] Sub-Matrix - Constant\\n\";\n    float data[15] = {0,1,2,3,0,4,5,6,7,0,8,9,10,11,0};\n    tiny::Mat base(data,3,4,5);\n    tiny::Mat sub = base.view_roi(1,1,2,2);\n    sub -= 1.5f;\n    sub.print_matrix(true);\n}\n\n// Group 4.6: Matrix Element-wise Division\nvoid test_matrix_division()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.6: Matrix Element-wise Division Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.6.1] Element-wise Division (Same Dimensions, No Zero)\\n\";\n    tiny::Mat A(2, 2), B(2, 2);\n    A(0,0) = 10; A(0,1) = 20; A(1,0) = 30; A(1,1) = 40;\n    B(0,0) = 2;  B(0,1) = 4;  B(1,0) = 5;  B(1,1) = 8;\n    A /= B;\n    A.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.6.2] Dimension Mismatch (Expect Error)\\n\";\n    tiny::Mat wrongDim(3, 3);\n    A /= wrongDim;\n\n    std::cout &lt;&lt; \"\\n[Test 4.6.3] Division by Matrix Containing Zero (Expect Error)\\n\";\n    tiny::Mat C(2, 2), D(2, 2);\n    C(0,0)=5; C(0,1)=10; C(1,0)=15; C(1,1)=20;\n    D(0,0)=1; D(0,1)=0;  D(1,0)=3;  D(1,1)=4;  // Contains zero\n    C /= D;\n    C.print_matrix(true);  // Should remain unchanged\n}\n\n// Group 4.7: Constant Division\nvoid test_constant_division()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.7: Matrix Division by Constant Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.7.1] Divide Full Matrix by Positive Constant\\n\";\n    tiny::Mat mat1(2, 3);\n    for (int i = 0; i &lt; 2; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat1(i, j) = static_cast&lt;float&gt;(i * 3 + j + 2);  // Avoid zero\n    mat1 /= 2.0f;\n    mat1.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.7.2] Divide Matrix by Negative Constant\\n\";\n    tiny::Mat mat2(2, 2);\n    mat2(0,0)=6; mat2(0,1)=12; mat2(1,0)=18; mat2(1,1)=24;\n    mat2 /= -3.0f;\n    mat2.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.7.3] Division by Zero Constant (Expect Error)\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3(0,0)=1; mat3(0,1)=2; mat3(1,0)=3; mat3(1,1)=4;\n    mat3 /= 0.0f;\n    mat3.print_matrix(true);  // Should remain unchanged\n}\n\n// Group 4.8: Matrix Exponentiation\nvoid test_matrix_exponentiation()\n{\n    std::cout &lt;&lt; \"\\n[Group 4.8: Matrix Exponentiation Tests]\\n\";\n\n    std::cout &lt;&lt; \"\\n[Test 4.8.1] Raise Each Element to Power of 2\\n\";\n    tiny::Mat mat1(2, 2);\n    mat1(0,0)=2; mat1(0,1)=3; mat1(1,0)=4; mat1(1,1)=5;\n    tiny::Mat result1 = mat1 ^ 2;\n    result1.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.8.2] Raise Each Element to Power of 0\\n\";\n    tiny::Mat mat2(2, 2);\n    mat2(0,0)=7; mat2(0,1)=-3; mat2(1,0)=0.5f; mat2(1,1)=10;\n    tiny::Mat result2 = mat2 ^ 0;\n    result2.print_matrix(true);  // Expect all 1\n\n    std::cout &lt;&lt; \"\\n[Test 4.8.3] Raise Each Element to Power of 1\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3(0,0)=9; mat3(0,1)=8; mat3(1,0)=7; mat3(1,1)=6;\n    tiny::Mat result3 = mat3 ^ 1;\n    result3.print_matrix(true);  // Expect same as original\n\n    std::cout &lt;&lt; \"\\n[Test 4.8.4] Raise Each Element to Power of -1 (Expect Error or Warning)\\n\";\n    tiny::Mat mat4(2, 2);\n    mat4(0,0)=1; mat4(0,1)=2; mat4(1,0)=4; mat4(1,1)=5;\n    tiny::Mat result4 = mat4 ^ -1;\n    result4.print_matrix(true);\n\n    std::cout &lt;&lt; \"\\n[Test 4.8.5] Raise Matrix Containing Zero to Power of 3\\n\";\n    tiny::Mat mat5(2, 2);\n    mat5(0,0)=0; mat5(0,1)=2; mat5(1,0)=-1; mat5(1,1)=3;\n    tiny::Mat result5 = mat5 ^ 3;\n    result5.print_matrix(true);\n}\n\n// ============================================================================\n// Group 5: Matrix Properties - Linear Algebra Tests\n// ============================================================================\n// Purpose: Test matrix properties and basic linear algebra operations\n// Group 5.1: Matrix Transpose\nvoid test_matrix_transpose()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.1: Matrix Properties - Matrix Transpose Tests]\\n\";\n\n    // Test 5.1.1: Basic 2x3 matrix transpose\n    std::cout &lt;&lt; \"\\n[Test 5.1.1] Transpose of 2x3 Matrix\\n\";\n    tiny::Mat mat1(2, 3);\n    int val = 1;\n    for (int i = 0; i &lt; 2; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat1(i, j) = val++;\n\n    std::cout &lt;&lt; \"Original 2x3 Matrix:\\n\";\n    mat1.print_matrix(true);\n\n    tiny::Mat transposed1 = mat1.transpose();\n    std::cout &lt;&lt; \"Transposed 3x2 Matrix:\\n\";\n    transposed1.print_matrix(true);\n\n    // Test 5.1.2: Square matrix transpose (3x3)\n    std::cout &lt;&lt; \"\\n[Test 5.1.2] Transpose of 3x3 Square Matrix\\n\";\n    tiny::Mat mat2(3, 3);\n    val = 1;\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat2(i, j) = val++;\n\n    std::cout &lt;&lt; \"Original 3x3 Matrix:\\n\";\n    mat2.print_matrix(true);\n\n    tiny::Mat transposed2 = mat2.transpose();\n    std::cout &lt;&lt; \"Transposed 3x3 Matrix:\\n\";\n    transposed2.print_matrix(true);\n\n    // Test 5.1.3: Matrix with padding (4x2, stride=3)\n    std::cout &lt;&lt; \"\\n[Test 5.1.3] Transpose of Matrix with Padding\\n\";\n    float data[12] = {1, 2, 0, 3, 4, 0, 5, 6, 0, 7, 8, 0};  // stride=3, 4 rows\n    tiny::Mat mat3(data, 4, 2, 3);\n    std::cout &lt;&lt; \"Original 4x2 Matrix (with padding):\\n\";\n    mat3.print_matrix(true);\n\n    tiny::Mat transposed3 = mat3.transpose();\n    std::cout &lt;&lt; \"Transposed 2x4 Matrix:\\n\";\n    transposed3.print_matrix(true);\n\n    // Test 5.1.4: Transpose of empty matrix\n    std::cout &lt;&lt; \"\\n[Test 5.1.4] Transpose of Empty Matrix\\n\";\n    tiny::Mat mat4;\n    mat4.print_matrix(true);\n\n    tiny::Mat transposed4 = mat4.transpose();\n    transposed4.print_matrix(true);\n}\n\n// Group 5.2: Matrix Minor and Cofactor\nvoid test_matrix_cofactor()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.2: Matrix Minor and Cofactor Tests]\\n\";\n\n    // Test 5.2.1: Minor of 3x3 Matrix - Standard Case\n    std::cout &lt;&lt; \"\\n[Test 5.2.1] Minor of 3x3 Matrix (Remove Row 1, Col 1)\\n\";\n    tiny::Mat mat1(3, 3);\n    int val = 1;\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat1(i, j) = val++;\n\n    std::cout &lt;&lt; \"Original 3x3 Matrix:\\n\";\n    mat1.print_matrix(true);\n\n    tiny::Mat minor1 = mat1.minor(1, 1);\n    std::cout &lt;&lt; \"Minor Matrix (remove row 1, col 1, no sign):\\n\";\n    minor1.print_matrix(true);  // Expected: [[1,3],[7,9]]\n\n    // Test 5.2.2: Cofactor of 3x3 Matrix - Same position\n    std::cout &lt;&lt; \"\\n[Test 5.2.2] Cofactor of 3x3 Matrix (Remove Row 1, Col 1)\\n\";\n    std::cout &lt;&lt; \"Note: Cofactor matrix is the same as minor matrix.\\n\";\n    std::cout &lt;&lt; \"      The sign (-1)^(i+j) is applied when computing cofactor value, not to matrix elements.\\n\";\n    tiny::Mat cof1 = mat1.cofactor(1, 1);\n    std::cout &lt;&lt; \"Cofactor Matrix (same as minor):\\n\";\n    cof1.print_matrix(true);  // Expected: [[1,3],[7,9]] (same as minor)\n\n    // Test 5.2.3: Minor - Remove first row and first column\n    std::cout &lt;&lt; \"\\n[Test 5.2.3] Minor (Remove Row 0, Col 0)\\n\";\n    tiny::Mat minor2 = mat1.minor(0, 0);\n    minor2.print_matrix(true);  // Expected: [[5,6],[8,9]]\n\n    // Test 5.2.4: Cofactor - Remove first row and first column\n    std::cout &lt;&lt; \"\\n[Test 5.2.4] Cofactor (Remove Row 0, Col 0)\\n\";\n    std::cout &lt;&lt; \"Note: Cofactor matrix is the same as minor matrix.\\n\";\n    tiny::Mat cof2 = mat1.cofactor(0, 0);\n    cof2.print_matrix(true);  // Expected: [[5,6],[8,9]] (same as minor)\n\n    // Test 5.2.5: Cofactor - Remove row 0, col 1\n    std::cout &lt;&lt; \"\\n[Test 5.2.5] Cofactor (Remove Row 0, Col 1)\\n\";\n    std::cout &lt;&lt; \"Note: Cofactor matrix is the same as minor matrix.\\n\";\n    std::cout &lt;&lt; \"      When computing cofactor value, sign (-1)^(0+1) = -1 would be applied.\\n\";\n    tiny::Mat cof2_neg = mat1.cofactor(0, 1);\n    std::cout &lt;&lt; \"Cofactor Matrix (same as minor):\\n\";\n    cof2_neg.print_matrix(true);  // Expected: [[4,6],[7,9]] (same as minor, no sign in matrix)\n\n    // Test 5.2.6: Minor - Remove last row and last column\n    std::cout &lt;&lt; \"\\n[Test 5.2.6] Minor (Remove Row 2, Col 2)\\n\";\n    tiny::Mat minor3 = mat1.minor(2, 2);\n    minor3.print_matrix(true);  // Expected: [[1,2],[4,5]]\n\n    // Test 5.2.7: Cofactor - Remove last row and last column\n    std::cout &lt;&lt; \"\\n[Test 5.2.7] Cofactor (Remove Row 2, Col 2)\\n\";\n    std::cout &lt;&lt; \"Note: Cofactor matrix is the same as minor matrix.\\n\";\n    tiny::Mat cof3 = mat1.cofactor(2, 2);\n    cof3.print_matrix(true);  // Expected: [[1,2],[4,5]] (same as minor)\n\n    // Test 5.2.8: 4x4 Matrix Example - Minor\n    std::cout &lt;&lt; \"\\n[Test 5.2.8] Minor of 4x4 Matrix (Remove Row 2, Col 1)\\n\";\n    tiny::Mat mat4(4, 4);\n    val = 1;\n    for (int i = 0; i &lt; 4; ++i)\n        for (int j = 0; j &lt; 4; ++j)\n            mat4(i, j) = val++;\n\n    mat4.print_matrix(true);\n    tiny::Mat minor4 = mat4.minor(2, 1);\n    std::cout &lt;&lt; \"Minor Matrix:\\n\";\n    minor4.print_matrix(true);\n\n    // Test 5.2.9: 4x4 Matrix Example - Cofactor\n    std::cout &lt;&lt; \"\\n[Test 5.2.9] Cofactor of 4x4 Matrix (Remove Row 2, Col 1)\\n\";\n    std::cout &lt;&lt; \"Note: Cofactor matrix is the same as minor matrix.\\n\";\n    std::cout &lt;&lt; \"      When computing cofactor value, sign (-1)^(2+1) = -1 would be applied.\\n\";\n    tiny::Mat cof4 = mat4.cofactor(2, 1);\n    std::cout &lt;&lt; \"Cofactor Matrix (same as minor):\\n\";\n    cof4.print_matrix(true);\n\n    // Test 5.2.10: Non-square Matrix (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.2.10] Non-square Matrix (Expect Error)\\n\";\n    tiny::Mat rectMat(3, 4);\n    std::cout &lt;&lt; \"Testing minor():\\n\";\n    rectMat.minor(1, 1).print_matrix(true);  // Should trigger error and return empty matrix\n    std::cout &lt;&lt; \"Testing cofactor():\\n\";\n    rectMat.cofactor(1, 1).print_matrix(true);  // Should trigger error and return empty matrix\n}\n\n// Group 5.3: Matrix Determinant\nvoid test_matrix_determinant()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.3: Matrix Determinant Tests]\\n\";\n\n    // Test 5.3.1: 1x1 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.3.1] 1x1 Matrix Determinant\\n\";\n    tiny::Mat mat1(1, 1);\n    mat1(0, 0) = 7;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat1.print_matrix(true);\n    std::cout &lt;&lt; \"Determinant: \" &lt;&lt; mat1.determinant() &lt;&lt; \"  (Expected: 7)\\n\";\n\n    // Test 5.3.2: 2x2 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.3.2] 2x2 Matrix Determinant\\n\";\n    tiny::Mat mat2(2, 2);\n    mat2(0, 0) = 3; mat2(0, 1) = 8;\n    mat2(1, 0) = 4; mat2(1, 1) = 6;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat2.print_matrix(true);\n    std::cout &lt;&lt; \"Determinant: \" &lt;&lt; mat2.determinant() &lt;&lt; \"  (Expected: -14)\\n\";\n\n    // Test 5.3.3: 3x3 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.3.3] 3x3 Matrix Determinant\\n\";\n    tiny::Mat mat3(3, 3);\n    mat3(0,0) = 1; mat3(0,1) = 2; mat3(0,2) = 3;\n    mat3(1,0) = 0; mat3(1,1) = 4; mat3(1,2) = 5;\n    mat3(2,0) = 1; mat3(2,1) = 0; mat3(2,2) = 6;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat3.print_matrix(true);\n    std::cout &lt;&lt; \"Determinant: \" &lt;&lt; mat3.determinant() &lt;&lt; \"  (Expected: 22)\\n\";\n\n    // Test 5.3.4: 4x4 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.3.4] 4x4 Matrix Determinant\\n\";\n    tiny::Mat mat4(4, 4);\n    int val = 1;\n    for (int i = 0; i &lt; 4; ++i)\n        for (int j = 0; j &lt; 4; ++j)\n            mat4(i, j) = val++;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat4.print_matrix(true);\n    std::cout &lt;&lt; \"Note: This matrix has linearly dependent rows (each row differs by constant 4),\\n\";\n    std::cout &lt;&lt; \"      so the determinant should be 0.\\n\";\n    std::cout &lt;&lt; \"Determinant: \" &lt;&lt; mat4.determinant() &lt;&lt; \"  (Expected: 0)\\n\";  \n\n    // Test 5.3.5: 5x5 Matrix (Tests Auto-select Mechanism)\n    std::cout &lt;&lt; \"\\n[Test 5.3.5] 5x5 Matrix Determinant (Tests Auto-select to LU Method)\\n\";\n    tiny::Mat mat5_basic(5, 5);\n    // Create a well-conditioned 5x5 matrix\n    mat5_basic(0,0) = 2; mat5_basic(0,1) = 1; mat5_basic(0,2) = 0; mat5_basic(0,3) = 0; mat5_basic(0,4) = 0;\n    mat5_basic(1,0) = 1; mat5_basic(1,1) = 2; mat5_basic(1,2) = 1; mat5_basic(1,3) = 0; mat5_basic(1,4) = 0;\n    mat5_basic(2,0) = 0; mat5_basic(2,1) = 1; mat5_basic(2,2) = 2; mat5_basic(2,3) = 1; mat5_basic(2,4) = 0;\n    mat5_basic(3,0) = 0; mat5_basic(3,1) = 0; mat5_basic(3,2) = 1; mat5_basic(3,3) = 2; mat5_basic(3,4) = 1;\n    mat5_basic(4,0) = 0; mat5_basic(4,1) = 0; mat5_basic(4,2) = 0; mat5_basic(4,3) = 1; mat5_basic(4,4) = 2;\n    std::cout &lt;&lt; \"Matrix (5x5, tridiagonal):\\n\";\n    mat5_basic.print_matrix(true);\n    float det5_basic = mat5_basic.determinant();\n    std::cout &lt;&lt; \"Determinant (auto-select, should use LU for n &gt; 4): \" &lt;&lt; det5_basic &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Note: For n = 5 &gt; 4, auto-select should use LU decomposition (O(n\u00b3)).\\n\";\n\n    // Test 5.3.6: Non-square Matrix (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.3.6] Non-square Matrix (Expect Error)\\n\";\n    tiny::Mat rectMat(3, 4);\n    std::cout &lt;&lt; \"Matrix (3x4, non-square):\\n\";\n    rectMat.print_matrix(true);\n    float det_rect = rectMat.determinant();  // should trigger error\n    std::cout &lt;&lt; \"Determinant: \" &lt;&lt; det_rect &lt;&lt; \"  (Expected: 0 with error message)\\n\";\n\n    // Test 5.3.7: Comparison of Different Methods (5x5 Matrix)\n    std::cout &lt;&lt; \"\\n[Test 5.3.7] Comparison of Different Methods (5x5 Matrix)\\n\";\n    tiny::Mat mat_test(5, 5);\n    // Create a test matrix\n    for (int i = 0; i &lt; 5; ++i)\n    {\n        for (int j = 0; j &lt; 5; ++j)\n        {\n            mat_test(i, j) = static_cast&lt;float&gt;((i + 1) * (j + 1) + (i == j ? 1.0f : 0.0f));\n        }\n    }\n    std::cout &lt;&lt; \"Matrix (5x5):\\n\";\n    mat_test.print_matrix(true);\n    float det_auto = mat_test.determinant();\n    float det_laplace = mat_test.determinant_laplace();\n    float det_lu = mat_test.determinant_lu();\n    float det_gaussian = mat_test.determinant_gaussian();\n    std::cout &lt;&lt; \"Determinant (auto-select): \" &lt;&lt; det_auto &lt;&lt; \"  (should use LU for n &gt; 4)\\n\";\n    std::cout &lt;&lt; \"Determinant (Laplace):     \" &lt;&lt; det_laplace &lt;&lt; \"  (O(n!), slow for n=5)\\n\";\n    std::cout &lt;&lt; \"Determinant (LU):          \" &lt;&lt; det_lu &lt;&lt; \"  (O(n\u00b3), efficient)\\n\";\n    std::cout &lt;&lt; \"Determinant (Gaussian):    \" &lt;&lt; det_gaussian &lt;&lt; \"  (O(n\u00b3), efficient)\\n\";\n    std::cout &lt;&lt; \"Note: All methods should give the same result (within numerical precision).\\n\";\n    std::cout &lt;&lt; \"      Auto-select should use LU for n &gt; 4, avoiding slow Laplace expansion.\\n\";\n\n    // Test 5.3.8: Large Matrix (6x6) - Tests Efficient Methods\n    std::cout &lt;&lt; \"\\n[Test 5.3.8] Large Matrix (6x6) - Tests Efficient Methods\\n\";\n    tiny::Mat mat6(6, 6);\n    for (int i = 0; i &lt; 6; ++i)\n    {\n        for (int j = 0; j &lt; 6; ++j)\n        {\n            mat6(i, j) = static_cast&lt;float&gt;((i + 1) * (j + 1) + (i == j ? 0.5f : 0.0f));\n        }\n    }\n    std::cout &lt;&lt; \"Matrix (6x6, showing first 4x4 block):\\n\";\n    for (int i = 0; i &lt; 4; ++i)\n    {\n        for (int j = 0; j &lt; 4; ++j)\n        {\n            std::cout &lt;&lt; std::setw(10) &lt;&lt; mat6(i, j) &lt;&lt; \" \";\n        }\n        std::cout &lt;&lt; \"...\\n\";\n    }\n    std::cout &lt;&lt; \"...\\n\";\n    float det6_auto = mat6.determinant();\n    float det6_lu = mat6.determinant_lu();\n    float det6_gaussian = mat6.determinant_gaussian();\n    std::cout &lt;&lt; \"Determinant (auto-select, uses LU): \" &lt;&lt; det6_auto &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Determinant (LU):                   \" &lt;&lt; det6_lu &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Determinant (Gaussian):             \" &lt;&lt; det6_gaussian &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Note: For n &gt; 4, auto-select uses LU decomposition (O(n\u00b3) instead of O(n!)).\\n\";\n\n    // Test 5.3.9: Large Matrix (8x8) - Performance Test\n    std::cout &lt;&lt; \"\\n[Test 5.3.9] Large Matrix (8x8) - Performance Comparison\\n\";\n    tiny::Mat mat8(8, 8);\n    for (int i = 0; i &lt; 8; ++i)\n    {\n        for (int j = 0; j &lt; 8; ++j)\n        {\n            mat8(i, j) = static_cast&lt;float&gt;((i + 1) * (j + 1));\n        }\n    }\n    std::cout &lt;&lt; \"Matrix (8x8, showing first 4x4 block):\\n\";\n    // Print partial matrix for display\n    for (int i = 0; i &lt; 4; ++i)\n    {\n        for (int j = 0; j &lt; 4; ++j)\n        {\n            std::cout &lt;&lt; std::setw(10) &lt;&lt; mat8(i, j) &lt;&lt; \" \";\n        }\n        std::cout &lt;&lt; \"...\\n\";\n    }\n    std::cout &lt;&lt; \"...\\n\";\n    float det8_lu = mat8.determinant_lu();\n    float det8_gaussian = mat8.determinant_gaussian();\n    std::cout &lt;&lt; \"Determinant (LU):       \" &lt;&lt; det8_lu &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Determinant (Gaussian): \" &lt;&lt; det8_gaussian &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Note: Both methods are O(n\u00b3) and should be much faster than Laplace expansion.\\n\";\n\n}\n\n// Group 5.4: Matrix Adjoint\nvoid test_matrix_adjoint()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.4: Matrix Adjoint Tests]\\n\";\n\n    // Test 5.4.1: 1x1 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.4.1] Adjoint of 1x1 Matrix\\n\";\n    tiny::Mat mat1(1, 1);\n    mat1(0, 0) = 5;\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat1.print_matrix(true);\n    tiny::Mat adj1 = mat1.adjoint();\n    std::cout &lt;&lt; \"Adjoint Matrix:\\n\";\n    adj1.print_matrix(true);  // Expected: [1]\n\n    // Test 5.4.2: 2x2 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.4.2] Adjoint of 2x2 Matrix\\n\";\n    tiny::Mat mat2(2, 2);\n    mat2(0, 0) = 1; mat2(0, 1) = 2;\n    mat2(1, 0) = 3; mat2(1, 1) = 4;\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat2.print_matrix(true);\n    tiny::Mat adj2 = mat2.adjoint();\n    std::cout &lt;&lt; \"Adjoint Matrix:\\n\";\n    adj2.print_matrix(true);  // Expected: [4, -2; -3, 1]\n\n    // Test 5.4.3: 3x3 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.4.3] Adjoint of 3x3 Matrix\\n\";\n    tiny::Mat mat3(3, 3);\n    mat3(0,0) = 1; mat3(0,1) = 2; mat3(0,2) = 3;\n    mat3(1,0) = 0; mat3(1,1) = 4; mat3(1,2) = 5;\n    mat3(2,0) = 1; mat3(2,1) = 0; mat3(2,2) = 6;\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat3.print_matrix(true);\n    tiny::Mat adj3 = mat3.adjoint();\n    std::cout &lt;&lt; \"Adjoint Matrix:\\n\";\n    adj3.print_matrix(true);\n    // No simple expected value, but should compute correctly\n\n    // Test 5.4.4: Non-Square Matrix (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.4.4] Adjoint of Non-Square Matrix (Expect Error)\\n\";\n    tiny::Mat rectMat(2, 3);\n    std::cout &lt;&lt; \"Original Matrix (2x3, non-square):\\n\";\n    rectMat.print_matrix(true);\n    tiny::Mat adjRect = rectMat.adjoint();\n    std::cout &lt;&lt; \"Adjoint Matrix (should be empty due to error):\\n\";\n    adjRect.print_matrix(true);  // Should be empty or default matrix\n\n}\n\n// Group 5.5: Matrix Normalization\nvoid test_matrix_normalize()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.5: Matrix Normalization Tests]\\n\";\n\n    // Test 5.5.1: Standard normalization\n    std::cout &lt;&lt; \"\\n[Test 5.5.1] Normalize a Standard 2x2 Matrix\\n\";\n    tiny::Mat mat1(2, 2);\n    mat1(0, 0) = 3.0f; mat1(0, 1) = 4.0f;\n    mat1(1, 0) = 3.0f; mat1(1, 1) = 4.0f;\n\n    std::cout &lt;&lt; \"Before normalization:\\n\";\n    mat1.print_matrix(true);\n\n    mat1.normalize();\n\n    std::cout &lt;&lt; \"After normalization (Expected L2 norm = 1):\\n\";\n    mat1.print_matrix(true);\n\n    // Test 5.5.2: Matrix with padding\n    std::cout &lt;&lt; \"\\n[Test 5.5.2] Normalize a 2x2 Matrix with Stride=4 (Padding Test)\\n\";\n    float data_with_padding[8] = {3.0f, 4.0f, 0.0f, 0.0f, 3.0f, 4.0f, 0.0f, 0.0f};\n    tiny::Mat mat2(data_with_padding, 2, 2, 4);  // 2x2 matrix, stride 4\n\n    std::cout &lt;&lt; \"Before normalization:\\n\";\n    mat2.print_matrix(true);\n\n    mat2.normalize();\n\n    std::cout &lt;&lt; \"After normalization:\\n\";\n    mat2.print_matrix(true);\n\n    // Test 5.5.3: Zero matrix normalization\n    std::cout &lt;&lt; \"\\n[Test 5.5.3] Normalize a Zero Matrix (Expect Warning)\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3.clear();  // Assuming clear() sets all elements to zero\n\n    mat3.print_matrix(true);\n    mat3.normalize();  // Should trigger warning\n}\n\n// Group 5.6: Matrix Norm Calculation\nvoid test_matrix_norm()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.6: Matrix Norm Calculation Tests]\\n\";\n\n    // Test 5.6.1: Simple 2x2 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.6.1] 2x2 Matrix Norm (Expect 5.0)\\n\";\n    tiny::Mat mat1(2, 2);\n    mat1(0, 0) = 3.0f; mat1(0, 1) = 4.0f;\n    mat1(1, 0) = 0.0f; mat1(1, 1) = 0.0f;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat1.print_matrix(true);\n    float norm1 = mat1.norm();\n    std::cout &lt;&lt; \"Calculated Norm: \" &lt;&lt; norm1 &lt;&lt; \"\\n\";\n\n    // Test 5.6.2: Zero Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.6.2] Zero Matrix Norm (Expect 0.0)\\n\";\n    tiny::Mat mat2(3, 3);\n    mat2.clear();  // Assuming clear() sets all elements to zero\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat2.print_matrix(true);\n    float norm2 = mat2.norm();\n    std::cout &lt;&lt; \"Calculated Norm: \" &lt;&lt; norm2 &lt;&lt; \"\\n\";\n\n    // Test 5.6.3: Matrix with Negative Values\n    std::cout &lt;&lt; \"\\n[Test 5.6.3] Matrix with Negative Values\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3(0, 0) = -1.0f; mat3(0, 1) = -2.0f;\n    mat3(1, 0) = -3.0f; mat3(1, 1) = -4.0f;\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat3.print_matrix(true);\n    float norm3 = mat3.norm();\n    std::cout &lt;&lt; \"Calculated Norm: \" &lt;&lt; norm3 &lt;&lt; \"  (Expect sqrt(30) \u2248 5.477)\\n\";\n\n    // Test 5.6.4: Matrix with Padding\n    std::cout &lt;&lt; \"\\n[Test 5.6.4] 2x2 Matrix with Stride=4 (Padding Test)\\n\";\n    float data4[8] = {1.0f, 2.0f, 0.0f, 0.0f, 3.0f, 4.0f, 0.0f, 0.0f};\n    tiny::Mat mat4(data4, 2, 2, 4);  // 2x2 matrix, stride 4\n    std::cout &lt;&lt; \"Matrix:\\n\";\n    mat4.print_matrix(true);\n    float norm4 = mat4.norm();\n    std::cout &lt;&lt; \"Calculated Norm: \" &lt;&lt; norm4 &lt;&lt; \"  (Expect sqrt(30) \u2248 5.477)\\n\";\n}\n\n// Group 5.7: Matrix Inversion\nvoid test_inverse_adjoint_adjoint()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.7: Matrix Inversion Tests]\\n\";\n\n    // Test 5.7.1: 2x2 Regular Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.7.1] Inverse of 2x2 Matrix\\n\";\n    tiny::Mat mat1(2, 2);\n    mat1(0, 0) = 4;  mat1(0, 1) = 7;\n    mat1(1, 0) = 2;  mat1(1, 1) = 6;\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat1.print_matrix(true);\n    tiny::Mat inv1 = mat1.inverse_adjoint();\n    std::cout &lt;&lt; \"Inverse Matrix:\\n\";\n    inv1.print_matrix(true);\n    std::cout &lt;&lt; \"Expected Approx:\\n[ 0.6  -0.7 ]\\n[ -0.2  0.4 ]\\n\";\n\n    // Test 5.7.2: Singular Matrix (Determinant = 0)\n    std::cout &lt;&lt; \"\\n[Test 5.7.2] Singular Matrix (Expect Error)\\n\";\n    tiny::Mat mat2(2, 2);\n    mat2(0, 0) = 1;  mat2(0, 1) = 2;\n    mat2(1, 0) = 2;  mat2(1, 1) = 4;   // Rank-deficient, det = 0\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat2.print_matrix(true);\n    std::cout &lt;&lt; \"Note: This matrix is singular (determinant = 0), so inverse should fail.\\n\";\n    tiny::Mat inv2 = mat2.inverse_adjoint();\n    std::cout &lt;&lt; \"Inverse Matrix (Should be zero matrix):\\n\";\n    inv2.print_matrix(true);\n\n    // Test 5.7.3: 3x3 Regular Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.7.3] Inverse of 3x3 Matrix\\n\";\n    tiny::Mat mat3(3, 3);\n    mat3(0,0) = 3; mat3(0,1) = 0; mat3(0,2) = 2;\n    mat3(1,0) = 2; mat3(1,1) = 0; mat3(1,2) = -2;\n    mat3(2,0) = 0; mat3(2,1) = 1; mat3(2,2) = 1;\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat3.print_matrix(true);\n    tiny::Mat inv3 = mat3.inverse_adjoint();\n    std::cout &lt;&lt; \"Inverse Matrix:\\n\";\n    inv3.print_matrix(true);\n\n    // Test 5.7.4: Non-Square Matrix (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.7.4] Non-Square Matrix (Expect Error)\\n\";\n    tiny::Mat mat4(2, 3);\n    std::cout &lt;&lt; \"Original Matrix (2x3, non-square):\\n\";\n    mat4.print_matrix(true);\n    tiny::Mat inv4 = mat4.inverse_adjoint();\n    std::cout &lt;&lt; \"Inverse Matrix (should be empty due to error):\\n\";\n    inv4.print_matrix(true);\n}\n\n// Group 5.8: Matrix Utilities\nvoid test_matrix_utilities()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.8: Matrix Utilities Tests]\\n\";\n\n    // Test 5.8.1: Identity Matrix (eye)\n    std::cout &lt;&lt; \"\\n[Test 5.8.1] Generate Identity Matrix (eye)\\n\";\n    tiny::Mat I3 = tiny::Mat::eye(3);\n    std::cout &lt;&lt; \"3x3 Identity Matrix:\\n\";\n    I3.print_matrix(true);\n\n    tiny::Mat I5 = tiny::Mat::eye(5);\n    std::cout &lt;&lt; \"5x5 Identity Matrix:\\n\";\n    I5.print_matrix(true);\n\n    // Test 5.8.2: Ones Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.8.2] Generate Ones Matrix\\n\";\n    tiny::Mat ones_3x4 = tiny::Mat::ones(3, 4);\n    std::cout &lt;&lt; \"3x4 Ones Matrix:\\n\";\n    ones_3x4.print_matrix(true);\n\n    tiny::Mat ones_4x4 = tiny::Mat::ones(4);\n    std::cout &lt;&lt; \"4x4 Ones Matrix (Square):\\n\";\n    ones_4x4.print_matrix(true);\n\n    // Test 5.8.3: Matrix Augmentation\n    std::cout &lt;&lt; \"\\n[Test 5.8.3] Augment Two Matrices Horizontally [A | B]\\n\";\n\n    // Prepare matrices A (2x2) and B (2x3)\n    tiny::Mat A(2, 2);\n    A(0,0) = 1;  A(0,1) = 2;\n    A(1,0) = 3;  A(1,1) = 4;\n\n    tiny::Mat B(2, 3);\n    B(0,0) = 5;  B(0,1) = 6;  B(0,2) = 7;\n    B(1,0) = 8;  B(1,1) = 9;  B(1,2) = 10;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B:\\n\";\n    B.print_matrix(true);\n\n    tiny::Mat AB = tiny::Mat::augment(A, B);\n    std::cout &lt;&lt; \"Augmented Matrix [A | B]:\\n\";\n    AB.print_matrix(true);\n\n    // Test 5.8.4: Row mismatch case\n    std::cout &lt;&lt; \"\\n[Test 5.8.4] Augment with Row Mismatch (Expect Error)\\n\";\n    tiny::Mat C(3, 2);  // 3x2 matrix\n    tiny::Mat invalidAug = tiny::Mat::augment(A, C);\n    invalidAug.print_info();  // Should show empty matrix due to error\n\n    // Test 5.8.5: Vertical Stack (vstack)\n    std::cout &lt;&lt; \"\\n[Test 5.8.5] Vertically Stack Two Matrices [A; B]\\n\";\n\n    // Prepare matrices A (2x3) and B (2x3)\n    tiny::Mat A_vstack(2, 3);\n    A_vstack(0,0) = 1;  A_vstack(0,1) = 2;  A_vstack(0,2) = 3;\n    A_vstack(1,0) = 4;  A_vstack(1,1) = 5;  A_vstack(1,2) = 6;\n\n    tiny::Mat B_vstack(2, 3);\n    B_vstack(0,0) = 7;  B_vstack(0,1) = 8;  B_vstack(0,2) = 9;\n    B_vstack(1,0) = 10; B_vstack(1,1) = 11; B_vstack(1,2) = 12;\n\n    std::cout &lt;&lt; \"Matrix A (top):\\n\";\n    A_vstack.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B (bottom):\\n\";\n    B_vstack.print_matrix(true);\n\n    tiny::Mat AB_vstack = tiny::Mat::vstack(A_vstack, B_vstack);\n    std::cout &lt;&lt; \"Vertically Stacked Matrix [A; B]:\\n\";\n    AB_vstack.print_matrix(true);\n    std::cout &lt;&lt; \"Expected: 4x3 matrix with A on top, B on bottom\\n\";\n\n    // Test 5.8.6: Vertical Stack with different row counts\n    std::cout &lt;&lt; \"\\n[Test 5.8.6] Vertical Stack with Different Row Counts (Same Columns)\\n\";\n    tiny::Mat A_small(1, 3);\n    A_small(0,0) = 1; A_small(0,1) = 2; A_small(0,2) = 3;\n\n    tiny::Mat B_large(3, 3);\n    B_large(0,0) = 4;  B_large(0,1) = 5;  B_large(0,2) = 6;\n    B_large(1,0) = 7;  B_large(1,1) = 8;  B_large(1,2) = 9;\n    B_large(2,0) = 10; B_large(2,1) = 11; B_large(2,2) = 12;\n\n    std::cout &lt;&lt; \"Matrix A (1x3):\\n\";\n    A_small.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B (3x3):\\n\";\n    B_large.print_matrix(true);\n\n    tiny::Mat AB_mixed = tiny::Mat::vstack(A_small, B_large);\n    std::cout &lt;&lt; \"Vertically Stacked Matrix [A; B] (1x3 + 3x3 = 4x3):\\n\";\n    AB_mixed.print_matrix(true);\n\n    // Test 5.8.7: Column mismatch case (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.8.7] VStack with Column Mismatch (Expect Error)\\n\";\n    tiny::Mat A_col(2, 2);\n    A_col(0,0) = 1; A_col(0,1) = 2;\n    A_col(1,0) = 3; A_col(1,1) = 4;\n\n    tiny::Mat B_col(2, 3);  // Different column count\n    B_col(0,0) = 5; B_col(0,1) = 6; B_col(0,2) = 7;\n    B_col(1,0) = 8; B_col(1,1) = 9; B_col(1,2) = 10;\n\n    std::cout &lt;&lt; \"Matrix A (2x2):\\n\";\n    A_col.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B (2x3, different columns):\\n\";\n    B_col.print_matrix(true);\n\n    tiny::Mat invalidVStack = tiny::Mat::vstack(A_col, B_col);\n    std::cout &lt;&lt; \"Result (should be empty due to error):\\n\";\n    invalidVStack.print_info();  // Should show empty matrix due to error\n\n}\n\n// Group 5.9: Gaussian Elimination\nvoid test_gaussian_eliminate()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.9: Gaussian Elimination Tests]\\n\";\n\n    // Test 5.9.1: Simple 3x3 System\n    std::cout &lt;&lt; \"\\n[Test 5.9.1] 3x3 Matrix (Simple Upper Triangular)\\n\";\n    tiny::Mat mat1(3, 3);\n    mat1(0,0) = 2; mat1(0,1) = 1; mat1(0,2) = -1;\n    mat1(1,0) = -3; mat1(1,1) = -1; mat1(1,2) = 2;\n    mat1(2,0) = -2; mat1(2,1) = 1; mat1(2,2) = 2;\n\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat1.print_matrix(true);\n\n    tiny::Mat result1 = mat1.gaussian_eliminate();\n\n    std::cout &lt;&lt; \"After Gaussian Elimination (Should be upper triangular):\\n\";\n    result1.print_matrix(true);\n\n    // Test 5.9.2: 3x4 Augmented Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.9.2] 3x4 Augmented Matrix (Linear System Ax = b)\\n\";\n    tiny::Mat mat2(3, 4);\n    mat2(0,0) = 1; mat2(0,1) = 2; mat2(0,2) = -1; mat2(0,3) =  8;\n    mat2(1,0) = -3; mat2(1,1) = -1; mat2(1,2) = 2; mat2(1,3) = -11;\n    mat2(2,0) = -2; mat2(2,1) = 1; mat2(2,2) = 2; mat2(2,3) = -3;\n\n    std::cout &lt;&lt; \"Original Augmented Matrix [A | b]:\\n\";\n    mat2.print_matrix(true);\n\n    tiny::Mat result2 = mat2.gaussian_eliminate();\n\n    std::cout &lt;&lt; \"After Gaussian Elimination (Row Echelon Form):\\n\";\n    result2.print_matrix(true);\n\n    // Test 5.9.3: Singular Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.9.3] Singular Matrix (No Unique Solution)\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3(0,0) = 1; mat3(0,1) = 2;\n    mat3(1,0) = 2; mat3(1,1) = 4;  // Linearly dependent rows\n\n    std::cout &lt;&lt; \"Original Singular Matrix:\\n\";\n    mat3.print_matrix(true);\n\n    tiny::Mat result3 = mat3.gaussian_eliminate();\n    std::cout &lt;&lt; \"After Gaussian Elimination (Should show rows of zeros):\\n\";\n    result3.print_matrix(true);\n\n    // Test 5.9.4: Zero Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.9.4] Zero Matrix\\n\";\n    tiny::Mat mat4(3, 3);\n    mat4.clear();  // Assuming clear() sets all elements to zero\n    mat4.print_matrix(true);\n\n    tiny::Mat result4 = mat4.gaussian_eliminate();\n    std::cout &lt;&lt; \"After Gaussian Elimination (Should be a zero matrix):\\n\";\n    result4.print_matrix(true);\n}\n\n\n// Group 5.10: Row Reduce from Gaussian (RREF Calculation)\nvoid test_row_reduce_from_gaussian()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.10: Row Reduce from Gaussian (RREF) Tests]\\n\";\n\n    // Test 5.10.1: Simple 3x4 augmented matrix (representing a system of equations)\n    std::cout &lt;&lt; \"\\n[Test 5.10.1] 3x4 Augmented Matrix\\n\";\n    tiny::Mat mat1(3, 4);\n\n    // Matrix:\n    // [ 1  2 -1  -4 ]\n    // [ 2  3 -1 -11 ]\n    // [-2  0 -3  22 ]\n    mat1(0,0) = 1;  mat1(0,1) = 2;  mat1(0,2) = -1; mat1(0,3) = -4;\n    mat1(1,0) = 2;  mat1(1,1) = 3;  mat1(1,2) = -1; mat1(1,3) = -11;\n    mat1(2,0) = -2; mat1(2,1) = 0;  mat1(2,2) = -3; mat1(2,3) = 22;\n\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat1.print_matrix(true);\n\n    tiny::Mat rref1 = mat1.gaussian_eliminate().row_reduce_from_gaussian();\n    std::cout &lt;&lt; \"RREF Result:\\n\";\n    rref1.print_matrix(true);\n\n    // Test 5.10.2: 2x3 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.10.2] 2x3 Matrix\\n\";\n    tiny::Mat mat2(2, 3);\n    mat2(0,0) = 1; mat2(0,1) = 2;  mat2(0,2) = 3;\n    mat2(1,0) = 4; mat2(1,1) = 5;  mat2(1,2) = 6;\n\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat2.print_matrix(true);\n\n    tiny::Mat rref2 = mat2.gaussian_eliminate().row_reduce_from_gaussian();\n    std::cout &lt;&lt; \"RREF Result:\\n\";\n    rref2.print_matrix(true);\n\n    // Test 5.10.3: Already reduced matrix (should remain the same)\n    std::cout &lt;&lt; \"\\n[Test 5.10.3] Already Reduced Matrix\\n\";\n    tiny::Mat mat3(2, 3);\n    mat3(0,0) = 1; mat3(0,1) = 0; mat3(0,2) = 2;\n    mat3(1,0) = 0; mat3(1,1) = 1; mat3(1,2) = 3;\n\n    std::cout &lt;&lt; \"Original Matrix:\\n\";\n    mat3.print_matrix(true);\n\n    tiny::Mat rref3 = mat3.row_reduce_from_gaussian();\n    std::cout &lt;&lt; \"RREF Result:\\n\";\n    rref3.print_matrix(true);\n}\n\n// Group 5.11: Gaussian Inverse\nvoid test_inverse_gje()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.11: Gaussian Inverse Tests]\\n\";\n\n    // Test 5.11.1: Regular 2x2 Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.11.1] 2x2 Matrix Inverse\\n\";\n    tiny::Mat mat1(2, 2);\n    mat1(0, 0) = 4; mat1(0, 1) = 7;\n    mat1(1, 0) = 2; mat1(1, 1) = 6;\n    std::cout &lt;&lt; \"Original matrix (mat1):\\n\";\n    mat1.print_matrix(true);\n\n    tiny::Mat invMat1 = mat1.inverse_gje();\n    std::cout &lt;&lt; \"Inverse matrix (mat1):\\n\";\n    invMat1.print_matrix(true);\n\n    // Test 5.11.2: Identity Matrix (should return identity matrix)\n    std::cout &lt;&lt; \"\\n[Test 5.11.2] Identity Matrix Inverse\\n\";\n    tiny::Mat mat2 = tiny::Mat::eye(3);\n    std::cout &lt;&lt; \"Original matrix (Identity):\\n\";\n    mat2.print_matrix(true);\n\n    tiny::Mat invMat2 = mat2.inverse_gje();\n    std::cout &lt;&lt; \"Inverse matrix (Identity):\\n\";\n    invMat2.print_matrix(true); // Expected: Identity matrix\n\n    // Test 5.11.3: Singular Matrix (should return empty matrix or indicate error)\n    std::cout &lt;&lt; \"\\n[Test 5.11.3] Singular Matrix (Expected: No Inverse)\\n\";\n    tiny::Mat mat3(3, 3);\n    mat3(0, 0) = 1; mat3(0, 1) = 2; mat3(0, 2) = 3;\n    mat3(1, 0) = 4; mat3(1, 1) = 5; mat3(1, 2) = 6;\n    mat3(2, 0) = 7; mat3(2, 1) = 8; mat3(2, 2) = 9;  // Determinant is 0\n    std::cout &lt;&lt; \"Original matrix (singular):\\n\";\n    mat3.print_matrix(true);\n\n    tiny::Mat invMat3 = mat3.inverse_gje();\n    std::cout &lt;&lt; \"Inverse matrix (singular):\\n\";\n    invMat3.print_matrix(true); // Expected: empty matrix or error message\n\n    // Test 5.11.4: 3x3 Matrix with a valid inverse\n    std::cout &lt;&lt; \"\\n[Test 5.11.4] 3x3 Matrix Inverse\\n\";\n    tiny::Mat mat4(3, 3);\n    mat4(0, 0) = 4; mat4(0, 1) = 7; mat4(0, 2) = 2;\n    mat4(1, 0) = 3; mat4(1, 1) = 5; mat4(1, 2) = 1;\n    mat4(2, 0) = 8; mat4(2, 1) = 6; mat4(2, 2) = 9;\n    std::cout &lt;&lt; \"Original matrix (mat4):\\n\";\n    mat4.print_matrix(true);\n\n    tiny::Mat invMat4 = mat4.inverse_gje();\n    std::cout &lt;&lt; \"Inverse matrix (mat4):\\n\";\n    invMat4.print_matrix(true); // Check that the inverse is calculated correctly\n\n    // Test 5.11.5: Non-square Matrix (should return error or empty matrix)\n    std::cout &lt;&lt; \"\\n[Test 5.11.5] Non-square Matrix Inverse (Expected Error)\\n\";\n    tiny::Mat mat5(2, 3);\n    mat5(0, 0) = 1; mat5(0, 1) = 2; mat5(0, 2) = 3;\n    mat5(1, 0) = 4; mat5(1, 1) = 5; mat5(1, 2) = 6;\n    std::cout &lt;&lt; \"Original matrix (non-square):\\n\";\n    mat5.print_matrix(true);\n\n    tiny::Mat invMat5 = mat5.inverse_gje();\n    std::cout &lt;&lt; \"Inverse matrix (non-square):\\n\";\n    invMat5.print_matrix(true); // Expected: Error message or empty matrix\n}\n\n// Group 5.12: Dot Product\nvoid test_dotprod()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.12: Dot Product Tests]\\n\";\n\n    // Test 5.12.1: Valid Dot Product Calculation (Same Length Vectors)\n    std::cout &lt;&lt; \"\\n[Test 5.12.1] Valid Dot Product (Same Length Vectors)\\n\";\n    tiny::Mat vectorA(3, 1);  // Create a 3x1 vector\n    tiny::Mat vectorB(3, 1);  // Create a 3x1 vector\n\n    // Initialize vectors\n    vectorA(0, 0) = 1.0f;\n    vectorA(1, 0) = 2.0f;\n    vectorA(2, 0) = 3.0f;\n\n    vectorB(0, 0) = 4.0f;\n    vectorB(1, 0) = 5.0f;\n    vectorB(2, 0) = 6.0f;\n\n    std::cout &lt;&lt; \"Vector A:\\n\";\n    vectorA.print_matrix(true);\n    std::cout &lt;&lt; \"Vector B:\\n\";\n    vectorB.print_matrix(true);\n\n    // Compute the dot product\n    float result = vectorA.dotprod(vectorA, vectorB);\n    std::cout &lt;&lt; \"Dot product of vectorA and vectorB: \" &lt;&lt; result &lt;&lt; std::endl;  // Expected result: 1*4 + 2*5 + 3*6 = 32\n\n    // Test 5.12.2: Dot Product with Dimension Mismatch (Different Length Vectors)\n    std::cout &lt;&lt; \"\\n[Test 5.12.2] Invalid Dot Product (Dimension Mismatch)\\n\";\n    tiny::Mat vectorC(2, 1);  // Create a 2x1 vector (different size)\n    vectorC(0, 0) = 1.0f;\n    vectorC(1, 0) = 2.0f;\n\n    std::cout &lt;&lt; \"Vector A (3x1):\\n\";\n    vectorA.print_matrix(true);\n    std::cout &lt;&lt; \"Vector C (2x1, different size):\\n\";\n    vectorC.print_matrix(true);\n\n    float invalidResult = vectorA.dotprod(vectorA, vectorC);  // Should print an error and return 0\n    std::cout &lt;&lt; \"Dot product (dimension mismatch): \" &lt;&lt; invalidResult &lt;&lt; std::endl;  // Expected: 0 and error message\n\n    // Test 5.12.3: Dot Product of Zero Vectors\n    std::cout &lt;&lt; \"\\n[Test 5.12.3] Dot Product of Zero Vectors\\n\";\n    tiny::Mat zeroVectorA(3, 1);  // Create a 3x1 zero vector\n    tiny::Mat zeroVectorB(3, 1);  // Create a 3x1 zero vector\n\n    // Initialize vectors\n    zeroVectorA(0, 0) = 0.0f;\n    zeroVectorA(1, 0) = 0.0f;\n    zeroVectorA(2, 0) = 0.0f;\n\n    zeroVectorB(0, 0) = 0.0f;\n    zeroVectorB(1, 0) = 0.0f;\n    zeroVectorB(2, 0) = 0.0f;\n\n    std::cout &lt;&lt; \"Zero Vector A:\\n\";\n    zeroVectorA.print_matrix(true);\n    std::cout &lt;&lt; \"Zero Vector B:\\n\";\n    zeroVectorB.print_matrix(true);\n\n    float zeroResult = zeroVectorA.dotprod(zeroVectorA, zeroVectorB);\n    std::cout &lt;&lt; \"Dot product of zero vectors: \" &lt;&lt; zeroResult &lt;&lt; std::endl;  // Expected: 0\n\n}\n\n// Group 5.13: Solve Linear System\nvoid test_solve()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.13: Solve Linear System Tests]\\n\";\n\n    // Test 5.13.1: Solving a simple 2x2 system\n    std::cout &lt;&lt; \"\\n[Test 5.13.1] Solving a Simple 2x2 System Ax = b\\n\";\n    tiny::Mat A(2, 2);\n    tiny::Mat b(2, 1);\n\n    A(0, 0) = 2; A(0, 1) = 1;\n    A(1, 0) = 1; A(1, 1) = 3;\n\n    b(0, 0) = 5;\n    b(1, 0) = 6;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b.print_matrix(true);\n\n    tiny::Mat solution = A.solve(A, b);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution.print_matrix(true);\n\n    // Test 5.13.2: Solving a 3x3 system\n    std::cout &lt;&lt; \"\\n[Test 5.13.2] Solving a 3x3 System Ax = b\\n\";\n    tiny::Mat A2(3, 3);\n    tiny::Mat b2(3, 1);\n\n    A2(0, 0) = 1; A2(0, 1) = 2; A2(0, 2) = 1;\n    A2(1, 0) = 2; A2(1, 1) = 0; A2(1, 2) = 3;\n    A2(2, 0) = 3; A2(2, 1) = 2; A2(2, 2) = 1;\n\n    b2(0, 0) = 9;\n    b2(1, 0) = 8;\n    b2(2, 0) = 7;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A2.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b2.print_matrix(true);\n\n    tiny::Mat solution2 = A2.solve(A2, b2);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution2.print_matrix(true);\n\n    // Test 5.13.3: Solving a system where one row is all zeros\n    std::cout &lt;&lt; \"\\n[Test 5.13.3] Solving a System Where One Row is All Zeros (Expect Failure or Infinite Solutions)\\n\";\n    tiny::Mat A3(3, 3);\n    tiny::Mat b3(3, 1);\n\n    A3(0, 0) = 1; A3(0, 1) = 2; A3(0, 2) = 3;\n    A3(1, 0) = 0; A3(1, 1) = 0; A3(1, 2) = 0; // Zero row\n    A3(2, 0) = 4; A3(2, 1) = 5; A3(2, 2) = 6;\n\n    b3(0, 0) = 9;\n    b3(1, 0) = 0; // Inconsistent, no solution should be possible\n    b3(2, 0) = 15;\n\n    std::cout &lt;&lt; \"Matrix A (has zero row):\\n\";\n    A3.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b3.print_matrix(true);\n\n    tiny::Mat solution3 = A3.solve(A3, b3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution3.print_matrix(true);\n\n    // Test 5.13.4: Solving a system with zero determinant (singular matrix)\n    std::cout &lt;&lt; \"\\n[Test 5.13.4] Solving a System with Zero Determinant (Singular Matrix)\\n\";\n    tiny::Mat A4(3, 3);\n    tiny::Mat b4(3, 1);\n\n    A4(0, 0) = 2; A4(0, 1) = 4; A4(0, 2) = 1;\n    A4(1, 0) = 1; A4(1, 1) = 2; A4(1, 2) = 3;\n    A4(2, 0) = 3; A4(2, 1) = 6; A4(2, 2) = 2; // The matrix is singular (row 2 = 2 * row 1)\n\n    b4(0, 0) = 5;\n    b4(1, 0) = 6;\n    b4(2, 0) = 7;\n\n    std::cout &lt;&lt; \"Matrix A (singular, determinant = 0):\\n\";\n    A4.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b4.print_matrix(true);\n\n    tiny::Mat solution4 = A4.solve(A4, b4);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution4.print_matrix(true); // Expect no solution or an error message\n\n    // Test 5.13.5: Solving a system with linearly dependent rows\n    std::cout &lt;&lt; \"\\n[Test 5.13.5] Solving a System with Linearly Dependent Rows (Expect Failure or Infinite Solutions)\\n\";\n    tiny::Mat A5(3, 3);\n    tiny::Mat b5(3, 1);\n\n    A5(0, 0) = 1; A5(0, 1) = 1; A5(0, 2) = 1;\n    A5(1, 0) = 2; A5(1, 1) = 2; A5(1, 2) = 2;\n    A5(2, 0) = 3; A5(2, 1) = 3; A5(2, 2) = 3; // All rows are linearly dependent\n\n    b5(0, 0) = 6;\n    b5(1, 0) = 12;\n    b5(2, 0) = 18;\n\n    std::cout &lt;&lt; \"Matrix A (all rows linearly dependent):\\n\";\n    A5.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b5.print_matrix(true);\n\n    tiny::Mat solution5 = A5.solve(A5, b5);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution5.print_matrix(true); // Expect an error message or infinite solutions\n\n    // Test 5.13.6: Solving a larger 4x4 system\n    std::cout &lt;&lt; \"\\n[Test 5.13.6] Solving a Larger 4x4 System Ax = b\\n\";\n    tiny::Mat A6(4, 4);\n    tiny::Mat b6(4, 1);\n\n    A6(0, 0) = 4; A6(0, 1) = 2; A6(0, 2) = 3; A6(0, 3) = 1;\n    A6(1, 0) = 2; A6(1, 1) = 5; A6(1, 2) = 1; A6(1, 3) = 2;\n    A6(2, 0) = 3; A6(2, 1) = 1; A6(2, 2) = 6; A6(2, 3) = 3;\n    A6(3, 0) = 1; A6(3, 1) = 2; A6(3, 2) = 3; A6(3, 3) = 4;\n\n    b6(0, 0) = 10;\n    b6(1, 0) = 12;\n    b6(2, 0) = 14;\n    b6(3, 0) = 16;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A6.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b6.print_matrix(true);\n\n    tiny::Mat solution6 = A6.solve(A6, b6);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution6.print_matrix(true); // Should print the solution vector\n\n}\n\n// Group 5.14: Band Solve\nvoid test_band_solve()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.14: Band Solve Tests]\\n\";\n\n    // Test 5.14.1: Simple 3x3 Band Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.14.1] Simple 3x3 Band Matrix\\n\";\n    tiny::Mat A1(3, 3);\n    tiny::Mat b1(3, 1);\n\n    // Define the matrix A and vector b for the system Ax = b\n    A1(0, 0) = 2; A1(0, 1) = 1; A1(0, 2) = 0;\n    A1(1, 0) = 1; A1(1, 1) = 3; A1(1, 2) = 2;\n    A1(2, 0) = 0; A1(2, 1) = 1; A1(2, 2) = 4;\n\n    b1(0, 0) = 5;\n    b1(1, 0) = 6;\n    b1(2, 0) = 7;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A1.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b1.print_matrix(true);\n\n    // Solve Ax = b using band_solve\n    tiny::Mat solution1 = A1.band_solve(A1, b1, 3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution1.print_matrix(true);\n\n    // Test 5.14.2: 4x4 Band Matrix with different right-hand side vector\n    std::cout &lt;&lt; \"\\n[Test 5.14.2] 4x4 Band Matrix\\n\";\n    tiny::Mat A2(4, 4);\n    tiny::Mat b2(4, 1);\n\n    // Define the matrix A and vector b\n    A2(0, 0) = 2; A2(0, 1) = 1; A2(0, 2) = 0; A2(0, 3) = 0;\n    A2(1, 0) = 1; A2(1, 1) = 3; A2(1, 2) = 2; A2(1, 3) = 0;\n    A2(2, 0) = 0; A2(2, 1) = 1; A2(2, 2) = 4; A2(2, 3) = 2;\n    A2(3, 0) = 0; A2(3, 1) = 0; A2(3, 2) = 1; A2(3, 3) = 5;\n\n    b2(0, 0) = 8;\n    b2(1, 0) = 9;\n    b2(2, 0) = 10;\n    b2(3, 0) = 11;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A2.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b2.print_matrix(true);\n\n    // Solve Ax = b using band_solve\n    tiny::Mat solution2 = A2.band_solve(A2, b2, 3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution2.print_matrix(true);\n\n    // Test 5.14.3: Incompatible dimensions (expect error)\n    std::cout &lt;&lt; \"\\n[Test 5.14.3] Incompatible Dimensions (Expect Error)\\n\";\n    tiny::Mat A3(3, 3);\n    tiny::Mat b3(2, 1);  // Incompatible dimension\n\n    A3(0, 0) = 1; A3(0, 1) = 2; A3(0, 2) = 3;\n    A3(1, 0) = 4; A3(1, 1) = 5; A3(1, 2) = 6;\n    A3(2, 0) = 7; A3(2, 1) = 8; A3(2, 2) = 9;\n\n    b3(0, 0) = 10;\n    b3(1, 0) = 11;\n\n    std::cout &lt;&lt; \"Matrix A (3x3):\\n\";\n    A3.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b (2x1, incompatible):\\n\";\n    b3.print_matrix(true);\n\n    // This should print an error because of incompatible dimensions\n    tiny::Mat solution3 = A3.band_solve(A3, b3, 3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution3.print_matrix(true);\n\n    // Test 5.14.4: Singular Matrix (Should fail)\n    std::cout &lt;&lt; \"\\n[Test 5.14.4] Singular Matrix (No Unique Solution)\\n\";\n    tiny::Mat A4(3, 3);\n    tiny::Mat b4(3, 1);\n\n    // Define a singular matrix (linearly dependent rows)\n    A4(0, 0) = 1; A4(0, 1) = 2; A4(0, 2) = 3;\n    A4(1, 0) = 2; A4(1, 1) = 4; A4(1, 2) = 6;\n    A4(2, 0) = 3; A4(2, 1) = 6; A4(2, 2) = 9;\n\n    b4(0, 0) = 10;\n    b4(1, 0) = 20;\n    b4(2, 0) = 30;\n\n    std::cout &lt;&lt; \"Matrix A (singular, linearly dependent rows):\\n\";\n    A4.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b4.print_matrix(true);\n\n    // This should print an error as the matrix is singular and does not have a unique solution\n    tiny::Mat solution4 = A4.band_solve(A4, b4, 3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution4.print_matrix(true);\n}\n\n// Group 5.15: Roots\nvoid test_roots()\n{\n    std::cout &lt;&lt; \"\\n[Group 5.15: Roots Tests]\\n\";\n\n    // Test 5.15.1: Simple 2x2 System\n    std::cout &lt;&lt; \"\\n[Test 5.15.1] Solving a Simple 2x2 System Ax = b\\n\";\n    tiny::Mat A1(2, 2);\n    tiny::Mat b1(2, 1);\n\n    // Define the matrix A and vector b for the system Ax = b\n    A1(0, 0) = 2; A1(0, 1) = 1;\n    A1(1, 0) = 1; A1(1, 1) = 3;\n\n    b1(0, 0) = 5;\n    b1(1, 0) = 6;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A1.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b1.print_matrix(true);\n\n    // Solve Ax = b using roots\n    tiny::Mat solution1 = A1.roots(A1, b1);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution1.print_matrix(true);\n\n    // Test 5.15.2: 3x3 System\n    std::cout &lt;&lt; \"\\n[Test 5.15.2] Solving a 3x3 System Ax = b\\n\";\n    tiny::Mat A2(3, 3);\n    tiny::Mat b2(3, 1);\n\n    A2(0, 0) = 1; A2(0, 1) = 2; A2(0, 2) = 1;\n    A2(1, 0) = 2; A2(1, 1) = 0; A2(1, 2) = 3;\n    A2(2, 0) = 3; A2(2, 1) = 2; A2(2, 2) = 1;\n\n    b2(0, 0) = 9;\n    b2(1, 0) = 8;\n    b2(2, 0) = 7;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    A2.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b2.print_matrix(true);\n\n    // Solve Ax = b using roots\n    tiny::Mat solution2 = A2.roots(A2, b2);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution2.print_matrix(true);\n\n    // Test 5.15.3: Singular Matrix\n    std::cout &lt;&lt; \"\\n[Test 5.15.3] Singular Matrix (No Unique Solution)\\n\";\n    tiny::Mat A3(2, 2);\n    tiny::Mat b3(2, 1);\n\n    // Define a singular matrix (linearly dependent rows)\n    A3(0, 0) = 1; A3(0, 1) = 2;\n    A3(1, 0) = 2; A3(1, 1) = 4;\n\n    b3(0, 0) = 5;\n    b3(1, 0) = 6;\n\n    std::cout &lt;&lt; \"Matrix A (singular, linearly dependent rows):\\n\";\n    A3.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b:\\n\";\n    b3.print_matrix(true);\n\n    // This should print an error as the matrix is singular and does not have a unique solution\n    tiny::Mat solution3 = A3.roots(A3, b3);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution3.print_matrix(true);\n\n    // Test 5.15.4: Incompatible Dimensions (Expect Error)\n    std::cout &lt;&lt; \"\\n[Test 5.15.4] Incompatible Dimensions (Expect Error)\\n\";\n    tiny::Mat A4(3, 3);\n    tiny::Mat b4(2, 1);  // Incompatible dimension\n\n    A4(0, 0) = 1; A4(0, 1) = 2; A4(0, 2) = 3;\n    A4(1, 0) = 4; A4(1, 1) = 5; A4(1, 2) = 6;\n    A4(2, 0) = 7; A4(2, 1) = 8; A4(2, 2) = 9;\n\n    b4(0, 0) = 10;\n    b4(1, 0) = 11;\n\n    std::cout &lt;&lt; \"Matrix A (3x3):\\n\";\n    A4.print_matrix(true);\n    std::cout &lt;&lt; \"Vector b (2x1, incompatible):\\n\";\n    b4.print_matrix(true);\n\n    // This should print an error because of incompatible dimensions\n    tiny::Mat solution4 = A4.roots(A4, b4);\n    std::cout &lt;&lt; \"Solution x:\\n\";\n    solution4.print_matrix(true);\n}\n\n// ============================================================================\n// Group 6: Linear System Solving - Core Application Tests\n// ============================================================================\n// Purpose: Test linear system solving - the core application of matrix library\n// Note: This group includes Gaussian elimination, solve methods, and dot product\n// Group 6.1: Gaussian Elimination (moved from Group 5.9)\n// Group 6.2: Solve Linear System (moved from Group 5.13)\n// Group 6.3: Dot Product (moved from Group 5.12)\n// Group 6.4: Band Solve (moved from Group 5.14)\n// Group 6.5: Roots (moved from Group 5.15)\n\n// ============================================================================\n// Group 10: Auxiliary Functions - Stream Operators Tests\n// ============================================================================\n// Purpose: Test I/O operations - auxiliary but important for debugging\nvoid test_stream_operators()\n{\n    std::cout &lt;&lt; \"\\n[Group 10: Auxiliary Functions - Stream Operators Tests]\\n\";\n\n    // Test 10.1: Test stream insertion operator (&lt;&lt;) for Mat\n    std::cout &lt;&lt; \"\\n[Test 10.1] Stream Insertion Operator (&lt;&lt;) for Mat\\n\";\n    tiny::Mat mat1(3, 3);\n    mat1(0, 0) = 1; mat1(0, 1) = 2; mat1(0, 2) = 3;\n    mat1(1, 0) = 4; mat1(1, 1) = 5; mat1(1, 2) = 6;\n    mat1(2, 0) = 7; mat1(2, 1) = 8; mat1(2, 2) = 9;\n\n    std::cout &lt;&lt; \"Matrix mat1:\\n\";\n    std::cout &lt;&lt; mat1 &lt;&lt; std::endl; // Use the &lt;&lt; operator to print mat1\n\n    // Test 10.2: Test stream insertion operator (&lt;&lt;) for Mat::ROI\n    std::cout &lt;&lt; \"\\n[Test 10.2] Stream Insertion Operator (&lt;&lt;) for Mat::ROI\\n\";\n    tiny::Mat::ROI roi(1, 2, 3, 4);\n    // ROI constructor: ROI(pos_x, pos_y, width, height)\n    // roi(1, 2, 3, 4) means: start at column 1, row 2, with width 3, height 4\n    std::cout &lt;&lt; \"ROI created: ROI(pos_x=1, pos_y=2, width=3, height=4)\\n\";\n    std::cout &lt;&lt; \"Expected output:\\n\";\n    std::cout &lt;&lt; \"  row start: 2 (pos_y)\\n\";\n    std::cout &lt;&lt; \"  col start: 1 (pos_x)\\n\";\n    std::cout &lt;&lt; \"  row count: 4 (height)\\n\";\n    std::cout &lt;&lt; \"  col count: 3 (width)\\n\";\n    std::cout &lt;&lt; \"\\nActual output:\\n\";\n    std::cout &lt;&lt; roi &lt;&lt; std::endl; // Use the &lt;&lt; operator to print roi\n\n    // Test 10.3: Test stream extraction operator (&gt;&gt;) for Mat\n    std::cout &lt;&lt; \"\\n[Test 10.3] Stream Extraction Operator (&gt;&gt;) for Mat\\n\";\n    tiny::Mat mat2(2, 2);\n    // Use istringstream to simulate input (for automated testing)\n    std::istringstream input1(\"10 20 30 40\");\n    std::cout &lt;&lt; \"Simulated input: \\\"10 20 30 40\\\"\\n\";\n    input1 &gt;&gt; mat2; // Use the &gt;&gt; operator to read from string stream\n    std::cout &lt;&lt; \"Matrix mat2 after input:\\n\";\n    std::cout &lt;&lt; mat2 &lt;&lt; std::endl; // Use the &lt;&lt; operator to print mat2\n    std::cout &lt;&lt; \"Expected: [10, 20; 30, 40]\\n\";\n\n    // Test 10.4: Test stream extraction operator (&gt;&gt;) for Mat (with different values)\n    std::cout &lt;&lt; \"\\n[Test 10.4] Stream Extraction Operator (&gt;&gt;) for Mat (2x3 matrix)\\n\";\n    tiny::Mat mat3(2, 3);\n    // Use istringstream to simulate input (for automated testing)\n    std::istringstream input2(\"1.5 2.5 3.5 4.5 5.5 6.5\");\n    std::cout &lt;&lt; \"Simulated input: \\\"1.5 2.5 3.5 4.5 5.5 6.5\\\"\\n\";\n    input2 &gt;&gt; mat3; // Use the &gt;&gt; operator to read from string stream\n    std::cout &lt;&lt; \"Matrix mat3 after input:\\n\";\n    std::cout &lt;&lt; mat3 &lt;&lt; std::endl; // Use the &lt;&lt; operator to print mat3\n    std::cout &lt;&lt; \"Expected: [1.5, 2.5, 3.5; 4.5, 5.5, 6.5]\\n\";\n}\n\n// ============================================================================\n// Group 11: Auxiliary Functions - Global Arithmetic Operators Tests\n// ============================================================================\n// Purpose: Test global operator overloads - syntactic sugar for convenience\nvoid test_matrix_operations()\n{\n    std::cout &lt;&lt; \"\\n[Group 11: Auxiliary Functions - Global Arithmetic Operators Tests]\\n\";\n\n    // Test 11.1: Matrix Addition (operator+)\n    std::cout &lt;&lt; \"\\n[Test 11.1] Matrix Addition (operator+)\\n\";\n    tiny::Mat matA(2, 2);\n    tiny::Mat matB(2, 2);\n\n    matA(0, 0) = 1; matA(0, 1) = 2;\n    matA(1, 0) = 3; matA(1, 1) = 4;\n\n    matB(0, 0) = 5; matB(0, 1) = 6;\n    matB(1, 0) = 7; matB(1, 1) = 8;\n\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B:\\n\";\n    matB.print_matrix(true);\n\n    tiny::Mat resultAdd = matA + matB;\n    std::cout &lt;&lt; \"matA + matB:\\n\";\n    std::cout &lt;&lt; resultAdd &lt;&lt; std::endl;  // Expected: [6, 8], [10, 12]\n\n    // Test 11.2: Matrix Addition with Constant (operator+)\n    std::cout &lt;&lt; \"\\n[Test 11.2] Matrix Addition with Constant (operator+)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Constant: 5.0\\n\";\n    tiny::Mat resultAddConst = matA + 5.0f;\n    std::cout &lt;&lt; \"matA + 5.0f:\\n\";\n    std::cout &lt;&lt; resultAddConst &lt;&lt; std::endl;  // Expected: [6, 7], [8, 9]\n\n    // Test 11.3: Matrix Subtraction (operator-)\n    std::cout &lt;&lt; \"\\n[Test 11.3] Matrix Subtraction (operator-)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B:\\n\";\n    matB.print_matrix(true);\n    tiny::Mat resultSub = matA - matB;\n    std::cout &lt;&lt; \"matA - matB:\\n\";\n    std::cout &lt;&lt; resultSub &lt;&lt; std::endl;  // Expected: [-4, -4], [-4, -4]\n\n    // Test 11.4: Matrix Subtraction with Constant (operator-)\n    std::cout &lt;&lt; \"\\n[Test 11.4] Matrix Subtraction with Constant (operator-)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Constant: 2.0\\n\";\n    tiny::Mat resultSubConst = matA - 2.0f;\n    std::cout &lt;&lt; \"matA - 2.0f:\\n\";\n    std::cout &lt;&lt; resultSubConst &lt;&lt; std::endl;  // Expected: [-1, 0], [1, 2]\n\n    // Test 11.5: Matrix Multiplication (operator*)\n    std::cout &lt;&lt; \"\\n[Test 11.5] Matrix Multiplication (operator*)\\n\";\n    tiny::Mat matC(2, 3);\n    tiny::Mat matD(3, 2);\n\n    matC(0, 0) = 1; matC(0, 1) = 2; matC(0, 2) = 3;\n    matC(1, 0) = 4; matC(1, 1) = 5; matC(1, 2) = 6;\n\n    matD(0, 0) = 7; matD(0, 1) = 8;\n    matD(1, 0) = 9; matD(1, 1) = 10;\n    matD(2, 0) = 11; matD(2, 1) = 12;\n\n    std::cout &lt;&lt; \"Matrix C (2x3):\\n\";\n    matC.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix D (3x2):\\n\";\n    matD.print_matrix(true);\n\n    tiny::Mat resultMul = matC * matD;\n    std::cout &lt;&lt; \"matC * matD:\\n\";\n    std::cout &lt;&lt; resultMul &lt;&lt; std::endl;  // Expected: [58, 64], [139, 154]\n\n    // Test 11.6: Matrix Multiplication with Constant (operator*)\n    std::cout &lt;&lt; \"\\n[Test 11.6] Matrix Multiplication with Constant (operator*)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Constant: 2.0\\n\";\n    tiny::Mat resultMulConst = matA * 2.0f;\n    std::cout &lt;&lt; \"matA * 2.0f:\\n\";\n    std::cout &lt;&lt; resultMulConst &lt;&lt; std::endl;  // Expected: [2, 4], [6, 8]\n\n    // Test 11.7: Matrix Division (operator/)\n    std::cout &lt;&lt; \"\\n[Test 11.7] Matrix Division (operator/)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Constant: 2.0\\n\";\n    tiny::Mat resultDiv = matA / 2.0f;\n    std::cout &lt;&lt; \"matA / 2.0f:\\n\";\n    std::cout &lt;&lt; resultDiv &lt;&lt; std::endl;  // Expected: [0.5, 1], [1.5, 2]\n\n    // Test 11.8: Matrix Division Element-wise (operator/)\n    std::cout &lt;&lt; \"\\n[Test 11.8] Matrix Division Element-wise (operator/)\\n\";\n    std::cout &lt;&lt; \"Matrix A:\\n\";\n    matA.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix B:\\n\";\n    matB.print_matrix(true);\n    tiny::Mat resultDivElem = matA / matB;\n    std::cout &lt;&lt; \"matA / matB:\\n\";\n    std::cout &lt;&lt; resultDivElem &lt;&lt; std::endl;  // Expected: [0.2, 0.333], [0.428, 0.5]\n\n    // Test 11.9: Matrix Comparison (operator==)\n    std::cout &lt;&lt; \"\\n[Test 11.9] Matrix Comparison (operator==)\\n\";\n    tiny::Mat matE(2, 2);\n    matE(0, 0) = 1; matE(0, 1) = 2;\n    matE(1, 0) = 3; matE(1, 1) = 4;\n\n    tiny::Mat matF(2, 2);\n    matF(0, 0) = 1; matF(0, 1) = 2;\n    matF(1, 0) = 3; matF(1, 1) = 4;\n\n    std::cout &lt;&lt; \"Matrix E:\\n\";\n    matE.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix F:\\n\";\n    matF.print_matrix(true);\n\n    bool isEqual = (matE == matF);\n    std::cout &lt;&lt; \"matE == matF: \" &lt;&lt; (isEqual ? \"True\" : \"False\") &lt;&lt; std::endl;  // Expected: True\n\n    matF(0, 0) = 5;  // Modify matF\n    std::cout &lt;&lt; \"\\nAfter modifying matF(0,0) = 5:\\n\";\n    std::cout &lt;&lt; \"Matrix E:\\n\";\n    matE.print_matrix(true);\n    std::cout &lt;&lt; \"Matrix F:\\n\";\n    matF.print_matrix(true);\n    isEqual = (matE == matF);\n    std::cout &lt;&lt; \"matE == matF after modification: \" &lt;&lt; (isEqual ? \"True\" : \"False\") &lt;&lt; std::endl;  // Expected: False\n}\n\n// ============================================================================\n// Group 12: Quality Assurance - Boundary Conditions and Error Handling Tests\n// ============================================================================\n// Purpose: Test error handling and edge cases - ensure robustness\nvoid test_boundary_conditions()\n{\n    std::cout &lt;&lt; \"\\n[Group 12: Quality Assurance - Boundary Conditions and Error Handling Tests]\\n\";\n\n    // Test 12.1: Null pointer handling in print functions\n    std::cout &lt;&lt; \"\\n[Test 12.1] Null Pointer Handling in print_matrix\\n\";\n    tiny::Mat null_mat;\n    null_mat.data = nullptr;  // Simulate null pointer\n    null_mat.print_matrix(true);  // Should handle gracefully\n\n    // Test 12.2: Null pointer handling in operator&lt;&lt;\n    std::cout &lt;&lt; \"\\n[Test 12.2] Null Pointer Handling in operator&lt;&lt;\\n\";\n    tiny::Mat null_mat2;\n    null_mat2.data = nullptr;\n    std::cout &lt;&lt; null_mat2 &lt;&lt; std::endl;  // Should handle gracefully\n\n    // Test 12.3: Invalid block parameters\n    std::cout &lt;&lt; \"\\n[Test 12.3] Invalid Block Parameters\\n\";\n    tiny::Mat mat(3, 3);\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat(i, j) = i * 3 + j + 1;\n\n    // Negative start position\n    tiny::Mat block1 = mat.block(-1, 0, 2, 2);\n    std::cout &lt;&lt; \"block(-1, 0, 2, 2): \" &lt;&lt; (block1.data == nullptr ? \"Empty (correct)\" : \"Error\") &lt;&lt; \"\\n\";\n\n    // Block exceeds boundaries\n    tiny::Mat block2 = mat.block(2, 2, 2, 2);\n    std::cout &lt;&lt; \"block(2, 2, 2, 2) on 3x3 matrix: \" &lt;&lt; (block2.data == nullptr ? \"Empty (correct)\" : \"Error\") &lt;&lt; \"\\n\";\n\n    // Zero or negative block size\n    tiny::Mat block3 = mat.block(0, 0, 0, 2);\n    std::cout &lt;&lt; \"block(0, 0, 0, 2): \" &lt;&lt; (block3.data == nullptr ? \"Empty (correct)\" : \"Error\") &lt;&lt; \"\\n\";\n\n    // Test 12.4: Invalid swap_rows parameters\n    std::cout &lt;&lt; \"\\n[Test 12.4] Invalid swap_rows Parameters\\n\";\n    tiny::Mat mat2(3, 3);\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat2(i, j) = i * 3 + j + 1;\n\n    std::cout &lt;&lt; \"Before invalid swap_rows:\\n\";\n    mat2.print_matrix(true);\n\n    // Negative index\n    mat2.swap_rows(-1, 1);\n    std::cout &lt;&lt; \"After swap_rows(-1, 1):\\n\";\n    mat2.print_matrix(true);\n\n    // Index out of range\n    mat2.swap_rows(0, 5);\n    std::cout &lt;&lt; \"After swap_rows(0, 5):\\n\";\n    mat2.print_matrix(true);\n\n    // Test 12.5: Invalid swap_cols parameters\n    std::cout &lt;&lt; \"\\n[Test 12.5] Invalid swap_cols Parameters\\n\";\n    tiny::Mat mat2_cols(3, 3);\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 3; ++j)\n            mat2_cols(i, j) = i * 3 + j + 1;\n\n    std::cout &lt;&lt; \"Before invalid swap_cols:\\n\";\n    mat2_cols.print_matrix(true);\n\n    // Negative index\n    mat2_cols.swap_cols(-1, 1);\n    std::cout &lt;&lt; \"After swap_cols(-1, 1):\\n\";\n    mat2_cols.print_matrix(true);\n\n    // Index out of range\n    mat2_cols.swap_cols(0, 5);\n    std::cout &lt;&lt; \"After swap_cols(0, 5):\\n\";\n    mat2_cols.print_matrix(true);\n\n    // Test 12.6: Division by zero\n    std::cout &lt;&lt; \"\\n[Test 12.6] Division by Zero\\n\";\n    tiny::Mat mat3(2, 2);\n    mat3(0, 0) = 1; mat3(0, 1) = 2;\n    mat3(1, 0) = 3; mat3(1, 1) = 4;\n\n    tiny::Mat result = mat3 / 0.0f;\n    std::cout &lt;&lt; \"mat3 / 0.0f: \" &lt;&lt; (result.data == nullptr ? \"Empty (correct)\" : \"Error\") &lt;&lt; \"\\n\";\n\n    // Test 12.7: Matrix division with zero elements\n    std::cout &lt;&lt; \"\\n[Test 12.7] Matrix Division with Zero Elements\\n\";\n    tiny::Mat mat4(2, 2);\n    mat4(0, 0) = 1; mat4(0, 1) = 2;\n    mat4(1, 0) = 3; mat4(1, 1) = 4;\n\n    tiny::Mat divisor(2, 2);\n    divisor(0, 0) = 1; divisor(0, 1) = 0;  // Contains zero\n    divisor(1, 0) = 3; divisor(1, 1) = 4;\n\n    mat4 /= divisor;\n    std::cout &lt;&lt; \"mat4 /= divisor (with zero):\\n\";\n    mat4.print_matrix(true);\n\n    // Test 12.8: Empty matrix operations\n    std::cout &lt;&lt; \"\\n[Test 12.8] Empty Matrix Operations\\n\";\n    tiny::Mat empty1, empty2;\n    tiny::Mat empty_sum = empty1 + empty2;\n    std::cout &lt;&lt; \"Empty matrix addition: \" &lt;&lt; (empty_sum.data == nullptr ? \"Empty (correct)\" : \"Error\") &lt;&lt; \"\\n\";\n}\n\n// ============================================================================\n// Group 13: Quality Assurance - Performance Benchmarks Tests\n// ============================================================================\n// Purpose: Test performance characteristics - critical for real-time applications\nvoid test_performance_benchmarks()\n{\n    std::cout &lt;&lt; \"\\n[Group 13: Quality Assurance - Performance Benchmarks Tests]\\n\";\n\n    // Ensure current task is added to watchdog before starting performance tests\n    #if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    ensure_task_wdt_added();\n    #endif\n\n    // Test 13.1: Matrix Addition Performance (reduced size to prevent timeout)\n    std::cout &lt;&lt; \"\\n[Test 13.1] Matrix Addition Performance\\n\";\n    tiny::Mat A(50, 50);  // Reduced from 100x100 to 50x50\n    tiny::Mat B(50, 50);\n    for (int i = 0; i &lt; 50; ++i)\n    {\n        for (int j = 0; j &lt; 50; ++j)\n        {\n            A(i, j) = static_cast&lt;float&gt;(i * 50 + j);\n            B(i, j) = static_cast&lt;float&gt;(i * 50 + j + 1);\n        }\n    }\n    TIME_REPEATED_OPERATION(tiny::Mat C = A + B;, PERFORMANCE_TEST_ITERATIONS, \"50x50 Matrix Addition\");\n\n    // Test 13.2: Matrix Multiplication Performance (reduced size)\n    std::cout &lt;&lt; \"\\n[Test 13.2] Matrix Multiplication Performance\\n\";\n    tiny::Mat D(30, 30);  // Reduced from 50x50 to 30x30\n    tiny::Mat E(30, 30);\n    for (int i = 0; i &lt; 30; ++i)\n    {\n        for (int j = 0; j &lt; 30; ++j)\n        {\n            D(i, j) = static_cast&lt;float&gt;(i * 30 + j);\n            E(i, j) = static_cast&lt;float&gt;(i * 30 + j + 1);\n        }\n    }\n    TIME_REPEATED_OPERATION(tiny::Mat F = D * E;, PERFORMANCE_TEST_ITERATIONS, \"30x30 Matrix Multiplication\");\n\n    // Test 13.3: Matrix Transpose Performance (reduced size)\n    std::cout &lt;&lt; \"\\n[Test 13.3] Matrix Transpose Performance\\n\";\n    tiny::Mat G(50, 30);  // Reduced from 100x50 to 50x30\n    for (int i = 0; i &lt; 50; ++i)\n        for (int j = 0; j &lt; 30; ++j)\n            G(i, j) = static_cast&lt;float&gt;(i * 30 + j);\n    TIME_REPEATED_OPERATION(tiny::Mat H = G.transpose();, PERFORMANCE_TEST_ITERATIONS, \"50x30 Matrix Transpose\");\n\n    // Test 13.4: Determinant Performance Comparison\n    // Note: Determinant calculation now has multiple methods:\n    //   - Laplace expansion: O(n!) - for small matrices (n &lt;= 4)\n    //   - LU decomposition: O(n\u00b3) - for large matrices (n &gt; 4, auto-selected)\n    //   - Gaussian elimination: O(n\u00b3) - alternative for large matrices\n    std::cout &lt;&lt; \"\\n[Test 13.4] Determinant Calculation Performance Comparison\\n\";\n\n    // Test 13.4.1: Small Matrix (4x4) - Laplace Expansion\n        std::cout &lt;&lt; \"\\n[Test 13.4.1] Small Matrix (4x4) - Laplace Expansion\\n\";\n    tiny::Mat I4(4, 4);\n    for (int i = 0; i &lt; 4; ++i)\n        for (int j = 0; j &lt; 4; ++j)\n            I4(i, j) = static_cast&lt;float&gt;(i * 4 + j + 1);\n\n    feed_watchdog();\n    TinyTimeMark_t det4_t0 = tiny_get_running_time();\n    for (int i = 0; i &lt; PERFORMANCE_TEST_ITERATIONS_HEAVY; ++i)\n    {\n        feed_watchdog();\n        float det = I4.determinant_laplace();\n        (void)det;\n        feed_watchdog();\n    }\n    TinyTimeMark_t det4_t1 = tiny_get_running_time();\n    double det4_dt_total_us = (double)(det4_t1 - det4_t0);\n    double det4_dt_avg_us = det4_dt_total_us / PERFORMANCE_TEST_ITERATIONS_HEAVY;\n    std::cout &lt;&lt; \"[Performance] 4x4 Determinant (Laplace, \" &lt;&lt; PERFORMANCE_TEST_ITERATIONS_HEAVY &lt;&lt; \" iterations): \"\n              &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; det4_dt_total_us &lt;&lt; \" us total, \"\n              &lt;&lt; det4_dt_avg_us &lt;&lt; \" us avg\\n\";\n\n    // Test 13.4.2: Large Matrix (8x8) - LU Decomposition\n        std::cout &lt;&lt; \"\\n[Test 13.4.2] Large Matrix (8x8) - LU Decomposition\\n\";\n    tiny::Mat I8(8, 8);\n    for (int i = 0; i &lt; 8; ++i)\n        for (int j = 0; j &lt; 8; ++j)\n            I8(i, j) = static_cast&lt;float&gt;((i + 1) * (j + 1));\n\n    feed_watchdog();\n    TinyTimeMark_t det8_lu_t0 = tiny_get_running_time();\n    for (int i = 0; i &lt; PERFORMANCE_TEST_ITERATIONS_HEAVY; ++i)\n    {\n        feed_watchdog();\n        float det = I8.determinant_lu();\n        (void)det;\n        feed_watchdog();\n    }\n    TinyTimeMark_t det8_lu_t1 = tiny_get_running_time();\n    double det8_lu_dt_total_us = (double)(det8_lu_t1 - det8_lu_t0);\n    double det8_lu_dt_avg_us = det8_lu_dt_total_us / PERFORMANCE_TEST_ITERATIONS_HEAVY;\n    std::cout &lt;&lt; \"[Performance] 8x8 Determinant (LU, \" &lt;&lt; PERFORMANCE_TEST_ITERATIONS_HEAVY &lt;&lt; \" iterations): \"\n              &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; det8_lu_dt_total_us &lt;&lt; \" us total, \"\n              &lt;&lt; det8_lu_dt_avg_us &lt;&lt; \" us avg\\n\";\n\n    // Test 13.4.3: Large Matrix (8x8) - Gaussian Elimination\n        std::cout &lt;&lt; \"\\n[Test 13.4.3] Large Matrix (8x8) - Gaussian Elimination\\n\";\n    feed_watchdog();\n    TinyTimeMark_t det8_gauss_t0 = tiny_get_running_time();\n    for (int i = 0; i &lt; PERFORMANCE_TEST_ITERATIONS_HEAVY; ++i)\n    {\n        feed_watchdog();\n        float det = I8.determinant_gaussian();\n        (void)det;\n        feed_watchdog();\n    }\n    TinyTimeMark_t det8_gauss_t1 = tiny_get_running_time();\n    double det8_gauss_dt_total_us = (double)(det8_gauss_t1 - det8_gauss_t0);\n    double det8_gauss_dt_avg_us = det8_gauss_dt_total_us / PERFORMANCE_TEST_ITERATIONS_HEAVY;\n    std::cout &lt;&lt; \"[Performance] 8x8 Determinant (Gaussian, \" &lt;&lt; PERFORMANCE_TEST_ITERATIONS_HEAVY &lt;&lt; \" iterations): \"\n              &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; det8_gauss_dt_total_us &lt;&lt; \" us total, \"\n              &lt;&lt; det8_gauss_dt_avg_us &lt;&lt; \" us avg\\n\";\n\n    // Test 13.4.4: Auto-select Method (8x8) - Should use LU\n        std::cout &lt;&lt; \"\\n[Test 13.4.4] Large Matrix (8x8) - Auto-select Method\\n\";\n    feed_watchdog();\n    TinyTimeMark_t det8_auto_t0 = tiny_get_running_time();\n    for (int i = 0; i &lt; PERFORMANCE_TEST_ITERATIONS_HEAVY; ++i)\n    {\n        feed_watchdog();\n        float det = I8.determinant();  // Auto-selects LU for n &gt; 4\n        (void)det;\n        feed_watchdog();\n    }\n    TinyTimeMark_t det8_auto_t1 = tiny_get_running_time();\n    double det8_auto_dt_total_us = (double)(det8_auto_t1 - det8_auto_t0);\n    double det8_auto_dt_avg_us = det8_auto_dt_total_us / PERFORMANCE_TEST_ITERATIONS_HEAVY;\n    std::cout &lt;&lt; \"[Performance] 8x8 Determinant (auto-select, \" &lt;&lt; PERFORMANCE_TEST_ITERATIONS_HEAVY &lt;&lt; \" iterations): \"\n              &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; det8_auto_dt_total_us &lt;&lt; \" us total, \"\n              &lt;&lt; det8_auto_dt_avg_us &lt;&lt; \" us avg\\n\";\n\n    std::cout &lt;&lt; \"\\n[Note] Performance Summary:\\n\";\n    std::cout &lt;&lt; \"  - Laplace expansion (O(n!)): Suitable only for small matrices (n &lt;= 4)\\n\";\n    std::cout &lt;&lt; \"  - LU decomposition (O(n\u00b3)): Efficient for large matrices, auto-selected for n &gt; 4\\n\";\n    std::cout &lt;&lt; \"  - Gaussian elimination (O(n\u00b3)): Alternative efficient method for large matrices\\n\";\n    std::cout &lt;&lt; \"  - Auto-select: Automatically chooses the best method based on matrix size\\n\";\n\n    // Test 13.5: Matrix Copy Performance (with padding, reduced size)\n    std::cout &lt;&lt; \"\\n[Test 13.5] Matrix Copy with Padding Performance\\n\";\n    float data[80] = {0};  // Reduced from 150 to 80\n    for (int i = 0; i &lt; 80; ++i) data[i] = static_cast&lt;float&gt;(i);\n    tiny::Mat J(data, 8, 8, 10);  // Reduced from 10x10 stride 15 to 8x8 stride 10\n    TIME_REPEATED_OPERATION(tiny::Mat K = J.copy_roi(0, 0, 8, 8);, PERFORMANCE_TEST_ITERATIONS, \"8x8 Copy ROI (with padding)\");\n\n    // Test 13.6: Element Access Performance (reduced size)\n    std::cout &lt;&lt; \"\\n[Test 13.6] Element Access Performance\\n\";\n    tiny::Mat L(50, 50);  // Reduced from 100x100 to 50x50\n    for (int i = 0; i &lt; 50; ++i)\n        for (int j = 0; j &lt; 50; ++j)\n            L(i, j) = static_cast&lt;float&gt;(i * 50 + j);\n\n    // Test 6: Element Access Performance (custom implementation for multi-line operation)\n    float sum = 0.0f;\n    std::cout &lt;&lt; \"[Performance] Computing element access (warmup)...\\n\";\n    feed_watchdog();  // Feed watchdog before starting\n    for (int w = 0; w &lt; PERFORMANCE_TEST_WARMUP; ++w)\n    {\n        feed_watchdog();  // Feed watchdog before each warmup\n        sum = 0.0f;\n        for (int i = 0; i &lt; 50; ++i)\n            for (int j = 0; j &lt; 50; ++j)\n                sum += L(i, j);\n        feed_watchdog();  // Feed watchdog after each warmup\n    }\n\n    TinyTimeMark_t elem_t0 = tiny_get_running_time();\n    for (int i = 0; i &lt; PERFORMANCE_TEST_ITERATIONS; ++i)\n    {\n        if (i % 20 == 0) feed_watchdog();  // Feed watchdog every 20 iterations (element access is fast)\n        sum = 0.0f;\n        for (int row = 0; row &lt; 50; ++row)\n            for (int col = 0; col &lt; 50; ++col)\n                sum += L(row, col);\n    }\n    feed_watchdog();  // Final feed after loop\n    TinyTimeMark_t elem_t1 = tiny_get_running_time();\n    double elem_dt_total_us = (double)(elem_t1 - elem_t0);\n    double dt_avg_us = elem_dt_total_us / PERFORMANCE_TEST_ITERATIONS;\n    std::cout &lt;&lt; \"[Performance] 50x50 Element Access (all elements) (\" &lt;&lt; PERFORMANCE_TEST_ITERATIONS &lt;&lt; \" iterations): \"\n              &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; elem_dt_total_us &lt;&lt; \" us total, \"\n              &lt;&lt; dt_avg_us &lt;&lt; \" us avg\\n\";\n}\n\n// ============================================================================\n// Group 14: Quality Assurance - Memory Layout Tests (Padding and Stride)\n// ============================================================================\n// Purpose: Test memory layout handling - important for performance and compatibility\nvoid test_memory_layout()\n{\n    std::cout &lt;&lt; \"\\n[Group 14: Quality Assurance - Memory Layout Tests (Padding and Stride)]\\n\";\n\n    // Test 14.1: Contiguous memory (pad=0, step=1)\n    std::cout &lt;&lt; \"\\n[Test 14.1] Contiguous Memory (no padding)\\n\";\n    tiny::Mat mat1(3, 4);\n    for (int i = 0; i &lt; 3; ++i)\n        for (int j = 0; j &lt; 4; ++j)\n            mat1(i, j) = static_cast&lt;float&gt;(i * 4 + j);\n    std::cout &lt;&lt; \"Matrix 3x4 (stride=4, pad=0):\\n\";\n    mat1.print_info();\n    mat1.print_matrix(true);\n\n    // Test 14.2: Padded memory (stride &gt; col)\n    std::cout &lt;&lt; \"\\n[Test 14.2] Padded Memory (stride &gt; col)\\n\";\n    float data[15] = {0, 1, 2, 3, 0, 4, 5, 6, 7, 0, 8, 9, 10, 11, 0};\n    tiny::Mat mat2(data, 3, 4, 5);\n    std::cout &lt;&lt; \"Matrix 3x4 (stride=5, pad=1):\\n\";\n    mat2.print_info();\n    mat2.print_matrix(true);\n\n    // Test 14.3: Operations with padded matrices\n    std::cout &lt;&lt; \"\\n[Test 14.3] Addition with Padded Matrices\\n\";\n    float data1[15] = {1, 2, 3, 4, 0, 5, 6, 7, 8, 0, 9, 10, 11, 12, 0};\n    float data2[15] = {10, 20, 30, 40, 0, 50, 60, 70, 80, 0, 90, 100, 110, 120, 0};\n    tiny::Mat mat3(data1, 3, 4, 5);\n    tiny::Mat mat4(data2, 3, 4, 5);\n    tiny::Mat mat5 = mat3 + mat4;\n    std::cout &lt;&lt; \"Result of padded matrix addition:\\n\";\n    mat5.print_info();\n    mat5.print_matrix(true);\n\n    // Test 14.4: ROI operations with padded matrices\n    std::cout &lt;&lt; \"\\n[Test 14.4] ROI Operations with Padded Matrices\\n\";\n    tiny::Mat roi = mat2.view_roi(1, 1, 2, 2);\n    std::cout &lt;&lt; \"ROI (1,1,2,2) from padded matrix:\\n\";\n    roi.print_info();\n    roi.print_matrix(true);\n\n    // Test 14.5: Copy operations preserve stride\n    std::cout &lt;&lt; \"\\n[Test 14.5] Copy Operations Preserve Stride\\n\";\n    tiny::Mat copied = mat2.copy_roi(0, 0, 3, 4);\n    std::cout &lt;&lt; \"Copied matrix (should have stride=4, no padding):\\n\";\n    copied.print_info();\n    copied.print_matrix(true);\n}\n\n// ============================================================================\n// Group 7: Advanced Linear Algebra - Matrix Decomposition Tests\n// ============================================================================\n// Purpose: Test matrix decompositions (LU, Cholesky, QR, SVD) - foundation for \n//          stable linear system solving and least squares problems\nvoid test_matrix_decomposition()\n{\n    std::cout &lt;&lt; \"\\n[Group 7: Advanced Linear Algebra - Matrix Decomposition Tests]\\n\";\n\n    // Test 7.1: is_positive_definite() - Basic functionality\n    std::cout &lt;&lt; \"\\n[Test 7.1] is_positive_definite() - Basic Functionality\\n\";\n\n    // Test 7.1.1: Positive definite matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.1.1] Positive Definite 3x3 Matrix\\n\";\n        tiny::Mat pd_mat(3, 3);\n        pd_mat(0, 0) = 4.0f; pd_mat(0, 1) = 1.0f; pd_mat(0, 2) = 0.0f;\n        pd_mat(1, 0) = 1.0f; pd_mat(1, 1) = 3.0f; pd_mat(1, 2) = 0.0f;\n        pd_mat(2, 0) = 0.0f; pd_mat(2, 1) = 0.0f; pd_mat(2, 2) = 2.0f;\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        pd_mat.print_matrix(true);\n\n        bool is_pd = pd_mat.is_positive_definite(1e-6f);\n        std::cout &lt;&lt; \"Is positive definite: \" &lt;&lt; (is_pd ? \"True\" : \"False\") \n                  &lt;&lt; \" (Expected: True) \" &lt;&lt; (is_pd ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.1.2: Non-positive definite matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.1.2] Non-Positive Definite Matrix\\n\";\n        tiny::Mat non_pd(2, 2);\n        non_pd(0, 0) = 1.0f; non_pd(0, 1) = 2.0f;\n        non_pd(1, 0) = 2.0f; non_pd(1, 1) = 1.0f;  // Has negative eigenvalue\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        non_pd.print_matrix(true);\n\n        bool is_pd = non_pd.is_positive_definite(1e-6f);\n        std::cout &lt;&lt; \"Is positive definite: \" &lt;&lt; (is_pd ? \"True\" : \"False\") \n                  &lt;&lt; \" (Expected: False) \" &lt;&lt; (!is_pd ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.2: LU Decomposition\n    std::cout &lt;&lt; \"\\n[Test 7.2] LU Decomposition\\n\";\n\n    // Test 7.2.1: Simple 3x3 matrix with pivoting\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.2.1] 3x3 Matrix - LU Decomposition with Pivoting\\n\";\n        tiny::Mat A(3, 3);\n        A(0, 0) = 2.0f; A(0, 1) = 1.0f; A(0, 2) = 1.0f;\n        A(1, 0) = 4.0f; A(1, 1) = 3.0f; A(1, 2) = 3.0f;\n        A(2, 0) = 2.0f; A(2, 1) = 1.0f; A(2, 2) = 2.0f;\n        std::cout &lt;&lt; \"Matrix A:\\n\";\n        A.print_matrix(true);\n\n        tiny::Mat::LUDecomposition lu = A.lu_decompose(true);\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (lu.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (lu.status == TINY_OK)\n        {\n            std::cout &lt;&lt; \"L matrix (lower triangular):\\n\";\n            lu.L.print_matrix(true);\n            std::cout &lt;&lt; \"U matrix (upper triangular):\\n\";\n            lu.U.print_matrix(true);\n            if (lu.pivoted)\n            {\n                std::cout &lt;&lt; \"P matrix (permutation):\\n\";\n                lu.P.print_matrix(true);\n            }\n\n            // Verify: P * A = L * U\n            tiny::Mat PA = lu.P * A;\n            tiny::Mat LU = lu.L * lu.U;\n            std::cout &lt;&lt; \"\\n[Verification] P * A should equal L * U\\n\";\n            float diff = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    diff += fabsf(PA(i, j) - LU(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"Total difference: \" &lt;&lt; diff &lt;&lt; (diff &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n        }\n    }\n\n    // Test 7.2.2: Solve using LU decomposition\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.2.2] Solve Linear System using LU Decomposition\\n\";\n        tiny::Mat A(3, 3);\n        A(0, 0) = 2.0f; A(0, 1) = 1.0f; A(0, 2) = 1.0f;\n        A(1, 0) = 4.0f; A(1, 1) = 3.0f; A(1, 2) = 3.0f;\n        A(2, 0) = 2.0f; A(2, 1) = 1.0f; A(2, 2) = 2.0f;\n        tiny::Mat b(3, 1);\n        b(0, 0) = 1.0f;\n        b(1, 0) = 2.0f;\n        b(2, 0) = 3.0f;\n\n        std::cout &lt;&lt; \"System: A * x = b\\n\";\n        std::cout &lt;&lt; \"A:\\n\";\n        A.print_matrix(true);\n        std::cout &lt;&lt; \"b:\\n\";\n        b.print_matrix(true);\n\n        tiny::Mat::LUDecomposition lu = A.lu_decompose(true);\n        tiny::Mat x = tiny::Mat::solve_lu(lu, b);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Solution x:\\n\";\n        x.print_matrix(true);\n\n        // Verify: A * x = b\n        tiny::Mat Ax = A * x;\n        float error = 0.0f;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            error += fabsf(Ax(i, 0) - b(i, 0));\n        }\n        std::cout &lt;&lt; \"Verification error: \" &lt;&lt; error &lt;&lt; (error &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.3: Cholesky Decomposition\n    std::cout &lt;&lt; \"\\n[Test 7.3] Cholesky Decomposition\\n\";\n\n    // Test 7.3.1: Symmetric positive definite matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.3.1] SPD Matrix - Cholesky Decomposition\\n\";\n        tiny::Mat spd(3, 3);\n        spd(0, 0) = 4.0f; spd(0, 1) = 2.0f; spd(0, 2) = 0.0f;\n        spd(1, 0) = 2.0f; spd(1, 1) = 5.0f; spd(1, 2) = 1.0f;\n        spd(2, 0) = 0.0f; spd(2, 1) = 1.0f; spd(2, 2) = 3.0f;\n        std::cout &lt;&lt; \"Matrix A (SPD):\\n\";\n        spd.print_matrix(true);\n\n        tiny::Mat::CholeskyDecomposition chol = spd.cholesky_decompose();\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (chol.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (chol.status == TINY_OK)\n        {\n            std::cout &lt;&lt; \"L matrix (lower triangular):\\n\";\n            chol.L.print_matrix(true);\n\n            // Verify: A = L * L^T\n            tiny::Mat LLT = chol.L * chol.L.transpose();\n            std::cout &lt;&lt; \"\\n[Verification] L * L^T should equal A\\n\";\n            float diff = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    diff += fabsf(LLT(i, j) - spd(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"Total difference: \" &lt;&lt; diff &lt;&lt; (diff &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n        }\n    }\n\n    // Test 7.3.2: Solve using Cholesky decomposition\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.3.2] Solve Linear System using Cholesky Decomposition\\n\";\n        tiny::Mat A(3, 3);\n        A(0, 0) = 4.0f; A(0, 1) = 2.0f; A(0, 2) = 0.0f;\n        A(1, 0) = 2.0f; A(1, 1) = 5.0f; A(1, 2) = 1.0f;\n        A(2, 0) = 0.0f; A(2, 1) = 1.0f; A(2, 2) = 3.0f;\n        tiny::Mat b(3, 1);\n        b(0, 0) = 2.0f;\n        b(1, 0) = 3.0f;\n        b(2, 0) = 1.0f;\n\n        tiny::Mat::CholeskyDecomposition chol = A.cholesky_decompose();\n        tiny::Mat x = tiny::Mat::solve_cholesky(chol, b);\n\n        std::cout &lt;&lt; \"Solution x:\\n\";\n        x.print_matrix(true);\n\n        // Verify: A * x = b\n        tiny::Mat Ax = A * x;\n        float error = 0.0f;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            error += fabsf(Ax(i, 0) - b(i, 0));\n        }\n        std::cout &lt;&lt; \"Verification error: \" &lt;&lt; error &lt;&lt; (error &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.4: QR Decomposition\n    std::cout &lt;&lt; \"\\n[Test 7.4] QR Decomposition\\n\";\n\n    // Test 7.4.1: General matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.4.1] General 3x3 Matrix - QR Decomposition\\n\";\n        tiny::Mat A(3, 3);\n        A(0, 0) = 1.0f; A(0, 1) = 2.0f; A(0, 2) = 3.0f;\n        A(1, 0) = 4.0f; A(1, 1) = 5.0f; A(1, 2) = 6.0f;\n        A(2, 0) = 7.0f; A(2, 1) = 8.0f; A(2, 2) = 9.0f;\n        std::cout &lt;&lt; \"Matrix A:\\n\";\n        A.print_matrix(true);\n\n        tiny::Mat::QRDecomposition qr = A.qr_decompose();\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (qr.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (qr.status == TINY_OK)\n        {\n            std::cout &lt;&lt; \"Q matrix (orthogonal):\\n\";\n            qr.Q.print_matrix(true);\n            std::cout &lt;&lt; \"R matrix (upper triangular):\\n\";\n            qr.R.print_matrix(true);\n\n            // Verify: A = Q * R\n            tiny::Mat QR = qr.Q * qr.R;\n            std::cout &lt;&lt; \"\\n[Verification] Q * R should equal A\\n\";\n            float diff = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    diff += fabsf(QR(i, j) - A(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"Total difference: \" &lt;&lt; diff &lt;&lt; (diff &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n\n            // Verify Q is orthogonal: Q^T * Q = I\n            tiny::Mat QtQ = qr.Q.transpose() * qr.Q;\n            tiny::Mat I = tiny::Mat::eye(3);\n            float ortho_diff = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    ortho_diff += fabsf(QtQ(i, j) - I(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"Q orthogonality error: \" &lt;&lt; ortho_diff &lt;&lt; (ortho_diff &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n        }\n    }\n\n    // Test 7.4.2: Solve using QR decomposition (least squares)\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.4.2] Least Squares Solution using QR Decomposition\\n\";\n        tiny::Mat A(3, 2);  // Overdetermined system\n        A(0, 0) = 1.0f; A(0, 1) = 1.0f;\n        A(1, 0) = 1.0f; A(1, 1) = 2.0f;\n        A(2, 0) = 1.0f; A(2, 1) = 3.0f;\n        tiny::Mat b(3, 1);\n        b(0, 0) = 2.0f;\n        b(1, 0) = 3.0f;\n        b(2, 0) = 4.0f;\n\n        std::cout &lt;&lt; \"Overdetermined system: A * x \u2248 b\\n\";\n        std::cout &lt;&lt; \"A:\\n\";\n        A.print_matrix(true);\n        std::cout &lt;&lt; \"b:\\n\";\n        b.print_matrix(true);\n\n        tiny::Mat::QRDecomposition qr = A.qr_decompose();\n        tiny::Mat x = tiny::Mat::solve_qr(qr, b);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Least squares solution x:\\n\";\n        x.print_matrix(true);\n\n        // Compute residual: ||A * x - b||\n        tiny::Mat Ax = A * x;\n        tiny::Mat residual = Ax - b;\n        float residual_norm = 0.0f;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            residual_norm += residual(i, 0) * residual(i, 0);\n        }\n        residual_norm = sqrtf(residual_norm);\n        std::cout &lt;&lt; \"Residual norm ||A*x - b||: \" &lt;&lt; residual_norm &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.5: SVD Decomposition\n    std::cout &lt;&lt; \"\\n[Test 7.5] Singular Value Decomposition (SVD)\\n\";\n\n    // Test 7.5.1: General matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.5.1] General 3x3 Matrix - SVD Decomposition\\n\";\n        tiny::Mat A(3, 3);\n        A(0, 0) = 1.0f; A(0, 1) = 2.0f; A(0, 2) = 3.0f;\n        A(1, 0) = 4.0f; A(1, 1) = 5.0f; A(1, 2) = 6.0f;\n        A(2, 0) = 7.0f; A(2, 1) = 8.0f; A(2, 2) = 9.0f;\n        std::cout &lt;&lt; \"Matrix A:\\n\";\n        A.print_matrix(true);\n\n        tiny::Mat::SVDDecomposition svd = A.svd_decompose(100, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (svd.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (svd.status == TINY_OK)\n        {\n            std::cout &lt;&lt; \"Singular values:\\n\";\n            svd.S.print_matrix(true);\n            std::cout &lt;&lt; \"Numerical rank: \" &lt;&lt; svd.rank &lt;&lt; \"\\n\";\n            std::cout &lt;&lt; \"Iterations: \" &lt;&lt; svd.iterations &lt;&lt; \"\\n\";\n\n            // Verify: A \u2248 U * S * V^T (for first rank columns)\n            if (svd.rank &gt; 0)\n            {\n                tiny::Mat US(svd.U.row, svd.rank);\n                for (int i = 0; i &lt; svd.U.row; ++i)\n                {\n                    for (int j = 0; j &lt; svd.rank; ++j)\n                    {\n                        US(i, j) = svd.U(i, j) * svd.S(j, 0);\n                    }\n                }\n                tiny::Mat Vt(svd.rank, svd.V.row);\n                for (int i = 0; i &lt; svd.rank; ++i)\n                {\n                    for (int j = 0; j &lt; svd.V.row; ++j)\n                    {\n                        Vt(i, j) = svd.V(j, i);  // V^T\n                    }\n                }\n                tiny::Mat USVt = US * Vt;\n\n                float diff = 0.0f;\n                for (int i = 0; i &lt; 3; ++i)\n                {\n                    for (int j = 0; j &lt; 3; ++j)\n                    {\n                        diff += fabsf(USVt(i, j) - A(i, j));\n                    }\n                }\n                std::cout &lt;&lt; \"Reconstruction error: \" &lt;&lt; diff &lt;&lt; (diff &lt; 0.5f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n            }\n        }\n    }\n\n    // Test 7.5.2: Pseudo-inverse using SVD\n    {\n        std::cout &lt;&lt; \"\\n[Test 7.5.2] Pseudo-inverse using SVD\\n\";\n        tiny::Mat A(3, 2);  // Non-square matrix\n        A(0, 0) = 1.0f; A(0, 1) = 2.0f;\n        A(1, 0) = 3.0f; A(1, 1) = 4.0f;\n        A(2, 0) = 5.0f; A(2, 1) = 6.0f;\n\n        std::cout &lt;&lt; \"Matrix A (3x2):\\n\";\n        A.print_matrix(true);\n\n        tiny::Mat::SVDDecomposition svd = A.svd_decompose(100, 1e-6f);\n        tiny::Mat A_plus = tiny::Mat::pseudo_inverse(svd, 1e-6f);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Pseudo-inverse A^+ (2x3):\\n\";\n        A_plus.print_matrix(true);\n\n        // Verify: A * A^+ * A \u2248 A\n        tiny::Mat AAplusA = A * A_plus * A;\n        float diff = 0.0f;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            for (int j = 0; j &lt; 2; ++j)\n            {\n                diff += fabsf(AAplusA(i, j) - A(i, j));\n            }\n        }\n        std::cout &lt;&lt; \"Verification error (A * A^+ * A \u2248 A): \" &lt;&lt; diff &lt;&lt; (diff &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 7.6: Performance Tests\n    std::cout &lt;&lt; \"\\n[Test 7.6] Matrix Decomposition Performance Tests\\n\";\n\n    tiny::Mat perf_mat(4, 4);\n    perf_mat(0, 0) = 4.0f; perf_mat(0, 1) = 2.0f; perf_mat(0, 2) = 1.0f; perf_mat(0, 3) = 0.0f;\n    perf_mat(1, 0) = 2.0f; perf_mat(1, 1) = 5.0f; perf_mat(1, 2) = 1.0f; perf_mat(1, 3) = 0.0f;\n    perf_mat(2, 0) = 1.0f; perf_mat(2, 1) = 1.0f; perf_mat(2, 2) = 3.0f; perf_mat(2, 3) = 1.0f;\n    perf_mat(3, 0) = 0.0f; perf_mat(3, 1) = 0.0f; perf_mat(3, 2) = 1.0f; perf_mat(3, 3) = 2.0f;\n\n    // Test 7.6.1: LU decomposition performance\n    std::cout &lt;&lt; \"\\n[Test 7.6.1] LU Decomposition Performance\\n\";\n    TIME_OPERATION(\n        tiny::Mat::LUDecomposition perf_lu = perf_mat.lu_decompose(true);\n        (void)perf_lu;\n    , \"LU Decomposition (4x4 matrix)\");\n\n    // Test 7.6.2: Cholesky decomposition performance\n    std::cout &lt;&lt; \"\\n[Test 7.6.2] Cholesky Decomposition Performance\\n\";\n    TIME_OPERATION(\n        tiny::Mat::CholeskyDecomposition perf_chol = perf_mat.cholesky_decompose();\n        (void)perf_chol;\n    , \"Cholesky Decomposition (4x4 SPD matrix)\");\n\n    // Test 7.6.3: QR decomposition performance\n    std::cout &lt;&lt; \"\\n[Test 7.6.3] QR Decomposition Performance\\n\";\n    TIME_OPERATION(\n        tiny::Mat::QRDecomposition perf_qr = perf_mat.qr_decompose();\n        (void)perf_qr;\n    , \"QR Decomposition (4x4 matrix)\");\n\n    // Test 7.6.4: SVD decomposition performance\n    std::cout &lt;&lt; \"\\n[Test 7.6.4] SVD Decomposition Performance\\n\";\n    TIME_OPERATION(\n        tiny::Mat::SVDDecomposition perf_svd = perf_mat.svd_decompose(50, 1e-5f);\n        (void)perf_svd;\n    , \"SVD Decomposition (4x4 matrix)\");\n\n    std::cout &lt;&lt; \"\\n[Matrix Decomposition Tests Complete]\\n\";\n}\n\n// ============================================================================\n// Group 8: Advanced Linear Algebra - Gram-Schmidt Orthogonalization Tests\n// ============================================================================\n// Purpose: Test Gram-Schmidt orthogonalization process - fundamental operation for\n//          QR decomposition, eigenvalue decomposition, and basis transformation\nvoid test_gram_schmidt_orthogonalize()\n{\n    std::cout &lt;&lt; \"\\n[Group 8: Advanced Linear Algebra - Gram-Schmidt Orthogonalization Tests]\\n\";\n\n    // Test 8.1: Basic orthogonalization of linearly independent vectors\n    {\n        std::cout &lt;&lt; \"\\n[Test 8.1] Basic Orthogonalization - Linearly Independent Vectors\\n\";\n        tiny::Mat vectors(3, 3);\n        // Create three linearly independent vectors\n        vectors(0, 0) = 1.0f; vectors(0, 1) = 1.0f; vectors(0, 2) = 0.0f;\n        vectors(1, 0) = 0.0f; vectors(1, 1) = 1.0f; vectors(1, 2) = 1.0f;\n        vectors(2, 0) = 1.0f; vectors(2, 1) = 0.0f; vectors(2, 2) = 1.0f;\n\n        std::cout &lt;&lt; \"Input vectors (each column is a vector):\\n\";\n        vectors.print_matrix(true);\n\n        tiny::Mat Q, R;\n        bool success = tiny::Mat::gram_schmidt_orthogonalize(vectors, Q, R, 1e-6f);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (success ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (success)\n        {\n            std::cout &lt;&lt; \"Orthogonalized vectors Q (each column is orthogonal):\\n\";\n            Q.print_matrix(true);\n            std::cout &lt;&lt; \"Coefficients R (upper triangular):\\n\";\n            R.print_matrix(true);\n\n            // Verify orthogonality: Q^T * Q should be identity (or close to it)\n            tiny::Mat QtQ = Q.transpose() * Q;\n            tiny::Mat I = tiny::Mat::eye(3);\n            float ortho_error = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    ortho_error += fabsf(QtQ(i, j) - I(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"\\n[Verification] Q^T * Q should be identity\\n\";\n            std::cout &lt;&lt; \"Orthogonality error: \" &lt;&lt; ortho_error \n                      &lt;&lt; (ortho_error &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n\n            // Verify normalization: each column of Q should be unit vector\n            std::cout &lt;&lt; \"\\n[Verification] Each column of Q should be normalized\\n\";\n            bool all_normalized = true;\n            for (int j = 0; j &lt; 3; ++j)\n            {\n                float norm = 0.0f;\n                for (int i = 0; i &lt; 3; ++i)\n                {\n                    norm += Q(i, j) * Q(i, j);\n                }\n                norm = sqrtf(norm);\n                float norm_error = fabsf(norm - 1.0f);\n                std::cout &lt;&lt; \"  Column \" &lt;&lt; j &lt;&lt; \" norm: \" &lt;&lt; norm \n                          &lt;&lt; \" (error: \" &lt;&lt; norm_error &lt;&lt; \")\";\n                if (norm_error &gt; 0.01f)\n                {\n                    all_normalized = false;\n                    std::cout &lt;&lt; \" [FAIL]\";\n                }\n                else\n                {\n                    std::cout &lt;&lt; \" [PASS]\";\n                }\n                std::cout &lt;&lt; \"\\n\";\n            }\n\n            // Verify reconstruction: vectors should equal Q * R (approximately)\n            tiny::Mat QR = Q * R;\n            float recon_error = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                for (int j = 0; j &lt; 3; ++j)\n                {\n                    recon_error += fabsf(QR(i, j) - vectors(i, j));\n                }\n            }\n            std::cout &lt;&lt; \"\\n[Verification] Q * R should reconstruct original vectors\\n\";\n            std::cout &lt;&lt; \"Reconstruction error: \" &lt;&lt; recon_error \n                      &lt;&lt; (recon_error &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n        }\n    }\n\n    // Test 8.2: Orthogonalization with near-linear-dependent vectors\n    {\n        std::cout &lt;&lt; \"\\n[Test 8.2] Orthogonalization - Near-Linear-Dependent Vectors\\n\";\n        tiny::Mat vectors(3, 3);\n        // Create vectors where third is almost a linear combination of first two\n        vectors(0, 0) = 1.0f; vectors(0, 1) = 0.0f; vectors(0, 2) = 1.0f;\n        vectors(1, 0) = 0.0f; vectors(1, 1) = 1.0f; vectors(1, 2) = 1.0f;\n        vectors(2, 0) = 0.0f; vectors(2, 1) = 0.0f; vectors(2, 2) = 0.001f;  // Very small third component\n\n        std::cout &lt;&lt; \"Input vectors (third vector is nearly linear dependent):\\n\";\n        vectors.print_matrix(true);\n\n        tiny::Mat Q, R;\n        bool success = tiny::Mat::gram_schmidt_orthogonalize(vectors, Q, R, 1e-6f);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (success ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (success)\n        {\n            std::cout &lt;&lt; \"Orthogonalized vectors Q:\\n\";\n            Q.print_matrix(true);\n            std::cout &lt;&lt; \"Coefficients R:\\n\";\n            R.print_matrix(true);\n\n            // Check if third column was handled correctly (should be zero or orthogonal)\n            float third_col_norm = 0.0f;\n            for (int i = 0; i &lt; 3; ++i)\n            {\n                third_col_norm += Q(i, 2) * Q(i, 2);\n            }\n            third_col_norm = sqrtf(third_col_norm);\n            std::cout &lt;&lt; \"\\n[Note] Third column norm: \" &lt;&lt; third_col_norm \n                      &lt;&lt; \" (should be 0 if linearly dependent, or 1 if orthogonalized)\\n\";\n        }\n    }\n\n    // Test 8.3: Orthogonalization of 2D vectors\n    {\n        std::cout &lt;&lt; \"\\n[Test 8.3] Orthogonalization - 2D Vectors (2x2)\\n\";\n        tiny::Mat vectors(2, 2);\n        vectors(0, 0) = 3.0f; vectors(0, 1) = 1.0f;\n        vectors(1, 0) = 1.0f; vectors(1, 1) = 2.0f;\n\n        std::cout &lt;&lt; \"Input vectors:\\n\";\n        vectors.print_matrix(true);\n\n        tiny::Mat Q, R;\n        bool success = tiny::Mat::gram_schmidt_orthogonalize(vectors, Q, R, 1e-6f);\n\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (success ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (success)\n        {\n            std::cout &lt;&lt; \"Orthogonalized vectors Q:\\n\";\n            Q.print_matrix(true);\n            std::cout &lt;&lt; \"Coefficients R:\\n\";\n            R.print_matrix(true);\n\n            // Verify orthogonality\n            float dot_product = 0.0f;\n            for (int i = 0; i &lt; 2; ++i)\n            {\n                dot_product += Q(i, 0) * Q(i, 1);\n            }\n            std::cout &lt;&lt; \"\\n[Verification] Dot product of Q columns: \" &lt;&lt; dot_product \n                      &lt;&lt; \" (should be ~0 for orthogonal) \" \n                      &lt;&lt; (fabsf(dot_product) &lt; 0.01f ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n        }\n    }\n\n    // Test 8.4: Error handling - invalid input\n    {\n        std::cout &lt;&lt; \"\\n[Test 8.4] Error Handling - Invalid Input\\n\";\n        tiny::Mat empty_mat;  // Empty matrix\n        tiny::Mat Q, R;\n        bool success = tiny::Mat::gram_schmidt_orthogonalize(empty_mat, Q, R, 1e-6f);\n        std::cout &lt;&lt; \"Empty matrix test: \" &lt;&lt; (success ? \"FAIL (should return false)\" : \"PASS (correctly rejected)\") &lt;&lt; \"\\n\";\n    }\n}\n\n// ============================================================================\n// Group 9: System Identification - Eigenvalue and Eigenvector Decomposition Tests\n// ============================================================================\n// Purpose: Test eigenvalue decomposition - critical for SHM and system identification\n//          applications (modal analysis, natural frequencies, mode shapes)\nvoid test_eigenvalue_decomposition()\n{\n    std::cout &lt;&lt; \"\\n[Group 9: System Identification - Eigenvalue and Eigenvector Decomposition Tests]\\n\";\n\n    // Test 9.1: is_symmetric() - Basic functionality\n    std::cout &lt;&lt; \"\\n[Test 9.1] is_symmetric() - Basic Functionality\\n\";\n\n    // Test 9.1.1: Symmetric matrix\n    {\n        std::cout &lt;&lt; \"[Test 9.1.1] Symmetric 3x3 Matrix\\n\";\n        tiny::Mat sym_mat1(3, 3);\n        sym_mat1(0, 0) = 4.0f; sym_mat1(0, 1) = 1.0f; sym_mat1(0, 2) = 2.0f;\n        sym_mat1(1, 0) = 1.0f; sym_mat1(1, 1) = 3.0f; sym_mat1(1, 2) = 0.0f;\n        sym_mat1(2, 0) = 2.0f; sym_mat1(2, 1) = 0.0f; sym_mat1(2, 2) = 5.0f;\n        bool is_sym1 = sym_mat1.is_symmetric(1e-5f);\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        sym_mat1.print_matrix(true);\n        std::cout &lt;&lt; \"Is symmetric: \" &lt;&lt; (is_sym1 ? \"True\" : \"False\") &lt;&lt; \" (Expected: True)\\n\";\n    }\n\n    // Test 9.1.2: Non-symmetric matrix (keep for later tests)\n    tiny::Mat non_sym_mat(3, 3);\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.1.2] Non-Symmetric 3x3 Matrix\\n\";\n        non_sym_mat(0, 0) = 1.0f; non_sym_mat(0, 1) = 2.0f; non_sym_mat(0, 2) = 3.0f;\n        non_sym_mat(1, 0) = 4.0f; non_sym_mat(1, 1) = 5.0f; non_sym_mat(1, 2) = 6.0f;\n        non_sym_mat(2, 0) = 7.0f; non_sym_mat(2, 1) = 8.0f; non_sym_mat(2, 2) = 9.0f;\n        bool is_sym2 = non_sym_mat.is_symmetric(1e-5f);\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        non_sym_mat.print_matrix(true);\n        std::cout &lt;&lt; \"Is symmetric: \" &lt;&lt; (is_sym2 ? \"True\" : \"False\") &lt;&lt; \" (Expected: False)\\n\";\n    }\n\n    // Test 9.1.3: Non-square matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.1.3] Non-Square Matrix (2x3)\\n\";\n        tiny::Mat rect_mat(2, 3);\n        bool is_sym3 = rect_mat.is_symmetric(1e-5f);\n        std::cout &lt;&lt; \"Is symmetric: \" &lt;&lt; (is_sym3 ? \"True\" : \"False\") &lt;&lt; \" (Expected: False)\\n\";\n    }\n\n    // Test 9.1.4: Symmetric matrix with small numerical errors\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.1.4] Symmetric Matrix with Small Numerical Errors\\n\";\n        tiny::Mat sym_mat2(2, 2);\n        // Use 1e-5 error which is within float precision (float has ~7 significant digits)\n        // For 2.0, we can represent 2.00001 accurately\n        float error_value = 1e-5f;\n        sym_mat2(0, 0) = 1.0f; \n        sym_mat2(0, 1) = 2.0f + error_value;\n        sym_mat2(1, 0) = 2.0f; \n        sym_mat2(1, 1) = 3.0f;\n        std::cout &lt;&lt; \"Matrix with error \" &lt;&lt; error_value &lt;&lt; \":\\n\";\n        sym_mat2.print_matrix(true);\n        float diff = fabsf(sym_mat2(0, 1) - sym_mat2(1, 0));\n        std::cout &lt;&lt; \"Difference: |A(0,1) - A(1,0)| = \";\n        // Use scientific notation for small values\n        if (diff &lt; 1e-3f)\n        {\n            std::cout &lt;&lt; std::scientific &lt;&lt; std::setprecision(6) &lt;&lt; diff &lt;&lt; std::fixed;\n        }\n        else\n        {\n            std::cout &lt;&lt; std::setprecision(6) &lt;&lt; diff;\n        }\n        std::cout &lt;&lt; \" (Expected: \" &lt;&lt; error_value &lt;&lt; \")\\n\";\n\n        // Verify the difference is actually stored\n        float stored_value = sym_mat2(0, 1);\n        float expected_stored = 2.0f + error_value;\n        std::cout &lt;&lt; \"A(0,1) stored value: \" &lt;&lt; std::setprecision(8) &lt;&lt; stored_value \n                  &lt;&lt; \" (Expected: \" &lt;&lt; expected_stored &lt;&lt; \")\\n\";\n\n        bool is_sym4 = sym_mat2.is_symmetric(1e-4f); // tolerance &gt; error, should pass\n        std::cout &lt;&lt; \"Is symmetric (tolerance=1e-4): \" &lt;&lt; (is_sym4 ? \"True\" : \"False\") \n                  &lt;&lt; \" (Expected: True, tolerance &gt; error) \";\n        std::cout &lt;&lt; (is_sym4 ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n\n        bool is_sym5 = sym_mat2.is_symmetric(1e-6f); // tolerance &lt; error, should fail\n        std::cout &lt;&lt; \"Is symmetric (tolerance=1e-6): \" &lt;&lt; (is_sym5 ? \"True\" : \"False\") \n                  &lt;&lt; \" (Expected: False, tolerance &lt; error) \";\n        bool correct_result = !is_sym5; // Should be False (not symmetric)\n        std::cout &lt;&lt; (correct_result ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n\n        // Additional check: verify the difference is close to expected\n        float diff_error = fabsf(diff - error_value);\n        std::cout &lt;&lt; \"Difference accuracy: |actual_diff - expected_diff| = \" \n                  &lt;&lt; std::scientific &lt;&lt; std::setprecision(2) &lt;&lt; diff_error &lt;&lt; std::fixed;\n        bool diff_accurate = (diff_error &lt; error_value * 0.1f); // Within 10% of error value\n        std::cout &lt;&lt; \" \" &lt;&lt; (diff_accurate ? \"[PASS - difference stored correctly]\" : \"[FAIL - float precision issue]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2: power_iteration() - Dominant eigenvalue\n    std::cout &lt;&lt; \"\\n[Test 9.2] power_iteration() - Dominant Eigenvalue\\n\";\n\n    // Test 9.2.1: Simple 2x2 symmetric matrix (known eigenvalues)\n    tiny::Mat mat2x2(2, 2);\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.1] Simple 2x2 Matrix\\n\";\n        mat2x2(0, 0) = 2.0f; mat2x2(0, 1) = 1.0f;\n        mat2x2(1, 0) = 1.0f; mat2x2(1, 1) = 2.0f;\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        mat2x2.print_matrix(true);\n\n        // Expected values: eigenvalues are 3 and 1 (for matrix [2,1; 1,2])\n        // Characteristic equation: det([2-\u03bb, 1; 1, 2-\u03bb]) = (2-\u03bb)\u00b2 - 1 = \u03bb\u00b2 - 4\u03bb + 3 = 0\n        // Solutions: \u03bb = (4 \u00b1 \u221a(16-12))/2 = (4 \u00b1 2)/2 = 3 or 1\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\\n\";\n        std::cout &lt;&lt; \"  Expected dominant eigenvector (for \u03bb=3): approximately [0.707, 0.707] or [-0.707, -0.707] (normalized)\\n\";\n        std::cout &lt;&lt; \"  Expected dominant eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\\n\";\n\n        tiny::Mat::EigenPair result_power = mat2x2.power_iteration(1000, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"  Dominant eigenvalue: \" &lt;&lt; result_power.eigenvalue \n                  &lt;&lt; \" (Expected: 3.0, largest eigenvalue)\\n\";\n        std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; result_power.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Status: \" &lt;&lt; (result_power.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Dominant eigenvector:\\n\";\n        result_power.eigenvector.print_matrix(true);\n\n        // Check if result matches expected\n        float error = fabsf(result_power.eigenvalue - 3.0f);\n        std::cout &lt;&lt; \"  Error from expected (3.0): \" &lt;&lt; error &lt;&lt; (error &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2.2: 3x3 matrix (SHM-like stiffness matrix) - keep for later tests\n    tiny::Mat stiffness(3, 3);\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.2] 3x3 Stiffness Matrix (SHM Application)\\n\";\n        stiffness(0, 0) = 2.0f; stiffness(0, 1) = -1.0f; stiffness(0, 2) = 0.0f;\n        stiffness(1, 0) = -1.0f; stiffness(1, 1) = 2.0f; stiffness(1, 2) = -1.0f;\n        stiffness(2, 0) = 0.0f; stiffness(2, 1) = -1.0f; stiffness(2, 2) = 2.0f;\n        std::cout &lt;&lt; \"Stiffness Matrix:\\n\";\n        stiffness.print_matrix(true);\n\n        // Expected values for 3x3 tridiagonal symmetric matrix [2,-1,0; -1,2,-1; 0,-1,2]\n        // This is a standard tridiagonal matrix with known eigenvalues\n        // Approximate eigenvalues: \u03bb\u2081 \u2248 3.414, \u03bb\u2082 \u2248 2.000, \u03bb\u2083 \u2248 0.586\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\\n\";\n        std::cout &lt;&lt; \"  Expected primary frequency: sqrt(3.414) \u2248 1.848 rad/s\\n\";\n\n        tiny::Mat::EigenPair result_stiff = stiffness.power_iteration(500, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"  Dominant eigenvalue (primary frequency squared): \" &lt;&lt; result_stiff.eigenvalue &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Primary frequency: \" &lt;&lt; sqrtf(result_stiff.eigenvalue) &lt;&lt; \" rad/s (Expected: ~1.848 rad/s)\\n\";\n        std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; result_stiff.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Status: \" &lt;&lt; (result_stiff.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n\n        float expected_eigen = 3.414f;\n        float error = fabsf(result_stiff.eigenvalue - expected_eigen);\n        std::cout &lt;&lt; \"  Error from expected (\" &lt;&lt; expected_eigen &lt;&lt; \"): \" &lt;&lt; error &lt;&lt; (error &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2.3: Non-square matrix (should fail)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.3] Non-Square Matrix (Expect Error)\\n\";\n        tiny::Mat non_square(2, 3);\n        tiny::Mat::EigenPair result_error = non_square.power_iteration(100, 1e-6f);\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_error.status == TINY_OK ? \"OK\" : \"Error (Expected)\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2.4: inverse_power_iteration() - Smallest eigenvalue (Critical for System Identification)\n    std::cout &lt;&lt; \"\\n[Test 9.2.4] inverse_power_iteration() - Smallest Eigenvalue (System Identification)\\n\";\n\n    // Test 9.2.4.1: Simple 2x2 symmetric matrix (known eigenvalues)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.4.1] Simple 2x2 Matrix - Smallest Eigenvalue\\n\";\n        std::cout &lt;&lt; \"Matrix (same as Test 9.2.1):\\n\";\n        mat2x2.print_matrix(true);\n\n        // Expected values: eigenvalues are 3 and 1 (for matrix [2,1; 1,2])\n        // Power iteration finds \u03bb_max = 3, inverse power iteration should find \u03bb_min = 1\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\\n\";\n        std::cout &lt;&lt; \"  Expected smallest eigenvalue: 1.0\\n\";\n        std::cout &lt;&lt; \"  Expected smallest eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\\n\";\n        std::cout &lt;&lt; \"  Note: This is critical for system identification - smallest eigenvalue = fundamental frequency\\n\";\n\n        tiny::Mat::EigenPair result_inv_power = mat2x2.inverse_power_iteration(1000, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"  Smallest eigenvalue: \" &lt;&lt; result_inv_power.eigenvalue \n                  &lt;&lt; \" (Expected: 1.0, smallest eigenvalue)\\n\";\n        std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; result_inv_power.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Status: \" &lt;&lt; (result_inv_power.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Smallest eigenvector:\\n\";\n        result_inv_power.eigenvector.print_matrix(true);\n\n        // Check if result matches expected\n        float error = fabsf(result_inv_power.eigenvalue - 1.0f);\n        std::cout &lt;&lt; \"  Error from expected (1.0): \" &lt;&lt; error &lt;&lt; (error &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n\n        // Compare with power iteration results (recompute for comparison)\n        tiny::Mat::EigenPair result_power_compare = mat2x2.power_iteration(1000, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Comparison] Power vs Inverse Power Iteration:\\n\";\n        std::cout &lt;&lt; \"  Power iteration (\u03bb_max): \" &lt;&lt; result_power_compare.eigenvalue &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Inverse power iteration (\u03bb_min): \" &lt;&lt; result_inv_power.eigenvalue &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Ratio (\u03bb_max/\u03bb_min): \" &lt;&lt; (result_power_compare.eigenvalue / result_inv_power.eigenvalue) \n                  &lt;&lt; \" (Expected: ~3.0) \" &lt;&lt; (fabsf(result_power_compare.eigenvalue / result_inv_power.eigenvalue - 3.0f) &lt; 0.1f ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2.4.2: 3x3 stiffness matrix - Smallest eigenvalue (SHM Application)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.4.2] 3x3 Stiffness Matrix - Smallest Eigenvalue (SHM Application)\\n\";\n        std::cout &lt;&lt; \"Stiffness Matrix (same as Test 9.2.2):\\n\";\n        stiffness.print_matrix(true);\n\n        // Expected values for 3x3 tridiagonal symmetric matrix [2,-1,0; -1,2,-1; 0,-1,2]\n        // Approximate eigenvalues: \u03bb\u2081 \u2248 3.414 (largest), \u03bb\u2082 \u2248 2.000, \u03bb\u2083 \u2248 0.586 (smallest)\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\\n\";\n        std::cout &lt;&lt; \"  Expected smallest eigenvalue: ~0.586 (fundamental frequency squared)\\n\";\n        std::cout &lt;&lt; \"  Expected fundamental frequency: sqrt(0.586) \u2248 0.765 rad/s\\n\";\n        std::cout &lt;&lt; \"  Note: Smallest eigenvalue is critical for system identification - represents fundamental mode\\n\";\n\n        tiny::Mat::EigenPair result_inv_stiff = stiffness.inverse_power_iteration(500, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"  Smallest eigenvalue (fundamental frequency squared): \" &lt;&lt; result_inv_stiff.eigenvalue &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Fundamental frequency: \" &lt;&lt; sqrtf(result_inv_stiff.eigenvalue) &lt;&lt; \" rad/s (Expected: ~0.765 rad/s)\\n\";\n        std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; result_inv_stiff.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Status: \" &lt;&lt; (result_inv_stiff.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  Smallest eigenvector (fundamental mode shape):\\n\";\n        result_inv_stiff.eigenvector.print_matrix(true);\n\n        float expected_eigen = 0.586f;\n        float error = fabsf(result_inv_stiff.eigenvalue - expected_eigen);\n        std::cout &lt;&lt; \"  Error from expected (\" &lt;&lt; expected_eigen &lt;&lt; \"): \" &lt;&lt; error &lt;&lt; (error &lt; 0.1f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n\n        // Compare with power iteration (recompute for comparison)\n        tiny::Mat::EigenPair result_stiff_compare = stiffness.power_iteration(500, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Comparison] Power vs Inverse Power Iteration for SHM:\\n\";\n        std::cout &lt;&lt; \"  Power iteration (primary frequency\u00b2): \" &lt;&lt; result_stiff_compare.eigenvalue \n                  &lt;&lt; \" \u2192 frequency: \" &lt;&lt; sqrtf(result_stiff_compare.eigenvalue) &lt;&lt; \" rad/s\\n\";\n        std::cout &lt;&lt; \"  Inverse power iteration (fundamental frequency\u00b2): \" &lt;&lt; result_inv_stiff.eigenvalue \n                  &lt;&lt; \" \u2192 frequency: \" &lt;&lt; sqrtf(result_inv_stiff.eigenvalue) &lt;&lt; \" rad/s\\n\";\n        std::cout &lt;&lt; \"  Frequency ratio: \" &lt;&lt; (sqrtf(result_stiff_compare.eigenvalue) / sqrtf(result_inv_stiff.eigenvalue))\n                  &lt;&lt; \" (Expected: ~2.4, ratio of highest to lowest mode)\\n\";\n    }\n\n    // Test 9.2.4.3: Non-square matrix (should fail)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.4.3] Non-Square Matrix (Expect Error)\\n\";\n        tiny::Mat non_square(2, 3);\n        tiny::Mat::EigenPair result_error = non_square.inverse_power_iteration(100, 1e-6f);\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_error.status == TINY_OK ? \"OK\" : \"Error (Expected)\") &lt;&lt; \"\\n\";\n        bool correct = (result_error.status != TINY_OK);\n        std::cout &lt;&lt; \"Error handling: \" &lt;&lt; (correct ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.2.4.4: Near-singular matrix (should handle gracefully)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.2.4.4] Near-Singular Matrix (Edge Case)\\n\";\n        tiny::Mat near_singular(3, 3);\n        // Create a matrix that is close to singular but still invertible\n        near_singular(0, 0) = 1.0f; near_singular(0, 1) = 0.0f; near_singular(0, 2) = 0.0f;\n        near_singular(1, 0) = 0.0f; near_singular(1, 1) = 1.0f; near_singular(1, 2) = 0.001f;\n        near_singular(2, 0) = 0.0f; near_singular(2, 1) = 0.001f; near_singular(2, 2) = 1.0f;\n        std::cout &lt;&lt; \"Matrix (near-singular but invertible):\\n\";\n        near_singular.print_matrix(true);\n\n        tiny::Mat::EigenPair result_near_sing = near_singular.inverse_power_iteration(500, 1e-5f);\n        std::cout &lt;&lt; \"\\n[Results]\\n\";\n        std::cout &lt;&lt; \"  Status: \" &lt;&lt; (result_near_sing.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        if (result_near_sing.status == TINY_OK)\n        {\n            std::cout &lt;&lt; \"  Smallest eigenvalue: \" &lt;&lt; result_near_sing.eigenvalue &lt;&lt; \"\\n\";\n            std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; result_near_sing.iterations &lt;&lt; \"\\n\";\n            std::cout &lt;&lt; \"  Note: Successfully handled near-singular matrix [PASS]\\n\";\n        }\n        else\n        {\n            std::cout &lt;&lt; \"  Note: Correctly detected problematic matrix [PASS]\\n\";\n        }\n    }\n\n    // Test 9.3: eigendecompose_jacobi() - Symmetric matrix decomposition\n    std::cout &lt;&lt; \"\\n[Test 9.3] eigendecompose_jacobi() - Symmetric Matrix Decomposition\\n\";\n\n    // Test 9.3.1: Simple 2x2 symmetric matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.3.1] 2x2 Symmetric Matrix - Complete Decomposition\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: 3.0, 1.0 (in any order)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvectors (for \u03bb=3): [0.707, 0.707] or [-0.707, -0.707] (normalized)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvectors (for \u03bb=1): [0.707, -0.707] or [-0.707, 0.707] (normalized)\\n\";\n\n        tiny::Mat::EigenDecomposition result_jacobi1 = mat2x2.eigendecompose_jacobi(1e-6f, 100);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_jacobi1.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Eigenvectors (each column is an eigenvector):\\n\";\n        result_jacobi1.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_jacobi1.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_jacobi1.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n\n        // Check eigenvalues\n        float ev1 = result_jacobi1.eigenvalues(0, 0);\n        float ev2 = result_jacobi1.eigenvalues(1, 0);\n        bool ev_check = ((fabsf(ev1 - 3.0f) &lt; 0.01f &amp;&amp; fabsf(ev2 - 1.0f) &lt; 0.01f) ||\n                         (fabsf(ev1 - 1.0f) &lt; 0.01f &amp;&amp; fabsf(ev2 - 3.0f) &lt; 0.01f));\n        std::cout &lt;&lt; \"Eigenvalue check (should be 3.0 and 1.0): \" &lt;&lt; (ev_check ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n\n        // Verify: A * v = lambda * v\n        std::cout &lt;&lt; \"\\n[Verification] Check A * v = lambda * v for first eigenvector:\\n\";\n        tiny::Mat Av = mat2x2 * result_jacobi1.eigenvectors.block(0, 0, 2, 1);\n        tiny::Mat lambda_v = result_jacobi1.eigenvalues(0, 0) * result_jacobi1.eigenvectors.block(0, 0, 2, 1);\n        std::cout &lt;&lt; \"A * v:\\n\";\n        Av.print_matrix(true);\n        std::cout &lt;&lt; \"lambda * v:\\n\";\n        lambda_v.print_matrix(true);\n        bool verify1 = matrices_approximately_equal(Av, lambda_v, 1e-4f);\n        std::cout &lt;&lt; \"Verification (A*v = \u03bb*v): \" &lt;&lt; (verify1 ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.3.2: 3x3 symmetric matrix (SHM stiffness matrix)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.3.2] 3x3 Stiffness Matrix (SHM Application)\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (approximate): 3.414, 2.000, 0.586\\n\";\n        std::cout &lt;&lt; \"  Expected natural frequencies: 1.848, 1.414, 0.765 rad/s\\n\";\n        std::cout &lt;&lt; \"  Note: Eigenvalues may appear in any order\\n\";\n\n        tiny::Mat::EigenDecomposition result_jacobi2 = stiffness.eigendecompose_jacobi(1e-5f, 100);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues (natural frequencies squared):\\n\";\n        result_jacobi2.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Natural frequencies (rad/s):\\n\";\n        float expected_freqs[3] = {1.848f, 1.414f, 0.765f};\n        for (int i = 0; i &lt; result_jacobi2.eigenvalues.row; ++i)\n        {\n            float freq = sqrtf(result_jacobi2.eigenvalues(i, 0));\n            std::cout &lt;&lt; \"  Mode \" &lt;&lt; i &lt;&lt; \": \" &lt;&lt; freq &lt;&lt; \" rad/s\";\n            // Check if frequency matches any expected value\n            bool matched = false;\n            for (int j = 0; j &lt; 3; ++j)\n            {\n                if (fabsf(freq - expected_freqs[j]) &lt; 0.1f)\n                {\n                    std::cout &lt;&lt; \" (Expected: ~\" &lt;&lt; expected_freqs[j] &lt;&lt; \" rad/s) [PASS]\";\n                    matched = true;\n                    break;\n                }\n            }\n            if (!matched) std::cout &lt;&lt; \" [CHECK]\";\n            std::cout &lt;&lt; \"\\n\";\n        }\n        std::cout &lt;&lt; \"Eigenvectors (mode shapes):\\n\";\n        result_jacobi2.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_jacobi2.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_jacobi2.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.3.3: Diagonal matrix (trivial case)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.3.3] Diagonal Matrix (Eigenvalues on diagonal)\\n\";\n        tiny::Mat diag_mat(3, 3);\n        diag_mat(0, 0) = 5.0f; diag_mat(0, 1) = 0.0f; diag_mat(0, 2) = 0.0f;\n        diag_mat(1, 0) = 0.0f; diag_mat(1, 1) = 3.0f; diag_mat(1, 2) = 0.0f;\n        diag_mat(2, 0) = 0.0f; diag_mat(2, 1) = 0.0f; diag_mat(2, 2) = 1.0f;\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        diag_mat.print_matrix(true);\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: 5.0, 3.0, 1.0 (diagonal elements, may be in any order)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvectors: standard basis vectors [1,0,0], [0,1,0], [0,0,1] (or their negatives)\\n\";\n        std::cout &lt;&lt; \"  Expected iterations: 1 (diagonal matrix should converge immediately)\\n\";\n\n        tiny::Mat::EigenDecomposition result_diag = diag_mat.eigendecompose_jacobi(1e-6f, 10);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_diag.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Eigenvectors:\\n\";\n        result_diag.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_diag.iterations &lt;&lt; \" (Expected: 1)\\n\";\n\n        // Check eigenvalues\n        float ev1 = result_diag.eigenvalues(0, 0);\n        float ev2 = result_diag.eigenvalues(1, 0);\n        float ev3 = result_diag.eigenvalues(2, 0);\n        bool ev_check = ((fabsf(ev1 - 5.0f) &lt; 0.01f || fabsf(ev1 - 3.0f) &lt; 0.01f || fabsf(ev1 - 1.0f) &lt; 0.01f) &amp;&amp;\n                         (fabsf(ev2 - 5.0f) &lt; 0.01f || fabsf(ev2 - 3.0f) &lt; 0.01f || fabsf(ev2 - 1.0f) &lt; 0.01f) &amp;&amp;\n                         (fabsf(ev3 - 5.0f) &lt; 0.01f || fabsf(ev3 - 3.0f) &lt; 0.01f || fabsf(ev3 - 1.0f) &lt; 0.01f));\n        std::cout &lt;&lt; \"Eigenvalue check (should be 5.0, 3.0, 1.0): \" &lt;&lt; (ev_check ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.4: eigendecompose_qr() - General matrix decomposition\n    std::cout &lt;&lt; \"\\n[Test 9.4] eigendecompose_qr() - General Matrix Decomposition\\n\";\n\n    // Test 9.4.1: General 2x2 matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.4.1] General 2x2 Matrix\\n\";\n        tiny::Mat gen_mat(2, 2);\n        gen_mat(0, 0) = 1.0f; gen_mat(0, 1) = 2.0f;\n        gen_mat(1, 0) = 3.0f; gen_mat(1, 1) = 4.0f;\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        gen_mat.print_matrix(true);\n\n        // Expected values for matrix [1,2; 3,4]\n        // Characteristic equation: det([1-\u03bb, 2; 3, 4-\u03bb]) = (1-\u03bb)(4-\u03bb) - 6 = \u03bb\u00b2 - 5\u03bb - 2 = 0\n        // Solutions: \u03bb = (5 \u00b1 \u221a(25+8))/2 = (5 \u00b1 \u221a33)/2 \u2248 5.372, -0.372\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: (5+\u221a33)/2 \u2248 5.372, (5-\u221a33)/2 \u2248 -0.372\\n\";\n        std::cout &lt;&lt; \"  Note: This is a non-symmetric matrix, eigenvalues are real but may have complex eigenvectors\\n\";\n\n        tiny::Mat::EigenDecomposition result_qr1 = gen_mat.eigendecompose_qr(100, 1e-5f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_qr1.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Eigenvectors:\\n\";\n        result_qr1.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_qr1.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_qr1.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n\n        // Check eigenvalues with detailed error reporting\n        float ev1 = result_qr1.eigenvalues(0, 0);\n        float ev2 = result_qr1.eigenvalues(1, 0);\n        float expected_ev1 = 5.372f;\n        float expected_ev2 = -0.372f;\n\n        // Match eigenvalues to expected values\n        float error1a = fabsf(ev1 - expected_ev1);\n        float error1b = fabsf(ev1 - expected_ev2);\n        float error2a = fabsf(ev2 - expected_ev1);\n        float error2b = fabsf(ev2 - expected_ev2);\n\n        bool match1 = (error1a &lt; error1b); // ev1 matches expected_ev1 better\n        float matched_ev1 = match1 ? expected_ev1 : expected_ev2;\n        float matched_ev2 = match1 ? expected_ev2 : expected_ev1;\n        float actual_error1 = match1 ? error1a : error1b;\n        float actual_error2 = match1 ? error2b : error2a;\n        float rel_error1 = actual_error1 / fabsf(matched_ev1);\n        float rel_error2 = actual_error2 / fabsf(matched_ev2);\n\n        std::cout &lt;&lt; \"Eigenvalue 1: \" &lt;&lt; ev1 &lt;&lt; \" (Expected: \" &lt;&lt; matched_ev1 &lt;&lt; \", Error: \" &lt;&lt; actual_error1 \n                  &lt;&lt; \", Rel Error: \" &lt;&lt; (rel_error1 * 100.0f) &lt;&lt; \"%) \";\n        bool pass1 = (rel_error1 &lt; 0.05f); // 5% relative error tolerance\n        std::cout &lt;&lt; (pass1 ? \"[PASS]\" : \"[FAIL - error too large]\") &lt;&lt; \"\\n\";\n\n        std::cout &lt;&lt; \"Eigenvalue 2: \" &lt;&lt; ev2 &lt;&lt; \" (Expected: \" &lt;&lt; matched_ev2 &lt;&lt; \", Error: \" &lt;&lt; actual_error2 \n                  &lt;&lt; \", Rel Error: \" &lt;&lt; (rel_error2 * 100.0f) &lt;&lt; \"%) \";\n        bool pass2 = (rel_error2 &lt; 0.05f); // 5% relative error tolerance\n        std::cout &lt;&lt; (pass2 ? \"[PASS]\" : \"[FAIL - error too large]\") &lt;&lt; \"\\n\";\n\n        bool ev_check = pass1 &amp;&amp; pass2;\n        std::cout &lt;&lt; \"Overall eigenvalue check: \" &lt;&lt; (ev_check ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.4.2: Non-symmetric 3x3 matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.4.2] Non-Symmetric 3x3 Matrix\\n\";\n        std::cout &lt;&lt; \"Matrix [1,2,3; 4,5,6; 7,8,9]:\\n\";\n        non_sym_mat.print_matrix(true);\n\n        // Expected values for matrix [1,2,3; 4,5,6; 7,8,9]\n        // Characteristic equation: \u03bb\u00b3 - 15\u03bb\u00b2 - 18\u03bb = 0\n        // Solutions: \u03bb(\u03bb\u00b2 - 15\u03bb - 18) = 0\n        // \u03bb\u2081 = 0, \u03bb\u2082,\u2083 = (15 \u00b1 \u221a(225+72))/2 = (15 \u00b1 \u221a297)/2 \u2248 16.12, -1.12\n        // However, QR algorithm may have numerical errors, especially for non-symmetric matrices\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\\n\";\n        std::cout &lt;&lt; \"  Note: This matrix is rank-deficient (determinant = 0), so one eigenvalue is 0\\n\";\n        std::cout &lt;&lt; \"  Note: QR algorithm may have numerical errors, especially for non-symmetric matrices\\n\";\n        std::cout &lt;&lt; \"  Acceptable range: largest eigenvalue ~15-18, smallest eigenvalue near 0\\n\";\n\n        tiny::Mat::EigenDecomposition result_qr2 = non_sym_mat.eigendecompose_qr(100, 1e-5f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_qr2.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Eigenvectors:\\n\";\n        result_qr2.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_qr2.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_qr2.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n\n        // Check results with detailed error reporting\n        float expected_evs[3] = {16.12f, -1.12f, 0.0f};\n        bool matched[3] = {false, false, false};\n        float errors[3] = {0.0f, 0.0f, 0.0f};\n        float rel_errors[3] = {0.0f, 0.0f, 0.0f};\n\n        // Match each computed eigenvalue to the closest expected value\n        for (int i = 0; i &lt; result_qr2.eigenvalues.row; ++i)\n        {\n            float ev = result_qr2.eigenvalues(i, 0);\n            float min_error = 1e10f;\n            int best_match = -1;\n\n            // Find closest expected eigenvalue\n            for (int j = 0; j &lt; 3; ++j)\n            {\n                if (!matched[j])\n                {\n                    float error = fabsf(ev - expected_evs[j]);\n                    if (error &lt; min_error)\n                    {\n                        min_error = error;\n                        best_match = j;\n                    }\n                }\n            }\n\n            if (best_match &gt;= 0)\n            {\n                matched[best_match] = true;\n                errors[best_match] = min_error;\n                float expected = expected_evs[best_match];\n                rel_errors[best_match] = (fabsf(expected) &gt; 1e-6f) ? (min_error / fabsf(expected)) : min_error;\n\n                std::cout &lt;&lt; \"Eigenvalue \" &lt;&lt; i &lt;&lt; \": \" &lt;&lt; ev &lt;&lt; \" (Expected: \" &lt;&lt; expected \n                          &lt;&lt; \", Error: \" &lt;&lt; min_error &lt;&lt; \", Rel Error: \" &lt;&lt; (rel_errors[best_match] * 100.0f) &lt;&lt; \"%) \";\n\n                // For zero eigenvalue, use absolute tolerance; for others, use relative tolerance\n                bool pass = (fabsf(expected) &lt; 0.1f) ? (min_error &lt; 0.1f) : (rel_errors[best_match] &lt; 0.15f); // 15% tolerance for QR\n                std::cout &lt;&lt; (pass ? \"[PASS]\" : \"[FAIL - error too large]\") &lt;&lt; \"\\n\";\n            }\n        }\n\n        // Overall check\n        bool overall_pass = true;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            bool pass = (fabsf(expected_evs[i]) &lt; 0.1f) ? (errors[i] &lt; 0.1f) : (rel_errors[i] &lt; 0.15f);\n            if (!pass) overall_pass = false;\n        }\n        std::cout &lt;&lt; \"Overall eigenvalue check: \" &lt;&lt; (overall_pass ? \"[PASS]\" : \"[FAIL - some eigenvalues have large errors]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.5: eigendecompose() - Automatic method selection\n    std::cout &lt;&lt; \"\\n[Test 9.5] eigendecompose() - Automatic Method Selection\\n\";\n\n    // Test 9.5.1: Symmetric matrix (should use Jacobi)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.5.1] Symmetric Matrix (Auto-select: Jacobi)\\n\";\n        tiny::Mat sym_mat1(3, 3);\n        sym_mat1(0, 0) = 4.0f; sym_mat1(0, 1) = 1.0f; sym_mat1(0, 2) = 2.0f;\n        sym_mat1(1, 0) = 1.0f; sym_mat1(1, 1) = 3.0f; sym_mat1(1, 2) = 0.0f;\n        sym_mat1(2, 0) = 2.0f; sym_mat1(2, 1) = 0.0f; sym_mat1(2, 2) = 5.0f;\n        std::cout &lt;&lt; \"Matrix:\\n\";\n        sym_mat1.print_matrix(true);\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Method: Should automatically use Jacobi (symmetric matrix detected)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (approximate): 6.67, 3.48, 1.85\\n\";\n        std::cout &lt;&lt; \"  Note: Eigenvalues may appear in any order\\n\";\n\n        tiny::Mat::EigenDecomposition result_auto1 = sym_mat1.eigendecompose(1e-5f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_auto1.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_auto1.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_auto1.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Method used: Jacobi (auto-selected for symmetric matrix)\\n\";\n    }\n\n    // Test 9.5.2: Non-symmetric matrix (should use QR)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.5.2] Non-Symmetric Matrix (Auto-select: QR)\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Method: Should automatically use QR (non-symmetric matrix detected)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\\n\";\n        std::cout &lt;&lt; \"  Note: One eigenvalue should be near 0 (rank-deficient matrix)\\n\";\n        std::cout &lt;&lt; \"  Note: QR algorithm may have numerical errors for non-symmetric matrices\\n\";\n        std::cout &lt;&lt; \"  Acceptable: largest ~15-18, smallest near 0, one near -1 to -3\\n\";\n\n        tiny::Mat::EigenDecomposition result_auto2 = non_sym_mat.eigendecompose(1e-5f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues:\\n\";\n        result_auto2.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_auto2.iterations &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_auto2.status == TINY_OK ? \"OK\" : \"Error\") &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Method used: QR (auto-selected for non-symmetric matrix)\\n\";\n\n        // Check results with detailed error reporting (same as Test 4.2)\n        float expected_evs[3] = {16.12f, -1.12f, 0.0f};\n        bool matched[3] = {false, false, false};\n        float errors[3] = {0.0f, 0.0f, 0.0f};\n        float rel_errors[3] = {0.0f, 0.0f, 0.0f};\n\n        for (int i = 0; i &lt; result_auto2.eigenvalues.row; ++i)\n        {\n            float ev = result_auto2.eigenvalues(i, 0);\n            float min_error = 1e10f;\n            int best_match = -1;\n\n            for (int j = 0; j &lt; 3; ++j)\n            {\n                if (!matched[j])\n                {\n                    float error = fabsf(ev - expected_evs[j]);\n                    if (error &lt; min_error)\n                    {\n                        min_error = error;\n                        best_match = j;\n                    }\n                }\n            }\n\n            if (best_match &gt;= 0)\n            {\n                matched[best_match] = true;\n                errors[best_match] = min_error;\n                float expected = expected_evs[best_match];\n                rel_errors[best_match] = (fabsf(expected) &gt; 1e-6f) ? (min_error / fabsf(expected)) : min_error;\n\n                std::cout &lt;&lt; \"Eigenvalue \" &lt;&lt; i &lt;&lt; \": \" &lt;&lt; ev &lt;&lt; \" (Expected: \" &lt;&lt; expected \n                          &lt;&lt; \", Error: \" &lt;&lt; min_error &lt;&lt; \", Rel Error: \" &lt;&lt; (rel_errors[best_match] * 100.0f) &lt;&lt; \"%) \";\n\n                bool pass = (fabsf(expected) &lt; 0.1f) ? (min_error &lt; 0.1f) : (rel_errors[best_match] &lt; 0.15f);\n                std::cout &lt;&lt; (pass ? \"[PASS]\" : \"[FAIL - error too large]\") &lt;&lt; \"\\n\";\n            }\n        }\n\n        bool overall_pass = true;\n        for (int i = 0; i &lt; 3; ++i)\n        {\n            bool pass = (fabsf(expected_evs[i]) &lt; 0.1f) ? (errors[i] &lt; 0.1f) : (rel_errors[i] &lt; 0.15f);\n            if (!pass) overall_pass = false;\n        }\n        std::cout &lt;&lt; \"Overall eigenvalue check: \" &lt;&lt; (overall_pass ? \"[PASS]\" : \"[FAIL - some eigenvalues have large errors]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.6: SHM Application Scenario - Structural Dynamics\n    std::cout &lt;&lt; \"\\n[Test 9.6] SHM Application - Structural Dynamics Analysis\\n\";\n\n    // Create a simple 4-DOF structural system (mass-spring system)\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.6.1] 4-DOF Mass-Spring System\\n\";\n        tiny::Mat K(4, 4);  // Stiffness matrix\n        K(0, 0) = 2.0f; K(0, 1) = -1.0f; K(0, 2) = 0.0f; K(0, 3) = 0.0f;\n        K(1, 0) = -1.0f; K(1, 1) = 2.0f; K(1, 2) = -1.0f; K(1, 3) = 0.0f;\n        K(2, 0) = 0.0f; K(2, 1) = -1.0f; K(2, 2) = 2.0f; K(2, 3) = -1.0f;\n        K(3, 0) = 0.0f; K(3, 1) = 0.0f; K(3, 2) = -1.0f; K(3, 3) = 1.0f;\n\n        std::cout &lt;&lt; \"Stiffness Matrix K:\\n\";\n        K.print_matrix(true);\n        std::cout &lt;&lt; \"Is symmetric: \" &lt;&lt; (K.is_symmetric(1e-6f) ? \"Yes\" : \"No\") &lt;&lt; \"\\n\";\n\n        // Quick frequency identification using power iteration\n        std::cout &lt;&lt; \"\\n[Quick Analysis] Primary frequency using power_iteration():\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected primary eigenvalue: ~3.53 (largest eigenvalue)\\n\";\n        std::cout &lt;&lt; \"  Expected primary frequency: sqrt(3.53) \u2248 1.88 rad/s\\n\";\n\n        tiny::Mat::EigenPair primary = K.power_iteration(500, 1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"  Primary eigenvalue: \" &lt;&lt; primary.eigenvalue &lt;&lt; \" (Expected: ~3.53)\\n\";\n        std::cout &lt;&lt; \"  Primary frequency: \" &lt;&lt; sqrtf(primary.eigenvalue) &lt;&lt; \" rad/s (Expected: ~1.88 rad/s)\\n\";\n        std::cout &lt;&lt; \"  Iterations: \" &lt;&lt; primary.iterations &lt;&lt; \"\\n\";\n        float error_primary = fabsf(primary.eigenvalue - 3.53f);\n        std::cout &lt;&lt; \"  Error from expected: \" &lt;&lt; error_primary &lt;&lt; (error_primary &lt; 0.2f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n\n        // Complete modal analysis using Jacobi\n        std::cout &lt;&lt; \"\\n[Complete Analysis] Full modal analysis using eigendecompose_jacobi():\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues (approximate): 3.53, 2.35, 1.00, 0.12\\n\";\n        std::cout &lt;&lt; \"  Expected natural frequencies: 1.88, 1.53, 1.00, 0.35 rad/s\\n\";\n        std::cout &lt;&lt; \"  Note: These are approximate values for the 4-DOF system\\n\";\n\n        tiny::Mat::EigenDecomposition modal = K.eigendecompose_jacobi(1e-5f, 100);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"All eigenvalues (natural frequencies squared):\\n\";\n        modal.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Natural frequencies (rad/s):\\n\";\n        float expected_freqs_4dof[4] = {1.88f, 1.53f, 1.00f, 0.35f};\n        for (int i = 0; i &lt; modal.eigenvalues.row; ++i)\n        {\n            float freq = sqrtf(modal.eigenvalues(i, 0));\n            std::cout &lt;&lt; \"  Mode \" &lt;&lt; i &lt;&lt; \": \" &lt;&lt; freq &lt;&lt; \" rad/s\";\n            // Check if frequency matches any expected value\n            bool matched = false;\n            for (int j = 0; j &lt; 4; ++j)\n            {\n                if (fabsf(freq - expected_freqs_4dof[j]) &lt; 0.15f)\n                {\n                    std::cout &lt;&lt; \" (Expected: ~\" &lt;&lt; expected_freqs_4dof[j] &lt;&lt; \" rad/s) [PASS]\";\n                    matched = true;\n                    break;\n                }\n            }\n            if (!matched) std::cout &lt;&lt; \" [CHECK]\";\n            std::cout &lt;&lt; \"\\n\";\n        }\n        std::cout &lt;&lt; \"Mode shapes (eigenvectors):\\n\";\n        modal.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Total iterations: \" &lt;&lt; modal.iterations &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.7: Edge Cases and Error Handling\n    std::cout &lt;&lt; \"\\n[Test 9.7] Edge Cases and Error Handling\\n\";\n\n    // Test 9.7.1: 1x1 matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.7.1] 1x1 Matrix\\n\";\n        tiny::Mat mat1x1(1, 1);\n        mat1x1(0, 0) = 5.0f;\n        std::cout &lt;&lt; \"Matrix: [5.0]\\n\";\n        std::cout &lt;&lt; \"[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalue: 5.0 (the matrix element itself)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvector: [1.0] (normalized)\\n\";\n\n        tiny::Mat::EigenDecomposition result_1x1 = mat1x1.eigendecompose(1e-6f);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalue: \" &lt;&lt; result_1x1.eigenvalues(0, 0) &lt;&lt; \" (Expected: 5.0)\\n\";\n        std::cout &lt;&lt; \"Eigenvector:\\n\";\n        result_1x1.eigenvectors.print_matrix(true);\n        float error = fabsf(result_1x1.eigenvalues(0, 0) - 5.0f);\n        std::cout &lt;&lt; \"Error from expected: \" &lt;&lt; error &lt;&lt; (error &lt; 0.01f ? \" [PASS]\" : \" [FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.7.2: Zero matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.7.2] Zero Matrix\\n\";\n        tiny::Mat zero_mat(3, 3);\n        zero_mat.clear();\n        tiny::Mat::EigenPair result_zero = zero_mat.power_iteration(100, 1e-6f);\n        std::cout &lt;&lt; \"Status: \" &lt;&lt; (result_zero.status == TINY_OK ? \"OK\" : \"Error (Expected)\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.7.3: Identity matrix\n    {\n        std::cout &lt;&lt; \"\\n[Test 9.7.3] Identity Matrix\\n\";\n        tiny::Mat I = tiny::Mat::eye(3);\n        std::cout &lt;&lt; \"Matrix (3x3 Identity):\\n\";\n        I.print_matrix(true);\n        std::cout &lt;&lt; \"\\n[Expected Results]\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvalues: 1.0, 1.0, 1.0 (all eigenvalues are 1)\\n\";\n        std::cout &lt;&lt; \"  Expected eigenvectors: Any orthonormal basis (e.g., standard basis vectors)\\n\";\n        std::cout &lt;&lt; \"  Expected iterations: 1 (should converge immediately)\\n\";\n\n        tiny::Mat::EigenDecomposition result_I = I.eigendecompose_jacobi(1e-6f, 10);\n        std::cout &lt;&lt; \"\\n[Actual Results]\\n\";\n        std::cout &lt;&lt; \"Eigenvalues (should all be 1.0):\\n\";\n        result_I.eigenvalues.print_matrix(true);\n        std::cout &lt;&lt; \"Eigenvectors:\\n\";\n        result_I.eigenvectors.print_matrix(true);\n        std::cout &lt;&lt; \"Iterations: \" &lt;&lt; result_I.iterations &lt;&lt; \" (Expected: 1)\\n\";\n\n        // Check all eigenvalues are 1.0\n        bool all_one = true;\n        for (int i = 0; i &lt; result_I.eigenvalues.row; ++i)\n        {\n            if (fabsf(result_I.eigenvalues(i, 0) - 1.0f) &gt; 0.01f)\n            {\n                all_one = false;\n                break;\n            }\n        }\n        std::cout &lt;&lt; \"All eigenvalues = 1.0: \" &lt;&lt; (all_one ? \"[PASS]\" : \"[FAIL]\") &lt;&lt; \"\\n\";\n    }\n\n    // Test 9.8: Performance Test for SHM Applications\n    std::cout &lt;&lt; \"\\n[Test 9.8] Performance Test for SHM Applications\\n\";\n\n    // Test 9.8.1: Power iteration performance (fast method for dominant eigenvalue)\n    std::cout &lt;&lt; \"\\n[Test 9.8.1] Power Iteration Performance (Real-time SHM - Dominant Eigenvalue)\\n\";\n    TIME_OPERATION(\n        tiny::Mat::EigenPair perf_result = stiffness.power_iteration(500, 1e-6f);\n        (void)perf_result;\n    , \"Power Iteration (3x3 matrix)\");\n\n    // Test 9.8.2: Inverse power iteration performance (system identification - smallest eigenvalue)\n    std::cout &lt;&lt; \"\\n[Test 9.8.2] Inverse Power Iteration Performance (System Identification - Smallest Eigenvalue)\\n\";\n    TIME_OPERATION(\n        tiny::Mat::EigenPair perf_inv_result = stiffness.inverse_power_iteration(500, 1e-6f);\n        (void)perf_inv_result;\n    , \"Inverse Power Iteration (3x3 matrix)\");\n\n    // Test 9.8.3: Jacobi method performance (complete eigendecomposition for symmetric matrices)\n    std::cout &lt;&lt; \"\\n[Test 9.8.3] Jacobi Method Performance (Complete Eigendecomposition - Symmetric Matrices)\\n\";\n    TIME_OPERATION(\n        tiny::Mat::EigenDecomposition perf_jacobi = stiffness.eigendecompose_jacobi(1e-5f, 100);\n        (void)perf_jacobi;\n    , \"Jacobi Decomposition (3x3 symmetric matrix)\");\n\n    // Test 9.8.4: QR method performance (complete eigendecomposition for general matrices)\n    std::cout &lt;&lt; \"\\n[Test 9.8.4] QR Method Performance (Complete Eigendecomposition - General Matrices)\\n\";\n    TIME_OPERATION(\n        tiny::Mat::EigenDecomposition perf_qr = non_sym_mat.eigendecompose_qr(100, 1e-5f);\n        (void)perf_qr;\n    , \"QR Decomposition (3x3 general matrix)\");\n\n    std::cout &lt;&lt; \"\\n[Eigenvalue Decomposition Tests Complete]\\n\";\n}\n\nvoid tiny_matrix_test()\n{\n    std::cout &lt;&lt; \"============ [tiny_matrix_test start] ============\\n\";\n    std::cout &lt;&lt; \"\\n[Test Organization: Application-Oriented Logic]\\n\";\n    std::cout &lt;&lt; \"  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\\n\\n\";\n\n    // ========================================================================\n    // Phase 1: Object Foundation (Groups 1-3)\n    // ========================================================================\n    // Purpose: Learn to create and manipulate matrix objects\n    // Group 1: Constructor &amp; Destructor\n    test_constructor_destructor();\n\n    // Group 2: Element Access\n    test_element_access();\n\n    // Group 3: ROI Operations\n    test_roi_operations();\n\n    // ========================================================================\n    // Phase 2: Basic Operations (Group 4)\n    // ========================================================================\n    // Purpose: Learn basic arithmetic operations\n    // Group 4: Arithmetic Operators\n    test_assignment_operator();\n    test_matrix_addition();\n    test_constant_addition();\n    test_matrix_subtraction();\n    test_constant_subtraction();\n    test_matrix_division();\n    test_constant_division();\n    test_matrix_exponentiation();\n\n    // ========================================================================\n    // Phase 3: Matrix Properties (Group 5)\n    // ========================================================================\n    // Purpose: Understand matrix properties and basic linear algebra\n    // Group 5: Matrix Properties\n    test_matrix_transpose();\n    test_matrix_cofactor();\n    test_matrix_determinant();\n    test_matrix_adjoint();\n    test_matrix_normalize();\n    test_matrix_norm();\n    test_inverse_adjoint_adjoint();\n    test_matrix_utilities();\n\n    // ========================================================================\n    // Phase 4: Linear System Solving (Group 6)\n    // ========================================================================\n    // Purpose: Core application - solving linear systems Ax = b\n    // Group 6: Linear System Solving\n    test_gaussian_eliminate();\n    test_row_reduce_from_gaussian();\n    test_inverse_gje();\n    test_dotprod();\n    test_solve();\n    test_band_solve();\n    test_roots();\n\n    // ========================================================================\n    // Phase 5: Advanced Linear Algebra (Groups 7-8)\n    // ========================================================================\n    // Purpose: Advanced linear algebra operations for stable and efficient solving\n    // Group 7: Matrix Decomposition\n    test_matrix_decomposition();\n\n    // Group 8: Gram-Schmidt Orthogonalization\n    test_gram_schmidt_orthogonalize();\n\n    // ========================================================================\n    // Phase 6: System Identification Applications (Group 9)\n    // ========================================================================\n    // Purpose: Eigenvalue decomposition for SHM and modal analysis\n    // Group 9: Eigenvalue Decomposition\n    test_eigenvalue_decomposition();\n\n    // ========================================================================\n    // Phase 7: Auxiliary Functions (Groups 10-11)\n    // ========================================================================\n    // Purpose: Convenience functions and I/O operations\n    // Group 10: Stream Operators\n    test_stream_operators();\n\n    // Group 11: Global Arithmetic Operators\n    test_matrix_operations();\n\n    // ========================================================================\n    // Phase 8: Quality Assurance (Groups 12-14)\n    // ========================================================================\n    // Purpose: Ensure robustness, performance, and correctness\n    // Group 12: Boundary Conditions and Error Handling\n    test_boundary_conditions();\n\n    // Group 13: Performance Benchmarks\n    test_performance_benchmarks();\n\n    // Group 14: Memory Layout\n    test_memory_layout();\n\n    std::cout &lt;&lt; \"============ [tiny_matrix_test end] ============\\n\";\n\n    // Remove current task from watchdog after all tests complete\n    // This prevents watchdog timeout after app_main() returns\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (task_wdt_added)\n    {\n        esp_task_wdt_delete(NULL);  // Remove current task from watchdog\n        task_wdt_added = false;\n    }\n#endif\n}\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#test-outputs","title":"TEST OUTPUTS","text":""},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-i-object-foundation-groups-1-3","title":"PHASE I: Object Foundation (Groups 1-3)","text":"<p>Purpose: Learn to create and manipulate matrix objects</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 1: Object Foundation - Constructor &amp; Destructor Tests]\n[Test 1.1] Default Constructor\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9a78\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.2] Constructor with Rows and Cols\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |\n           0            0            0            0       |\n           0            0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.3] Constructor with Rows, Cols and Stride\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9ad0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.4] Constructor with External Data\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fc9928c\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |\n           4            5            6            7       |\n           8            9           10           11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.5] Constructor with External Data and Stride\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc992e0\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.6] Copy Constructor\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9bd8\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 2: Object Foundation - Element Access Tests]\n[Test 2.1] Non-const Access\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            3\nelements        6\npaddings        0\nstride          3\nmemory          6\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         1.1          2.2          3.3       |\n         4.4          5.5          6.6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 2.2] Const Access\nconst_mat(0, 0): 1.1\n\n[Group 3: Object Foundation - Data Manipulation Tests (ROI Operations)]\n[Material Matrices]\nmatA:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            3\nelements        6\npaddings        0\nstride          3\nmemory          6\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2          0.3       |\n         0.4          0.5          0.6       |\n&lt;&lt;&lt; Matrix Elements\n\nmatB:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatC:\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9a78\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.1] Copy ROI - Over Range Case\n[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid column position \nmatB after copy_paste matA at (1, 2):\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nnothing changed.\n[Test 3.1] Copy ROI - Suitable Range Case\nmatB after copy_paste matA at (1, 1):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nsuccessfully copied.\n[Test 3.2] Copy Head\nmatC after copy_head matB:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.2] Copy Head - Memory Sharing Check\nmatB(0, 0) = 99.99f\nmatC:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       99.99            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.3] Get a View of ROI - Low Level Function\nget a view of ROI with overrange dimensions - rows:\n[Error] Invalid ROI request.\nget a view of ROI with overrange dimensions - cols:\n[Error] Invalid ROI request.\nget a view of ROI with suitable dimensions:\nroi3:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991dc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |    0.3            0            8 \n         0.4          0.5       |    0.6            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.4] Get a View of ROI - Using ROI Structure\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991dc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |    0.3            0            8 \n         0.4          0.5       |    0.6            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.5] Copy ROI - Low Level Function\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9bfc\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.6] Copy ROI - Using ROI Structure\ntime for copy_roi using ROI structure: 28 ms\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9c10\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.7] Block\ntime for block: 34 ms\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9c24\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.8] Swap Rows\nmatB before swap rows:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       99.99            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after swap_rows(0, 2):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           8          0.4          0.5          0.6       |      0 \n           4          0.1          0.2          0.3       |      0 \n       99.99            1            2            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.9] Swap Columns\nmatB before swap columns:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           8          0.4          0.5          0.6       |      0 \n           4          0.1          0.2          0.3       |      0 \n       99.99            1            2            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after swap_cols(0, 2):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.5          0.4            8          0.6       |      0 \n         0.2          0.1            4          0.3       |      0 \n           2            1        99.99            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.10] Clear\nmatB before clear:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.5          0.4            8          0.6       |      0 \n         0.2          0.1            4          0.3       |      0 \n           2            1        99.99            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after clear:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-ii-basic-operations-group-4","title":"PHASE II: Basic Operations (Group 4)","text":"<p>Purpose: Learn basic arithmetic operations</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 4.1: Basic Operations - Assignment Operator Tests]\n\n[Test 4.1.1] Assignment (Same Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.2] Assignment (Different Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.3] Assignment to Sub-Matrix (Expect Error)\n[Error] Assignment to a sub-matrix is not allowed.\nMatrix Elements &gt;&gt;&gt;\n           5            6       |      7            0            8 \n           9           10       |     11            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.4] Self-Assignment\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.2: Matrix Addition Tests]\n\n[Test 4.2.1] Matrix Addition (Same Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           2            3            4       |\n           5            6            7       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.2] Sub-Matrix Addition\nMatrix Elements &gt;&gt;&gt;\n          10           12       |      7            0            8 \n          18           20       |     11            0           12 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.3] Full Matrix + Sub-Matrix Addition\nMatrix Elements &gt;&gt;&gt;\n          12           14       |\n          20           22       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.4] Addition Dimension Mismatch (Expect Error)\n[Error] Matrix addition failed: Dimension mismatch (2x2 vs 3x3)\n\n[Group 4.3: Constant Addition Tests]\n\n[Test 4.3.1] Full Matrix + Constant\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.2] Sub-Matrix + Constant\nMatrix Elements &gt;&gt;&gt;\n           8            9       |      7            0            8 \n          12           13       |     11            0           12 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.3] Add Zero\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.4] Add Negative Constant\nMatrix Elements &gt;&gt;&gt;\n          -5            5       |\n          15           25       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.4: Matrix Subtraction Tests]\n\n[Test 4.4.1] Matrix Subtraction\nMatrix Elements &gt;&gt;&gt;\n           4            5       |\n           6            7       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.4.2] Subtraction Dimension Mismatch (Expect Error)\n[Error] Matrix subtraction failed: Dimension mismatch (2x2 vs 3x3)\n\n[Group 4.5: Constant Subtraction Tests]\n\n[Test 4.5.1] Full Matrix - Constant\nMatrix Elements &gt;&gt;&gt;\n          -1            0            1       |\n           2            3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.5.2] Sub-Matrix - Constant\nMatrix Elements &gt;&gt;&gt;\n         3.5          4.5       |      7            0            8 \n         7.5          8.5       |     11            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.6: Matrix Element-wise Division Tests]\n\n[Test 4.6.1] Element-wise Division (Same Dimensions, No Zero)\nMatrix Elements &gt;&gt;&gt;\n           5            5       |\n           6            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.6.2] Dimension Mismatch (Expect Error)\n[Error] Matrix division failed: Dimension mismatch (2x2 vs 3x3)\n\n[Test 4.6.3] Division by Matrix Containing Zero (Expect Error)\n[Error] Matrix division failed: Division by zero detected.\nMatrix Elements &gt;&gt;&gt;\n           5           10       |\n          15           20       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.7: Matrix Division by Constant Tests]\n\n[Test 4.7.1] Divide Full Matrix by Positive Constant\nMatrix Elements &gt;&gt;&gt;\n           1          1.5            2       |\n         2.5            3          3.5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.7.2] Divide Matrix by Negative Constant\nMatrix Elements &gt;&gt;&gt;\n          -2           -4       |\n          -6           -8       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.7.3] Division by Zero Constant (Expect Error)\n[Error] Matrix division by zero is undefined.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.8: Matrix Exponentiation Tests]\n\n[Test 4.8.1] Raise Each Element to Power of 2\nMatrix Elements &gt;&gt;&gt;\n           4            9       |\n          16           25       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.2] Raise Each Element to Power of 0\nMatrix Elements &gt;&gt;&gt;\n           1            1       |\n           1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.3] Raise Each Element to Power of 1\nMatrix Elements &gt;&gt;&gt;\n           9            8       |\n           7            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.4] Raise Each Element to Power of -1 (Expect Error or Warning)\n[Error] Negative exponent not supported in operator^.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.5] Raise Matrix Containing Zero to Power of 3\nMatrix Elements &gt;&gt;&gt;\n           0            8       |\n          -1           27       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-iii-matrix-properties-group-5","title":"PHASE III: Matrix Properties (Group 5)","text":"<p>Purpose: Understand matrix properties and basic linear algebra</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 5.1: Matrix Properties - Matrix Transpose Tests]\n\n[Test 5.1.1] Transpose of 2x3 Matrix\nOriginal 2x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nTransposed 3x2 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            4       |\n           2            5       |\n           3            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.2] Transpose of 3x3 Square Matrix\nOriginal 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nTransposed 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            4            7       |\n           2            5            8       |\n           3            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.3] Transpose of Matrix with Padding\nOriginal 4x2 Matrix (with padding):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |      0 \n           3            4       |      0 \n           5            6       |      0 \n           7            8       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nTransposed 2x4 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            3            5            7       |\n           2            4            6            8       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.4] Transpose of Empty Matrix\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.2: Matrix Minor and Cofactor Tests]\n\n[Test 5.2.1] Minor of 3x3 Matrix (Remove Row 1, Col 1)\nOriginal 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nMinor Matrix (remove row 1, col 1, no sign):\nMatrix Elements &gt;&gt;&gt;\n           1            3       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.2] Cofactor of 3x3 Matrix (Remove Row 1, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      The sign (-1)^(i+j) is applied when computing cofactor value, not to matrix elements.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           1            3       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.3] Minor (Remove Row 0, Col 0)\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.4] Cofactor (Remove Row 0, Col 0)\nNote: Cofactor matrix is the same as minor matrix.\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.5] Cofactor (Remove Row 0, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      When computing cofactor value, sign (-1)^(0+1) = -1 would be applied.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           4            6       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.6] Minor (Remove Row 2, Col 2)\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.7] Cofactor (Remove Row 2, Col 2)\nNote: Cofactor matrix is the same as minor matrix.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.8] Minor of 4x4 Matrix (Remove Row 2, Col 1)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3            4       |\n           5            6            7            8       |\n           9           10           11           12       |\n          13           14           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\nMinor Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            3            4       |\n           5            7            8       |\n          13           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.9] Cofactor of 4x4 Matrix (Remove Row 2, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      When computing cofactor value, sign (-1)^(2+1) = -1 would be applied.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           1            3            4       |\n           5            7            8       |\n          13           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.10] Non-square Matrix (Expect Error)\nTesting minor():\n[Error] Minor requires square matrix.\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nTesting cofactor():\n[Error] Minor requires square matrix.\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.3: Matrix Determinant Tests]\n\n[Test 5.3.1] 1x1 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: 7  (Expected: 7)\n\n[Test 5.3.2] 2x2 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           3            8       |\n           4            6       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: -14  (Expected: -14)\n\n[Test 5.3.3] 3x3 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            4            5       |\n           1            0            6       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: 22  (Expected: 22)\n\n[Test 5.3.4] 4x4 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3            4       |\n           5            6            7            8       |\n           9           10           11           12       |\n          13           14           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\nNote: This matrix has linearly dependent rows (each row differs by constant 4),\n      so the determinant should be 0.\nDeterminant: 0  (Expected: 0)\n\n[Test 5.3.5] 5x5 Matrix Determinant (Tests Auto-select to LU Method)\nMatrix (5x5, tridiagonal):\nMatrix Elements &gt;&gt;&gt;\n           2            1            0            0            0       |\n           1            2            1            0            0       |\n           0            1            2            1            0       |\n           0            0            1            2            1       |\n           0            0            0            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant (auto-select, should use LU for n &gt; 4): 6\nNote: For n = 5 &gt; 4, auto-select should use LU decomposition (O(n\u00b3)).\n\n[Test 5.3.6] Non-square Matrix (Expect Error)\nMatrix (3x4, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |\n           0            0            0            0       |\n           0            0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Determinant requires a square matrix.\nDeterminant: 0  (Expected: 0 with error message)\n\n[Test 5.3.7] Comparison of Different Methods (5x5 Matrix)\nMatrix (5x5):\nMatrix Elements &gt;&gt;&gt;\n           2            2            3            4            5       |\n           2            5            6            8           10       |\n           3            6           10           12           15       |\n           4            8           12           17           20       |\n           5           10           15           20           26       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant (auto-select): 56  (should use LU for n &gt; 4)\nDeterminant (Laplace):     56  (O(n!), slow for n=5)\nDeterminant (LU):          56  (O(n\u00b3), efficient)\nDeterminant (Gaussian):    56  (O(n\u00b3), efficient)\nNote: All methods should give the same result (within numerical precision).\n      Auto-select should use LU for n &gt; 4, avoiding slow Laplace expansion.\n\n[Test 5.3.8] Large Matrix (6x6) - Tests Efficient Methods\nMatrix (6x6, showing first 4x4 block):\n       1.5          2          3          4 ...\n         2        4.5          6          8 ...\n         3          6        9.5         12 ...\n         4          8         12       16.5 ...\n...\nDeterminant (auto-select, uses LU): 2.85938\nDeterminant (LU):                   2.85938\nDeterminant (Gaussian):             2.85938\nNote: For n &gt; 4, auto-select uses LU decomposition (O(n\u00b3) instead of O(n!)).\n\n[Test 5.3.9] Large Matrix (8x8) - Performance Comparison\nMatrix (8x8, showing first 4x4 block):\n         1          2          3          4 ...\n         2          4          6          8 ...\n         3          6          9         12 ...\n         4          8         12         16 ...\n...\n[Error] LU decomposition: Matrix is singular or near-singular.\nDeterminant (LU):       0\nDeterminant (Gaussian): 0\nNote: Both methods are O(n\u00b3) and should be much faster than Laplace expansion.\n\n[Group 5.4: Matrix Adjoint Tests]\n\n[Test 5.4.1] Adjoint of 1x1 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.2] Adjoint of 2x2 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n           4            2       |\n           3            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.3] Adjoint of 3x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            4            5       |\n           1            0            6       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n          24           12           -2       |\n          -5            3            5       |\n          -4           -2            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.4] Adjoint of Non-Square Matrix (Expect Error)\nOriginal Matrix (2x3, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Adjoint requires a square matrix.\nAdjoint Matrix (should be empty due to error):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.5: Matrix Normalization Tests]\n\n[Test 5.5.1] Normalize a Standard 2x2 Matrix\nBefore normalization:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter normalization (Expected L2 norm = 1):\nMatrix Elements &gt;&gt;&gt;\n    0.424264     0.565685       |\n    0.424264     0.565685       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.5.2] Normalize a 2x2 Matrix with Stride=4 (Padding Test)\nBefore normalization:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |      0            0 \n           3            4       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\nAfter normalization:\nMatrix Elements &gt;&gt;&gt;\n    0.424264     0.565685       |      0            0 \n    0.424264     0.565685       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.5.3] Normalize a Zero Matrix (Expect Warning)\nMatrix Elements &gt;&gt;&gt;\n           0            0       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.6: Matrix Norm Calculation Tests]\n\n[Test 5.6.1] 2x2 Matrix Norm (Expect 5.0)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5\n\n[Test 5.6.2] Zero Matrix Norm (Expect 0.0)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 0\n\n[Test 5.6.3] Matrix with Negative Values\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n          -1           -2       |\n          -3           -4       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5.47723  (Expect sqrt(30) \u2248 5.477)\n\n[Test 5.6.4] 2x2 Matrix with Stride=4 (Padding Test)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |      0            0 \n           3            4       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5.47723  (Expect sqrt(30) \u2248 5.477)\n\n[Group 5.7: Matrix Inversion Tests]\n\n[Test 5.7.1] Inverse of 2x2 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           4            7       |\n           2            6       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse Matrix:\nMatrix Elements &gt;&gt;&gt;\n         0.6          0.7       |\n         0.2          0.4       |\n&lt;&lt;&lt; Matrix Elements\n\nExpected Approx:\n[ 0.6  -0.7 ]\n[ -0.2  0.4 ]\n\n[Test 5.7.2] Singular Matrix (Expect Error)\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nNote: This matrix is singular (determinant = 0), so inverse should fail.\n[Error] Matrix is singular, cannot compute inverse.\nInverse Matrix (Should be zero matrix):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.7.3] Inverse of 3x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           3            0            2       |\n           2            0           -2       |\n           0            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse Matrix:\nMatrix Elements &gt;&gt;&gt;\n         0.2         -0.2           -0       |\n         0.2          0.3           -1       |\n         0.2          0.3            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.7.4] Non-Square Matrix (Expect Error)\nOriginal Matrix (2x3, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Determinant requires a square matrix.\n[Error] Matrix is singular, cannot compute inverse.\nInverse Matrix (should be empty due to error):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.8: Matrix Utilities Tests]\n\n[Test 5.8.1] Generate Identity Matrix (eye)\n3x3 Identity Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n5x5 Identity Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0            0            0       |\n           0            1            0            0            0       |\n           0            0            1            0            0       |\n           0            0            0            1            0       |\n           0            0            0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.2] Generate Ones Matrix\n3x4 Ones Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n4x4 Ones Matrix (Square):\nMatrix Elements &gt;&gt;&gt;\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.3] Augment Two Matrices Horizontally [A | B]\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\nAugmented Matrix [A | B]:\nMatrix Elements &gt;&gt;&gt;\n           1            2            5            6            7       |\n           3            4            8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.4] Augment with Row Mismatch (Expect Error)\n[Error] Cannot augment matrices: Row counts do not match (2 vs 3)\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9cf0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\n\n[Test 5.8.5] Vertically Stack Two Matrices [A; B]\nMatrix A (top):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (bottom):\nMatrix Elements &gt;&gt;&gt;\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nVertically Stacked Matrix [A; B]:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nExpected: 4x3 matrix with A on top, B on bottom\n\n[Test 5.8.6] Vertical Stack with Different Row Counts (Same Columns)\nMatrix A (1x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (3x3):\nMatrix Elements &gt;&gt;&gt;\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nVertically Stacked Matrix [A; B] (1x3 + 3x3 = 4x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.7] VStack with Column Mismatch (Expect Error)\nMatrix A (2x2):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (2x3, different columns):\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Cannot vstack matrices: Column counts do not match (2 vs 3)\nResult (should be empty due to error):\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9db0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-iv-linear-system-solving-group-6","title":"PHASE IV: Linear System Solving (Group 6)","text":"<p>Purpose: Core application - solving linear systems Ax = b</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 5.9: Gaussian Elimination Tests]\n\n[Test 5.9.1] 3x3 Matrix (Simple Upper Triangular)\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           2            1           -1       |\n          -3           -1            2       |\n          -2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should be upper triangular):\nMatrix Elements &gt;&gt;&gt;\n           2            1           -1       |\n           0          0.5          0.5       |\n           0            0           -1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.2] 3x4 Augmented Matrix (Linear System Ax = b)\nOriginal Augmented Matrix [A | b]:\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1            8       |\n          -3           -1            2          -11       |\n          -2            1            2           -3       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Row Echelon Form):\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1            8       |\n           0            5           -1           13       |\n           0            0            1            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.3] Singular Matrix (No Unique Solution)\nOriginal Singular Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should show rows of zeros):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.4] Zero Matrix\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should be a zero matrix):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.10: Row Reduce from Gaussian (RREF) Tests]\n\n[Test 5.10.1] 3x4 Augmented Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1           -4       |\n           2            3           -1          -11       |\n          -2            0           -3           22       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0           -8       |\n           0            1            0            1       |\n           0            0            1           -2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.10.2] 2x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0           -1       |\n           0            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.10.3] Already Reduced Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            2       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0            2       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.11: Gaussian Inverse Tests]\n\n[Test 5.11.1] 2x2 Matrix Inverse\nOriginal matrix (mat1):\nMatrix Elements &gt;&gt;&gt;\n           4            7       |\n           2            6       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (mat1):\nMatrix Elements &gt;&gt;&gt;\n         0.6         -0.7       |\n        -0.2          0.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.2] Identity Matrix Inverse\nOriginal matrix (Identity):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (Identity):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.3] Singular Matrix (Expected: No Inverse)\nOriginal matrix (singular):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Matrix is singular, cannot compute inverse.\nInverse matrix (singular):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.4] 3x3 Matrix Inverse\nOriginal matrix (mat4):\nMatrix Elements &gt;&gt;&gt;\n           4            7            2       |\n           3            5            1       |\n           8            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (mat4):\nMatrix Elements &gt;&gt;&gt;\n    -1.85714      2.42857     0.142857       |\n    0.904762    -0.952381   -0.0952381       |\n     1.04762     -1.52381     0.047619       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.5] Non-square Matrix Inverse (Expected Error)\nOriginal matrix (non-square):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Inversion requires a square matrix.\nInverse matrix (non-square):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.12: Dot Product Tests]\n\n[Test 5.12.1] Valid Dot Product (Same Length Vectors)\nVector A:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector B:\nMatrix Elements &gt;&gt;&gt;\n           4       |\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nDot product of vectorA and vectorB: 32\n\n[Test 5.12.2] Invalid Dot Product (Dimension Mismatch)\nVector A (3x1):\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector C (2x1, different size):\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Dot product requires matrices of the same size.\nDot product (dimension mismatch): 0\n\n[Test 5.12.3] Dot Product of Zero Vectors\nZero Vector A:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nZero Vector B:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nDot product of zero vectors: 0\n\n[Group 5.13: Solve Linear System Tests]\n\n[Test 5.13.1] Solving a Simple 2x2 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         1.8       |\n         1.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.2] Solving a 3x3 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            1       |\n           2            0            3       |\n           3            2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           8       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n          -1       |\n     3.33333       |\n     3.33333       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.3] Solving a System Where One Row is All Zeros (Expect Failure or Infinite Solutions)\nMatrix A (has zero row):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            0            0       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           0       |\n          15       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.4] Solving a System with Zero Determinant (Singular Matrix)\nMatrix A (singular, determinant = 0):\nMatrix Elements &gt;&gt;&gt;\n           2            4            1       |\n           1            2            3       |\n           3            6            2       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.5] Solving a System with Linearly Dependent Rows (Expect Failure or Infinite Solutions)\nMatrix A (all rows linearly dependent):\nMatrix Elements &gt;&gt;&gt;\n           1            1            1       |\n           2            2            2       |\n           3            3            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           6       |\n          12       |\n          18       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.6] Solving a Larger 4x4 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           4            2            3            1       |\n           2            5            1            2       |\n           3            1            6            3       |\n           1            2            3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          12       |\n          14       |\n          16       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n     1.80645       |\n    0.258065       |\n   -0.516129       |\n     3.80645       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.14: Band Solve Tests]\n\n[Test 5.14.1] Simple 3x3 Band Matrix\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            0       |\n           1            3            2       |\n           0            1            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         2.5       |\n           0       |\n        1.75       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.2] 4x4 Band Matrix\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            0            0       |\n           1            3            2            0       |\n           0            1            4            2       |\n           0            0            1            5       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           8       |\n           9       |\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n     3.51429       |\n    0.971429       |\n     1.28571       |\n     1.94286       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.3] Incompatible Dimensions (Expect Error)\nMatrix A (3x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b (2x1, incompatible):\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Matrix dimensions are not compatible for solving.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.4] Singular Matrix (No Unique Solution)\nMatrix A (singular, linearly dependent rows):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           2            4            6       |\n           3            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          20       |\n          30       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Zero pivot detected in bandSolve. Cannot proceed.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.15: Roots Tests]\n\n[Test 5.15.1] Solving a Simple 2x2 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         1.8       |\n         1.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.2] Solving a 3x3 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            1       |\n           2            0            3       |\n           3            2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           8       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n          -1       |\n     3.33333       |\n     3.33333       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.3] Singular Matrix (No Unique Solution)\nMatrix A (singular, linearly dependent rows):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, system may have no solution.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.4] Incompatible Dimensions (Expect Error)\nMatrix A (3x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b (2x1, incompatible):\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Cannot augment matrices: Row counts do not match (3 vs 2)\n[Error] Pivot is zero, system may have no solution.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-v-advanced-linear-algebra-groups-7-8","title":"PHASE V: Advanced Linear Algebra (Groups 7-8)","text":"<p>Purpose: Advanced linear algebra operations for stable and efficient solving</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 7: Advanced Linear Algebra - Matrix Decomposition Tests]\n\n[Test 7.1] is_positive_definite() - Basic Functionality\n\n[Test 7.1.1] Positive Definite 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           4            1            0       |\n           1            3            0       |\n           0            0            2       |\n&lt;&lt;&lt; Matrix Elements\n\nIs positive definite: True (Expected: True) [PASS]\n\n[Test 7.1.2] Non-Positive Definite Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nIs positive definite: False (Expected: False) [PASS]\n\n[Test 7.2] LU Decomposition\n\n[Test 7.2.1] 3x3 Matrix - LU Decomposition with Pivoting\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            1       |\n           4            3            3       |\n           2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nL matrix (lower triangular):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n         0.5            1            0       |\n         0.5            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\nU matrix (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n           4            3            3       |\n           0         -0.5         -0.5       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\nP matrix (permutation):\nMatrix Elements &gt;&gt;&gt;\n           0            1            0       |\n           1            0            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] P * A should equal L * U\nTotal difference: 0 [PASS]\n\n[Test 7.2.2] Solve Linear System using LU Decomposition\nSystem: A * x = b\nA:\nMatrix Elements &gt;&gt;&gt;\n           2            1            1       |\n           4            3            3       |\n           2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nb:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         0.5       |\n          -2       |\n           2       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error: 0 [PASS]\n\n[Test 7.3] Cholesky Decomposition\n\n[Test 7.3.1] SPD Matrix - Cholesky Decomposition\nMatrix A (SPD):\nMatrix Elements &gt;&gt;&gt;\n           4            2            0       |\n           2            5            1       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nL matrix (lower triangular):\nMatrix Elements &gt;&gt;&gt;\n           2            0            0       |\n           1            2            0       |\n           0          0.5      1.65831       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] L * L^T should equal A\nTotal difference: 2.38419e-07 [PASS]\n\n[Test 7.3.2] Solve Linear System using Cholesky Decomposition\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n    0.272727       |\n    0.454545       |\n    0.181818       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error: 0 [PASS]\n\n[Test 7.4] QR Decomposition\n\n[Test 7.4.1] General 3x3 Matrix - QR Decomposition\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nQ matrix (orthogonal):\nMatrix Elements &gt;&gt;&gt;\n    0.123091     0.904534     0.408248       |\n    0.492366     0.301511    -0.816497       |\n     0.86164    -0.301511     0.408248       |\n&lt;&lt;&lt; Matrix Elements\n\nR matrix (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n     8.12404      9.60114      11.0782       |\n           0     0.904534      1.80907       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Q * R should equal A\nTotal difference: 1.66893e-06 [PASS]\nQ orthogonality error: 2.83733e-07 [PASS]\n\n[Test 7.4.2] Least Squares Solution using QR Decomposition\nOverdetermined system: A * x \u2248 b\nA:\nMatrix Elements &gt;&gt;&gt;\n           1            1       |\n           1            2       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nb:\nMatrix Elements &gt;&gt;&gt;\n           2       |\n           3       |\n           4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nLeast squares solution x:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           1       |\n&lt;&lt;&lt; Matrix Elements\n\nResidual norm ||A*x - b||: 4.12953e-07\n\n[Test 7.5] Singular Value Decomposition (SVD)\n\n[Test 7.5.1] General 3x3 Matrix - SVD Decomposition\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nSingular values:\nMatrix Elements &gt;&gt;&gt;\n     1.06837       |\n     16.8481       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nNumerical rank: 2\nIterations: 7\nReconstruction error: 1.68085e-05 [PASS]\n\n[Test 7.5.2] Pseudo-inverse using SVD\nMatrix A (3x2):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n           5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nPseudo-inverse A^+ (2x3):\nMatrix Elements &gt;&gt;&gt;\n    -1.33333    -0.333333     0.666666       |\n     1.08333     0.333333    -0.416666       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error (A * A^+ * A \u2248 A): 5.72205e-06 [PASS]\n\n[Test 7.6] Matrix Decomposition Performance Tests\n\n[Test 7.6.1] LU Decomposition Performance\n[Performance] LU Decomposition (4x4 matrix): 110.00 us\n\n[Test 7.6.2] Cholesky Decomposition Performance\n[Performance] Cholesky Decomposition (4x4 SPD matrix): 62.00 us\n\n[Test 7.6.3] QR Decomposition Performance\n[Performance] QR Decomposition (4x4 matrix): 157.00 us\n\n[Test 7.6.4] SVD Decomposition Performance\n[Performance] SVD Decomposition (4x4 matrix): 288.00 us\n\n[Matrix Decomposition Tests Complete]\n\n[Group 8: Advanced Linear Algebra - Gram-Schmidt Orthogonalization Tests]\n\n[Test 8.1] Basic Orthogonalization - Linearly Independent Vectors\nInput vectors (each column is a vector):\nMatrix Elements &gt;&gt;&gt;\n        1.00         1.00         0.00       |\n        0.00         1.00         1.00       |\n        1.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q (each column is orthogonal):\nMatrix Elements &gt;&gt;&gt;\n        0.71         0.41        -0.58       |\n        0.00         0.82         0.58       |\n        0.71        -0.41         0.58       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n        1.41         0.71         0.71       |\n        0.00         1.22         0.41       |\n        0.00         0.00         1.15       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Q^T * Q should be identity\nOrthogonality error: 0.00 [PASS]\n\n[Verification] Each column of Q should be normalized\n  Column 0 norm: 1.00 (error: 0.00) [PASS]\n  Column 1 norm: 1.00 (error: 0.00) [PASS]\n  Column 2 norm: 1.00 (error: 0.00) [PASS]\n\n[Verification] Q * R should reconstruct original vectors\nReconstruction error: 0.00 [PASS]\n\n[Test 8.2] Orthogonalization - Near-Linear-Dependent Vectors\nInput vectors (third vector is nearly linear dependent):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         1.00       |\n        0.00         1.00         1.00       |\n        0.00         0.00         0.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         1.00       |\n        0.00         1.00         1.00       |\n        0.00         0.00         0.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Note] Third column norm: 1.00 (should be 0 if linearly dependent, or 1 if orthogonalized)\n\n[Test 8.3] Orthogonalization - 2D Vectors (2x2)\nInput vectors:\nMatrix Elements &gt;&gt;&gt;\n        3.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q:\nMatrix Elements &gt;&gt;&gt;\n        0.95        -0.32       |\n        0.32         0.95       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R:\nMatrix Elements &gt;&gt;&gt;\n        3.16         1.58       |\n        0.00         1.58       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Dot product of Q columns: 0.00 (should be ~0 for orthogonal) [PASS]\n\n[Test 8.4] Error Handling - Invalid Input\nEmpty matrix test: FAIL (should return false)\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-vi-system-identification-applications-group-9","title":"PHASE VI: System Identification Applications (Group 9)","text":"<p>Purpose: Eigenvalue decomposition for SHM and modal analysis</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 9: System Identification - Eigenvalue and Eigenvector Decomposition Tests]\n\n[Test 9.1] is_symmetric() - Basic Functionality\n[Test 9.1.1] Symmetric 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           4            1            2       |\n           1            3            0       |\n           2            0            5       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: True (Expected: True)\n\n[Test 9.1.2] Non-Symmetric 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: False (Expected: False)\n\n[Test 9.1.3] Non-Square Matrix (2x3)\nIs symmetric: False (Expected: False)\n\n[Test 9.1.4] Symmetric Matrix with Small Numerical Errors\nMatrix with error 1e-05:\nMatrix Elements &gt;&gt;&gt;\n           1      2.00001       |\n           2            3       |\n&lt;&lt;&lt; Matrix Elements\n\nDifference: |A(0,1) - A(1,0)| = 1.001358e-05 (Expected: 0.000010)\nA(0,1) stored value: 2.00001001 (Expected: 2.00001001)\nIs symmetric (tolerance=1e-4): True (Expected: True, tolerance &gt; error) [PASS]\nIs symmetric (tolerance=1e-6): False (Expected: False, tolerance &lt; error) [PASS]\nDifference accuracy: |actual_diff - expected_diff| = 1.36e-08 [PASS - difference stored correctly]\n\n[Test 9.2] power_iteration() - Dominant Eigenvalue\n\n[Test 9.2.1] Simple 2x2 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        2.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\n  Expected dominant eigenvector (for \u03bb=3): approximately [0.707, 0.707] or [-0.707, -0.707] (normalized)\n  Expected dominant eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\n\n[Actual Results]\n  Dominant eigenvalue: 3.00 (Expected: 3.0, largest eigenvalue)\n  Iterations: 2\n  Status: OK\n  Dominant eigenvector:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n        0.71       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (3.0): 0.00 [PASS]\n\n[Test 9.2.2] 3x3 Stiffness Matrix (SHM Application)\nStiffness Matrix:\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00       |\n       -1.00         2.00        -1.00       |\n        0.00        -1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\n  Expected primary frequency: sqrt(3.414) \u2248 1.848 rad/s\n\n[Actual Results]\n  Dominant eigenvalue (primary frequency squared): 3.41\n  Primary frequency: 1.85 rad/s (Expected: ~1.848 rad/s)\n  Iterations: 8\n  Status: OK\n  Error from expected (3.41): 0.00 [PASS]\n\n[Test 9.2.3] Non-Square Matrix (Expect Error)\n[Error] Power iteration requires a square matrix.\nStatus: Error (Expected)\n\n[Test 9.2.4] inverse_power_iteration() - Smallest Eigenvalue (System Identification)\n\n[Test 9.2.4.1] Simple 2x2 Matrix - Smallest Eigenvalue\nMatrix (same as Test 9.2.1):\nMatrix Elements &gt;&gt;&gt;\n        2.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\n  Expected smallest eigenvalue: 1.0\n  Expected smallest eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\n  Note: This is critical for system identification - smallest eigenvalue = fundamental frequency\n\n[Actual Results]\n  Smallest eigenvalue: 1.00 (Expected: 1.0, smallest eigenvalue)\n  Iterations: 6\n  Status: OK\n  Smallest eigenvector:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (1.0): 0.00 [PASS]\n\n[Comparison] Power vs Inverse Power Iteration:\n  Power iteration (\u03bb_max): 3.00\n  Inverse power iteration (\u03bb_min): 1.00\n  Ratio (\u03bb_max/\u03bb_min): 3.00 (Expected: ~3.0) [PASS]\n\n[Test 9.2.4.2] 3x3 Stiffness Matrix - Smallest Eigenvalue (SHM Application)\nStiffness Matrix (same as Test 9.2.2):\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00       |\n       -1.00         2.00        -1.00       |\n        0.00        -1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\n  Expected smallest eigenvalue: ~0.586 (fundamental frequency squared)\n  Expected fundamental frequency: sqrt(0.586) \u2248 0.765 rad/s\n  Note: Smallest eigenvalue is critical for system identification - represents fundamental mode\n\n[Actual Results]\n  Smallest eigenvalue (fundamental frequency squared): 0.59\n  Fundamental frequency: 0.77 rad/s (Expected: ~0.765 rad/s)\n  Iterations: 8\n  Status: OK\n  Smallest eigenvector (fundamental mode shape):\nMatrix Elements &gt;&gt;&gt;\n        0.50       |\n        0.71       |\n        0.50       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (0.59): 0.00 [PASS]\n\n[Comparison] Power vs Inverse Power Iteration for SHM:\n  Power iteration (primary frequency\u00b2): 3.41 \u2192 frequency: 1.85 rad/s\n  Inverse power iteration (fundamental frequency\u00b2): 0.59 \u2192 frequency: 0.77 rad/s\n  Frequency ratio: 2.41 (Expected: ~2.4, ratio of highest to lowest mode)\n\n[Test 9.2.4.3] Non-Square Matrix (Expect Error)\n[Error] Inverse power iteration requires a square matrix.\nStatus: Error (Expected)\nError handling: [PASS]\n\n[Test 9.2.4.4] Near-Singular Matrix (Edge Case)\nMatrix (near-singular but invertible):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\n  Status: OK\n  Smallest eigenvalue: 1.00\n  Iterations: 2\n  Note: Successfully handled near-singular matrix [PASS]\n\n[Test 9.3] eigendecompose_jacobi() - Symmetric Matrix Decomposition\n\n[Test 9.3.1] 2x2 Symmetric Matrix - Complete Decomposition\n[Expected Results]\n  Expected eigenvalues: 3.0, 1.0 (in any order)\n  Expected eigenvectors (for \u03bb=3): [0.707, 0.707] or [-0.707, -0.707] (normalized)\n  Expected eigenvectors (for \u03bb=1): [0.707, -0.707] or [-0.707, 0.707] (normalized)\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n        3.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors (each column is an eigenvector):\nMatrix Elements &gt;&gt;&gt;\n        0.71         0.71       |\n       -0.71         0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 2\nStatus: OK\nEigenvalue check (should be 3.0 and 1.0): [PASS]\n\n[Verification] Check A * v = lambda * v for first eigenvector:\nA * v:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nlambda * v:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification (A*v = \u03bb*v): [PASS]\n\n[Test 9.3.2] 3x3 Stiffness Matrix (SHM Application)\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414, 2.000, 0.586\n  Expected natural frequencies: 1.848, 1.414, 0.765 rad/s\n  Note: Eigenvalues may appear in any order\n\n[Actual Results]\nEigenvalues (natural frequencies squared):\nMatrix Elements &gt;&gt;&gt;\n        3.41       |\n        0.59       |\n        2.00       |\n&lt;&lt;&lt; Matrix Elements\n\nNatural frequencies (rad/s):\n  Mode 0: 1.85 rad/s (Expected: ~1.85 rad/s) [PASS]\n  Mode 1: 0.77 rad/s (Expected: ~0.76 rad/s) [PASS]\n  Mode 2: 1.41 rad/s (Expected: ~1.41 rad/s) [PASS]\nEigenvectors (mode shapes):\nMatrix Elements &gt;&gt;&gt;\n        0.50         0.50        -0.71       |\n       -0.71         0.71         0.00       |\n        0.50         0.50         0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 9\nStatus: OK\n\n[Test 9.3.3] Diagonal Matrix (Eigenvalues on diagonal)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        5.00         0.00         0.00       |\n        0.00         3.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 5.0, 3.0, 1.0 (diagonal elements, may be in any order)\n  Expected eigenvectors: standard basis vectors [1,0,0], [0,1,0], [0,0,1] (or their negatives)\n  Expected iterations: 1 (diagonal matrix should converge immediately)\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        5.00       |\n        3.00       |\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 1 (Expected: 1)\nEigenvalue check (should be 5.0, 3.0, 1.0): [PASS]\n\n[Test 9.4] eigendecompose_qr() - General Matrix Decomposition\n\n[Test 9.4.1] General 2x2 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        1.00         2.00       |\n        3.00         4.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: (5+\u221a33)/2 \u2248 5.372, (5-\u221a33)/2 \u2248 -0.372\n  Note: This is a non-symmetric matrix, eigenvalues are real but may have complex eigenvectors\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        5.37       |\n       -0.37       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        0.42         0.91       |\n        0.91        -0.42       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nEigenvalue 1: 5.37 (Expected: 5.37, Error: 0.00, Rel Error: 0.01%) [PASS]\nEigenvalue 2: -0.37 (Expected: -0.37, Error: 0.00, Rel Error: 0.07%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.4.2] Non-Symmetric 3x3 Matrix\nMatrix [1,2,3; 4,5,6; 7,8,9]:\nMatrix Elements &gt;&gt;&gt;\n        1.00         2.00         3.00       |\n        4.00         5.00         6.00       |\n        7.00         8.00         9.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\n  Note: This matrix is rank-deficient (determinant = 0), so one eigenvalue is 0\n  Note: QR algorithm may have numerical errors, especially for non-symmetric matrices\n  Acceptable range: largest eigenvalue ~15-18, smallest eigenvalue near 0\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n       16.12       |\n       -1.12       |\n        0.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        0.23         0.88         0.41       |\n        0.53         0.24        -0.82       |\n        0.82        -0.40         0.41       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nEigenvalue 0: 16.12 (Expected: 16.12, Error: 0.00, Rel Error: 0.02%) [PASS]\nEigenvalue 1: -1.12 (Expected: -1.12, Error: 0.00, Rel Error: 0.28%) [PASS]\nEigenvalue 2: 0.00 (Expected: 0.00, Error: 0.00, Rel Error: 0.00%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.5] eigendecompose() - Automatic Method Selection\n\n[Test 9.5.1] Symmetric Matrix (Auto-select: Jacobi)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        4.00         1.00         2.00       |\n        1.00         3.00         0.00       |\n        2.00         0.00         5.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Method: Should automatically use Jacobi (symmetric matrix detected)\n  Expected eigenvalues (approximate): 6.67, 3.48, 1.85\n  Note: Eigenvalues may appear in any order\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        1.85       |\n        3.48       |\n        6.67       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 8\nStatus: OK\nMethod used: Jacobi (auto-selected for symmetric matrix)\n\n[Test 9.5.2] Non-Symmetric Matrix (Auto-select: QR)\n[Expected Results]\n  Method: Should automatically use QR (non-symmetric matrix detected)\n  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\n  Note: One eigenvalue should be near 0 (rank-deficient matrix)\n  Note: QR algorithm may have numerical errors for non-symmetric matrices\n  Acceptable: largest ~15-18, smallest near 0, one near -1 to -3\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n       16.12       |\n       -1.12       |\n        0.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nMethod used: QR (auto-selected for non-symmetric matrix)\nEigenvalue 0: 16.12 (Expected: 16.12, Error: 0.00, Rel Error: 0.02%) [PASS]\nEigenvalue 1: -1.12 (Expected: -1.12, Error: 0.00, Rel Error: 0.28%) [PASS]\nEigenvalue 2: 0.00 (Expected: 0.00, Error: 0.00, Rel Error: 0.00%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.6] SHM Application - Structural Dynamics Analysis\n\n[Test 9.6.1] 4-DOF Mass-Spring System\nStiffness Matrix K:\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00         0.00       |\n       -1.00         2.00        -1.00         0.00       |\n        0.00        -1.00         2.00        -1.00       |\n        0.00         0.00        -1.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: Yes\n\n[Quick Analysis] Primary frequency using power_iteration():\n[Expected Results]\n  Expected primary eigenvalue: ~3.53 (largest eigenvalue)\n  Expected primary frequency: sqrt(3.53) \u2248 1.88 rad/s\n\n[Actual Results]\n  Primary eigenvalue: 3.53 (Expected: ~3.53)\n  Primary frequency: 1.88 rad/s (Expected: ~1.88 rad/s)\n  Iterations: 13\n  Error from expected: 0.00 [PASS]\n\n[Complete Analysis] Full modal analysis using eigendecompose_jacobi():\n[Expected Results]\n  Expected eigenvalues (approximate): 3.53, 2.35, 1.00, 0.12\n  Expected natural frequencies: 1.88, 1.53, 1.00, 0.35 rad/s\n  Note: These are approximate values for the 4-DOF system\n\n[Actual Results]\nAll eigenvalues (natural frequencies squared):\nMatrix Elements &gt;&gt;&gt;\n        3.53       |\n        1.00       |\n        2.35       |\n        0.12       |\n&lt;&lt;&lt; Matrix Elements\n\nNatural frequencies (rad/s):\n  Mode 0: 1.88 rad/s (Expected: ~1.88 rad/s) [PASS]\n  Mode 1: 1.00 rad/s (Expected: ~1.00 rad/s) [PASS]\n  Mode 2: 1.53 rad/s (Expected: ~1.53 rad/s) [PASS]\n  Mode 3: 0.35 rad/s (Expected: ~0.35 rad/s) [PASS]\nMode shapes (eigenvectors):\nMatrix Elements &gt;&gt;&gt;\n        0.43         0.58        -0.66         0.23       |\n       -0.66         0.58         0.23         0.43       |\n        0.58        -0.00         0.58         0.58       |\n       -0.23        -0.58        -0.43         0.66       |\n&lt;&lt;&lt; Matrix Elements\n\nTotal iterations: 17\n\n[Test 9.7] Edge Cases and Error Handling\n\n[Test 9.7.1] 1x1 Matrix\nMatrix: [5.0]\n[Expected Results]\n  Expected eigenvalue: 5.0 (the matrix element itself)\n  Expected eigenvector: [1.0] (normalized)\n\n[Actual Results]\nEigenvalue: 5.00 (Expected: 5.0)\nEigenvector:\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nError from expected: 0.00 [PASS]\n\n[Test 9.7.2] Zero Matrix\n[Error] Power iteration: computed vector norm too small.\nStatus: Error (Expected)\n\n[Test 9.7.3] Identity Matrix\nMatrix (3x3 Identity):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 1.0, 1.0, 1.0 (all eigenvalues are 1)\n  Expected eigenvectors: Any orthonormal basis (e.g., standard basis vectors)\n  Expected iterations: 1 (should converge immediately)\n\n[Actual Results]\nEigenvalues (should all be 1.0):\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n        1.00       |\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 1 (Expected: 1)\nAll eigenvalues = 1.0: [PASS]\n\n[Test 9.8] Performance Test for SHM Applications\n\n[Test 9.8.1] Power Iteration Performance (Real-time SHM - Dominant Eigenvalue)\n[Performance] Power Iteration (3x3 matrix): 101.00 us\n\n[Test 9.8.2] Inverse Power Iteration Performance (System Identification - Smallest Eigenvalue)\n[Performance] Inverse Power Iteration (3x3 matrix): 495.00 us\n\n[Test 9.8.3] Jacobi Method Performance (Complete Eigendecomposition - Symmetric Matrices)\n[Performance] Jacobi Decomposition (3x3 symmetric matrix): 129.00 us\n\n[Test 9.8.4] QR Method Performance (Complete Eigendecomposition - General Matrices)\n[Performance] QR Decomposition (3x3 general matrix): 760.00 us\n\n[Eigenvalue Decomposition Tests Complete]\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-vii-auxiliary-functions-groups-10-11","title":"PHASE VII: Auxiliary Functions (Groups 10-11)","text":"<p>Purpose: Convenience functions and I/O operations</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 10: Auxiliary Functions - Stream Operators Tests]\n\n[Test 10.1] Stream Insertion Operator (&lt;&lt;) for Mat\nMatrix mat1:\n1 2 3\n4 5 6\n7 8 9\n\n\n[Test 10.2] Stream Insertion Operator (&lt;&lt;) for Mat::ROI\nROI created: ROI(pos_x=1, pos_y=2, width=3, height=4)\nExpected output:\n  row start: 2 (pos_y)\n  col start: 1 (pos_x)\n  row count: 4 (height)\n  col count: 3 (width)\n\nActual output:\nrow start 2\ncol start 1\nrow count 4\ncol count 3\n\n\n[Test 10.3] Stream Extraction Operator (&gt;&gt;) for Mat\nSimulated input: \"10 20 30 40\"\nMatrix mat2 after input:\n10 20\n30 40\n\nExpected: [10, 20; 30, 40]\n\n[Test 10.4] Stream Extraction Operator (&gt;&gt;) for Mat (2x3 matrix)\nSimulated input: \"1.5 2.5 3.5 4.5 5.5 6.5\"\nMatrix mat3 after input:\n1.5 2.5 3.5\n4.5 5.5 6.5\n\nExpected: [1.5, 2.5, 3.5; 4.5, 5.5, 6.5]\n\n[Group 11: Auxiliary Functions - Global Arithmetic Operators Tests]\n\n[Test 11.1] Matrix Addition (operator+)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA + matB:\n6 8\n10 12\n\n\n[Test 11.2] Matrix Addition with Constant (operator+)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 5.0\nmatA + 5.0f:\n6 7\n8 9\n\n\n[Test 11.3] Matrix Subtraction (operator-)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA - matB:\n-4 -4\n-4 -4\n\n\n[Test 11.4] Matrix Subtraction with Constant (operator-)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA - 2.0f:\n-1 0\n1 2\n\n\n[Test 11.5] Matrix Multiplication (operator*)\nMatrix C (2x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix D (3x2):\nMatrix Elements &gt;&gt;&gt;\n           7            8       |\n           9           10       |\n          11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nmatC * matD:\n58 64\n139 154\n\n\n[Test 11.6] Matrix Multiplication with Constant (operator*)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA * 2.0f:\n2 4\n6 8\n\n\n[Test 11.7] Matrix Division (operator/)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA / 2.0f:\n0.5 1\n1.5 2\n\n\n[Test 11.8] Matrix Division Element-wise (operator/)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA / matB:\n0.2 0.333333\n0.428571 0.5\n\n\n[Test 11.9] Matrix Comparison (operator==)\nMatrix E:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix F:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nmatE == matF: True\n\nAfter modifying matF(0,0) = 5:\nMatrix E:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix F:\nMatrix Elements &gt;&gt;&gt;\n           5            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\noperator == Error: 0 0, m1.data=1, m2.data=5, diff=4\nmatE == matF after modification: False\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/MATRIX/tiny-matrix-test/#phase-viii-quality-assurance-groups-12-14","title":"PHASE VIII: Quality Assurance (Groups 12-14)","text":"<p>Purpose: Ensure robustness, performance, and correctness</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 12: Quality Assurance - Boundary Conditions and Error Handling Tests]\n\n[Test 12.1] Null Pointer Handling in print_matrix\n[Error] Cannot print matrix: data pointer is null.\n\n[Test 12.2] Null Pointer Handling in operator&lt;&lt;\n[Error] Cannot print matrix: data pointer is null.\n\n\n[Test 12.3] Invalid Block Parameters\n[Error] Invalid block parameters: negative start position or non-positive block size.\nblock(-1, 0, 2, 2): Error\n[Error] Block exceeds matrix boundaries.\nblock(2, 2, 2, 2) on 3x3 matrix: Error\n[Error] Invalid block parameters: negative start position or non-positive block size.\nblock(0, 0, 0, 2): Error\n\n[Test 12.4] Invalid swap_rows Parameters\nBefore invalid swap_rows:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: row index out of range\nAfter swap_rows(-1, 1):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: row index out of range\nAfter swap_rows(0, 5):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.5] Invalid swap_cols Parameters\nBefore invalid swap_cols:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: column index out of range\nAfter swap_cols(-1, 1):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: column index out of range\nAfter swap_cols(0, 5):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.6] Division by Zero\n[Error] Division by zero in operator/.\nmat3 / 0.0f: Error\n\n[Test 12.7] Matrix Division with Zero Elements\n[Error] Matrix division failed: Division by zero detected.\nmat4 /= divisor (with zero):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.8] Empty Matrix Operations\nEmpty matrix addition: Error\n\n[Group 13: Quality Assurance - Performance Benchmarks Tests]\n\n[Test 13.1] Matrix Addition Performance\n[Performance] 50x50 Matrix Addition (100 iterations): 15962.00 us total, 159.62 us avg\n\n[Test 13.2] Matrix Multiplication Performance\n[Performance] 30x30 Matrix Multiplication (100 iterations): 66056.00 us total, 660.56 us avg\n\n[Test 13.3] Matrix Transpose Performance\n[Performance] 50x30 Matrix Transpose (100 iterations): 17898.00 us total, 178.98 us avg\n\n[Test 13.4] Determinant Calculation Performance Comparison\n\n[Test 13.4.1] Small Matrix (4x4) - Laplace Expansion\n[Performance] 4x4 Determinant (Laplace, 10 iterations): 2883.00 us total, 288.30 us avg\n\n[Test 13.4.2] Large Matrix (8x8) - LU Decomposition\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Performance] 8x8 Determinant (LU, 10 iterations): 55564.00 us total, 5556.40 us avg\n\n[Test 13.4.3] Large Matrix (8x8) - Gaussian Elimination\n[Performance] 8x8 Determinant (Gaussian, 10 iterations): 349.00 us total, 34.90 us avg\n\n[Test 13.4.4] Large Matrix (8x8) - Auto-select Method\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Performance] 8x8 Determinant (auto-select, 10 iterations): 55582.00 us total, 5558.20 us avg\n\n[Note] Performance Summary:\n  - Laplace expansion (O(n!)): Suitable only for small matrices (n &lt;= 4)\n  - LU decomposition (O(n\u00b3)): Efficient for large matrices, auto-selected for n &gt; 4\n  - Gaussian elimination (O(n\u00b3)): Alternative efficient method for large matrices\n  - Auto-select: Automatically chooses the best method based on matrix size\n\n[Test 13.5] Matrix Copy with Padding Performance\n[Performance] 8x8 Copy ROI (with padding) (100 iterations): 2342.00 us total, 23.42 us avg\n\n[Test 13.6] Element Access Performance\n[Performance] Computing element access (warmup)...\n[Performance] 50x50 Element Access (all elements) (100 iterations): 9706.00 us total, 97.06 us avg\n\n[Group 14: Quality Assurance - Memory Layout Tests (Padding and Stride)]\n\n[Test 14.1] Contiguous Memory (no padding)\nMatrix 3x4 (stride=4, pad=0):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9af0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |\n        4.00         5.00         6.00         7.00       |\n        8.00         9.00        10.00        11.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.2] Padded Memory (stride &gt; col)\nMatrix 3x4 (stride=5, pad=1):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991e4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |   0.00 \n        4.00         5.00         6.00         7.00       |   0.00 \n        8.00         9.00        10.00        11.00       |   0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.3] Addition with Padded Matrices\nResult of padded matrix addition:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9c64\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       11.00        22.00        33.00        44.00       |   0.00 \n       55.00        66.00        77.00        88.00       |   0.00 \n       99.00       110.00       121.00       132.00       |   0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.4] ROI Operations with Padded Matrices\nROI (1,1,2,2) from padded matrix:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991fc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        5.00         6.00       |   7.00         0.00         8.00 \n        9.00        10.00       |  11.00         0.00         0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.5] Copy Operations Preserve Stride\nCopied matrix (should have stride=4, no padding):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9d98\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |\n        4.00         5.00         6.00         7.00       |\n        8.00         9.00        10.00        11.00       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"MATH/USAGE/usage/","title":"USAGE INSTRUCTIONS","text":"<p>Usage Instructions</p> <p>This document provides usage instructions for the <code>math</code> module in Python.  It includes examples and explanations of various functions and methods available in the module.</p>"},{"location":"MATH/USAGE/usage/#import-tinymath-as-a-whole","title":"Import TinyMath as a Whole","text":"<p>Info</p> <p>Suitable for C projects or projects with a simple structure in C++.</p> <pre><code>#include \"tiny_math.h\"\n</code></pre>"},{"location":"MATH/USAGE/usage/#import-tinymath-by-module","title":"Import TinyMath by Module","text":"<p>Info</p> <p>Suitable for projects that require precise control over module imports or complex C++ projects.</p> <pre><code>#include \"tiny_vec.h\" // Import vector module\n#include \"tiny_mat.h\" // Import matrix module\n</code></pre> <pre><code>#include \"tiny_matrix.hpp\" // Import advanced matrix module\n</code></pre> <p>Note</p> <ul> <li> <p><code>tiny_vec.h</code> and <code>tiny_mat.h</code> are header files for the C language version, suitable for C programming.</p> </li> <li> <p><code>tiny_matrix.hpp</code> is a header file for the C++ language version, suitable for C++ programming.</p> </li> </ul> <p>In simple terms, C language projects can only use <code>tiny_vec.h</code> and <code>tiny_mat.h</code>, while C++ projects can use <code>tiny_vec.h</code>, <code>tiny_mat.h</code>, and <code>tiny_matrix.hpp</code>.</p> <p>Tip</p> <p>For specific usage methods, please refer to the test code.</p>"},{"location":"MATH/VECTOR/api/","title":"VECTOR OPERATIONS","text":""},{"location":"MATH/VECTOR/api/#list-of-functions","title":"LIST OF FUNCTIONS","text":"<pre><code>// Addition\ntiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Subtraction\ntiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Multiplication\ntiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Division\ntiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero);\ntiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero);\n// Square root\ntiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len);\n// Dot product\ntiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len);\ntiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2);\n</code></pre>"},{"location":"MATH/VECTOR/api/#addition","title":"ADDITION","text":""},{"location":"MATH/VECTOR/api/#addition-of-two-vectors","title":"Addition of Two Vectors","text":"<pre><code>tiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> <p>Function: Computes the element-wise addition of two vectors.</p> <p>Parameters:</p> <ul> <li><code>input1</code>: Pointer to the first input vector.</li> <li><code>input2</code>: Pointer to the second input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vectors.</li> <li><code>step1</code>: Step size for the first input vector.</li> <li><code>step2</code>: Step size for the second input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#addition-of-a-vector-and-a-constant","title":"Addition of a Vector and a Constant","text":"<pre><code>tiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> <p>Function: Computes the element-wise addition of a vector and a constant.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> <li><code>C</code>: Constant value to be added.</li> <li><code>step_in</code>: Step size for the input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#subtraction","title":"SUBTRACTION","text":""},{"location":"MATH/VECTOR/api/#subtraction-of-two-vectors","title":"Subtraction of Two Vectors","text":"<pre><code>tiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> <p>Function: Computes the element-wise subtraction of two vectors.</p> <p>Parameters:</p> <ul> <li><code>input1</code>: Pointer to the first input vector.</li> <li><code>input2</code>: Pointer to the second input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vectors.</li> <li><code>step1</code>: Step size for the first input vector.</li> <li><code>step2</code>: Step size for the second input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#subtraction-of-a-vector-and-a-constant","title":"Subtraction of a Vector and a Constant","text":"<pre><code>tiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> <p>Function: Computes the element-wise subtraction of a vector and a constant.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> <li><code>C</code>: Constant value to be subtracted.</li> <li><code>step_in</code>: Step size for the input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#multiplication","title":"MULTIPLICATION","text":""},{"location":"MATH/VECTOR/api/#multiplication-of-two-vectors","title":"Multiplication of Two Vectors","text":"<pre><code>tiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> <p>Function: Computes the element-wise multiplication of two vectors.</p> <p>Parameters:</p> <ul> <li><code>input1</code>: Pointer to the first input vector.</li> <li><code>input2</code>: Pointer to the second input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vectors.</li> <li><code>step1</code>: Step size for the first input vector.</li> <li><code>step2</code>: Step size for the second input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#multiplication-of-a-vector-and-a-constant","title":"Multiplication of a Vector and a Constant","text":"<pre><code>tiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> <p>Function: Computes the element-wise multiplication of a vector and a constant.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> <li><code>C</code>: Constant value to be multiplied.</li> <li><code>step_in</code>: Step size for the input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#division","title":"DIVISION","text":""},{"location":"MATH/VECTOR/api/#division-of-two-vectors","title":"Division of Two Vectors","text":"<pre><code>tiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero);\n</code></pre> <p>Function: Computes the element-wise division of two vectors.</p> <p>Parameters:</p> <ul> <li><code>input1</code>: Pointer to the first input vector.</li> <li><code>input2</code>: Pointer to the second input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vectors.</li> <li><code>step1</code>: Step size for the first input vector.</li> <li><code>step2</code>: Step size for the second input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> <li><code>allow_divide_by_zero</code>: Flag to allow division by zero (true or false).</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#division-of-a-vector-and-a-constant","title":"Division of a Vector and a Constant","text":"<pre><code>tiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero);\n</code></pre> <p>Function: Computes the element-wise division of a vector and a constant.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> <li><code>C</code>: Constant value to be divided.</li> <li><code>step_in</code>: Step size for the input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> <li><code>allow_divide_by_zero</code>: Flag to allow division by zero (true or false).</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#square-root","title":"SQUARE ROOT","text":""},{"location":"MATH/VECTOR/api/#square-root-of-a-vector","title":"Square Root of a Vector","text":"<pre><code>tiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len);\n</code></pre> <p>Function: Computes the element-wise square root of a vector.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#square-root-of-a-vector-fast","title":"Square Root of a Vector (Fast)","text":"<pre><code>tiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len);\n</code></pre> <p>Function: Computes the element-wise square root of a vector (fast version).</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#inverse-square-root-of-a-vector","title":"Inverse Square Root of a Vector","text":"<pre><code>tiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len);\n</code></pre> <p>Function: Computes the element-wise inverse square root of a vector.</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#inverse-square-root-of-a-vector-fast","title":"Inverse Square Root of a Vector (Fast)","text":"<pre><code>tiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len);\n</code></pre> <p>Function: Computes the element-wise inverse square root of a vector (fast version).</p> <p>Parameters:</p> <ul> <li><code>input</code>: Pointer to the input vector.</li> <li><code>output</code>: Pointer to the output vector.</li> <li><code>len</code>: Length of the vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#dot-product","title":"DOT PRODUCT","text":""},{"location":"MATH/VECTOR/api/#dot-product-of-two-vectors","title":"Dot Product of Two Vectors","text":"<pre><code>tiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len);\n</code></pre> <p>Function: Computes the dot product of two vectors.</p> <p>Parameters:</p> <ul> <li><code>src1</code>: Pointer to the first input vector.</li> <li><code>src2</code>: Pointer to the second input vector.</li> <li><code>dest</code>: Pointer to the output scalar value.</li> <li><code>len</code>: Length of the vectors.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/api/#dot-product-of-two-vectors-with-different-steps","title":"Dot Product of Two Vectors with Different Steps","text":"<pre><code>tiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2);\n</code></pre> <p>Function: Computes the dot product of two vectors with different step sizes.</p> <p>Parameters:</p> <ul> <li><code>src1</code>: Pointer to the first input vector.</li> <li><code>src2</code>: Pointer to the second input vector.</li> <li><code>dest</code>: Pointer to the output scalar value.</li> <li><code>len</code>: Length of the vectors.</li> <li><code>step1</code>: Step size for the first input vector.</li> <li><code>step2</code>: Step size for the second input vector.</li> <li><code>step_out</code>: Step size for the output vector.</li> </ul> <p>Returns: Returns a <code>tiny_error_t</code> type error code indicating whether the operation was successful.</p>"},{"location":"MATH/VECTOR/code/","title":"CODE","text":""},{"location":"MATH/VECTOR/code/#tiny_vech","title":"tiny_vec.h","text":"<pre><code>/**\n * @file tiny_vec.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the submodule vec of the tiny_math middleware. This module is correspondign to the math &amp; dotprod functions in the ESP-DSP library.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n#include \"tiny_math_config.h\"\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32 // ESP32 DSP library\n\n#include \"dsps_math.h\" // math operations\n#include \"dsps_dotprod.h\" // dot product\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* FUNCTION PROTOTYPES */\ntiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\ntiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\ntiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\ntiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero);\ntiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero);\ntiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len);\ntiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2);\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/VECTOR/code/#tiny_vecc","title":"tiny_vec.c","text":"<pre><code>/**\n * @file tiny_vec.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the source file for the submodule vec of the tiny_math middleware. This module is correspondign to the math &amp; dotprod functions in the ESP-DSP library.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n *\n * @note IMPORTANT: Buffer Overflow Prevention\n *       When using step parameters, ensure that array bounds are respected:\n *       - For input arrays: (len-1) * step &lt; array_size\n *       - For output arrays: (len-1) * step_out &lt; array_size\n *       Example: If array has 10 elements and step=2, max safe len is 5.\n *       The library does not perform runtime bounds checking for performance reasons.\n *       Users must ensure valid array sizes before calling these functions.\n */\n\n#include \"tiny_vec.h\"\n\n// #ifdef __cplusplus\n\n/* ADDITION */\n\n// vector + vector | float\n\n/**\n * @name tiny_vec_add_f32\n * @brief Adds two vectors element-wise.\n * @param input1 Pointer to the first input vector.\n * @param input2 Pointer to the second input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param step1 Step size for the first input vector.\n * @param step2 Step size for the second input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function performs element-wise addition of two vectors with specified step sizes, and the output is also specified with a step size.\n */\ntiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector addition\n    dsps_add_f32(input1, input2, output, len, step1, step2, step_out);\n#else\n    // Fallback to a simple loop for vector addition\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input1[i * step1] + input2[i * step2];\n    }\n\n#endif\n\n    return TINY_OK;\n}\n\n// vector + constant | float\n/**\n * @name tiny_vec_addc_f32\n * @brief Adds a constant to each element of a vector.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param C Constant value to be added.\n * @param step_in Step size for the input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function adds a constant value to each element of the input vector, with specified step sizes for both input and output vectors.\n */\ntiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector addition\n    dsps_addc_f32(input, output, len, C, step_in, step_out);\n#else\n    // Fallback to a simple loop for vector addition\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input[i * step_in] + C;\n    }\n#endif\n    return TINY_OK;\n}\n\n/* SUBTRACTION */\n\n// vector - vector | float\n/**\n * @name tiny_vec_sub_f32\n * @brief Subtracts two vectors element-wise.\n * @param input1 Pointer to the first input vector.\n * @param input2 Pointer to the second input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param step1 Step size for the first input vector.\n * @param step2 Step size for the second input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function performs element-wise subtraction of two vectors with specified step sizes, and the output is also specified with a step size.\n */\ntiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector subtraction\n    dsps_sub_f32(input1, input2, output, len, step1, step2, step_out);\n#else\n    // Fallback to a simple loop for vector subtraction\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input1[i * step1] - input2[i * step2];\n    }\n#endif\n    return TINY_OK;\n}\n\n// vector - constant (add -c) | float\n/**\n * @name tiny_vec_subc_f32\n * @brief Subtracts a constant from each element of a vector.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param C Constant value to be subtracted.\n * @param step_in Step size for the input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function subtracts a constant value from each element of the input vector, with specified step sizes for both input and output vectors.\n */\ntiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector subtraction\n    dsps_addc_f32(input, output, len, -C, step_in, step_out);\n#else\n    // Fallback to a simple loop for vector subtraction\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input[i * step_in] - C;\n    }\n#endif\n    return TINY_OK;\n}\n\n/* MULTIPLICATION */\n\n// vector * vector (elementwise) | float\n/**\n * @name tiny_vec_mul_f32\n * @brief Multiplies two vectors element-wise.\n * @param input1 Pointer to the first input vector.\n * @param input2 Pointer to the second input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param step1 Step size for the first input vector.\n * @param step2 Step size for the second input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function performs element-wise multiplication of two vectors with specified step sizes, and the output is also specified with a step size.\n */\ntiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector multiplication\n    dsps_mul_f32(input1, input2, output, len, step1, step2, step_out);\n#else\n    // Fallback to a simple loop for vector multiplication\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input1[i * step1] * input2[i * step2];\n    }\n#endif\n    return TINY_OK;\n}\n\n// vector * constant | float\n/**\n * @name tiny_vec_mulc_f32\n * @brief Multiplies each element of a vector by a constant.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param C Constant value to be multiplied.\n * @param step_in Step size for the input vector.\n * @param step_out Step size for the output vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function multiplies each element of the input vector by a constant value, with specified step sizes for both input and output vectors.\n */\ntiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized vector multiplication\n    dsps_mulc_f32(input, output, len, C, step_in, step_out);\n#else\n    // Fallback to a simple loop for vector multiplication\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input[i * step_in] * C;\n    }\n#endif\n    return TINY_OK;\n}\n\n/* DIVISION */\n\n// vector / vector (elementwise) | float\n/**\n * @name tiny_vec_div_f32\n * @brief Divides one vector by another element-wise.\n * @param input1 Pointer to the numerator vector.\n * @param input2 Pointer to the denominator vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param step1 Step size for the numerator vector.\n * @param step2 Step size for the denominator vector.\n * @param step_out Step size for the output vector.\n * @param allow_divide_by_zero Whether to safely handle zero denominators (true: set output to 0; false: return error if any zero is found).\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function performs element-wise division with specified step sizes.\n *       If allow_divide_by_zero is false, the function will first scan for zero denominators and return an error immediately if any are found.\n */\ntiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero)\n{\n    if (NULL == input1 || NULL == input2 || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n    if (len &lt;= 0 || step1 &lt;= 0 || step2 &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    const float epsilon = TINY_MATH_MIN_DENOMINATOR;\n\n    // Step 1: Pre-check for zero denominators if not allowed\n    if (!allow_divide_by_zero)\n    {\n        for (int i = 0; i &lt; len; i++)\n        {\n            if (fabsf(input2[i * step2]) &lt; epsilon)\n            {\n                return TINY_ERR_MATH_ZERO_DIVISION;\n            }\n        }\n    }\n\n    // Step 2: Perform element-wise division\n    // Note: If allow_divide_by_zero=false, pre-check already passed, but we double-check for safety\n    for (int i = 0; i &lt; len; i++)\n    {\n        float denom = input2[i * step2];\n        float numer = input1[i * step1];\n\n        if (fabsf(denom) &lt; epsilon)\n        {\n            if (allow_divide_by_zero)\n            {\n                // Handle division by near-zero: mathematically correct behavior\n                if (fabsf(numer) &lt; epsilon)\n                {\n                    // 0/0 case: undefined, return 0.0f as safe fallback\n                    output[i * step_out] = 0.0f;\n                }\n                else\n                {\n                    // Non-zero divided by near-zero: return signed infinity\n                    // Use large number instead of INFINITY for compatibility\n                    output[i * step_out] = (numer &gt; 0.0f) ? TINY_MATH_LARGE_VALUE_F32 : -TINY_MATH_LARGE_VALUE_F32;\n                }\n            }\n            else\n            {\n                // This should not happen if pre-check passed, but return error for safety\n                return TINY_ERR_MATH_ZERO_DIVISION;\n            }\n        }\n        else\n        {\n            output[i * step_out] = numer / denom;\n        }\n    }\n\n    return TINY_OK;\n}\n\n// vector / constant | float\n/**\n * @name tiny_vec_divc_f32\n * @brief Divides each element of a vector by a constant using multiplication for performance.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @param C Constant value to divide by.\n * @param step_in Step size for the input vector.\n * @param step_out Step size for the output vector.\n * @param allow_divide_by_zero Whether to safely handle zero constant (true: set output to 0; false: return error if C is near zero).\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function divides each element of the input vector by a constant using multiplication for performance.\n *       If allow_divide_by_zero is false and C is near zero, the function returns an error.\n *       Otherwise, 1/C is precomputed and used as a multiplier.\n */\ntiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0 || step_in &lt;= 0 || step_out &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    const float epsilon = TINY_MATH_MIN_DENOMINATOR;\n\n    // Step 1: Handle constant C\n    if (fabsf(C) &lt; epsilon)\n    {\n        if (!allow_divide_by_zero)\n        {\n            return TINY_ERR_MATH_ZERO_DIVISION;\n        }\n\n        // Handle division by near-zero constant: mathematically correct behavior\n        // For vector / near-zero, each element should approach infinity\n        for (int i = 0; i &lt; len; i++)\n        {\n            float val = input[i * step_in];\n            if (fabsf(val) &lt; epsilon)\n            {\n                // 0 / near-zero: undefined, return 0.0f as safe fallback\n                output[i * step_out] = 0.0f;\n            }\n            else\n            {\n                // Non-zero divided by near-zero: return signed large value\n                output[i * step_out] = (val &gt; 0.0f) ? TINY_MATH_LARGE_VALUE_F32 : -TINY_MATH_LARGE_VALUE_F32;\n            }\n        }\n        return TINY_OK;\n    }\n\n    // Step 2: Use 1/C for performance\n    float invC = 1.0f / C;\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_mulc_f32(input, output, len, invC, step_in, step_out);\n#else\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i * step_out] = input[i * step_in] * invC;\n    }\n#endif\n\n    return TINY_OK;\n}\n\n/* SQUARE ROOT */\n\n// vector ^ 0.5 (sqrt-based)| float\n/**\n * @name tiny_vec_sqrt_f32\n * @brief Computes the square root of each element in a vector using standard library sqrtf.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function provides accurate results using math library sqrtf().\n *       It returns TINY_ERR_MATH_NEGATIVE_SQRT immediately if any element is negative.\n */\ntiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    // Pre-check: validate all inputs before processing to avoid partial results\n    for (int i = 0; i &lt; len; i++)\n    {\n        if (input[i] &lt; 0.0f)\n        {\n            return TINY_ERR_MATH_NEGATIVE_SQRT;\n        }\n    }\n\n    // All inputs validated, now perform computation\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i] = sqrtf(input[i]);  // high-precision sqrt\n    }\n\n    return TINY_OK;\n}\n\n// single value sqrt\n/**\n * @name tiny_sqrtf_f32\n * @brief Computes the square root of a single float value using bit manipulation.\n * @param f Input float value.\n * @return Square root of the input value.\n * @note This function uses bit manipulation to compute the square root of a float value.\n *       It returns 0.0f for negative inputs to prevent sqrt of negative values.\n *       WARNING: This is a fast approximation algorithm. Typical relative error is &lt; 1%.\n *       For high-precision applications, use tiny_vec_sqrt_f32() instead.\n */\ninline float tiny_sqrtf_f32(float f)\n{\n    if (f &lt; 0.0f) {\n        return 0.0f;  // Prevent sqrt of negative values\n    }\n\n    // Use union to avoid strict aliasing violation\n    union {\n        float f;\n        uint32_t i;\n    } input_conv = {f};\n\n    union {\n        float f;\n        uint32_t i;\n    } result_conv;\n\n    result_conv.i = 0x1fbb4000 + (input_conv.i &gt;&gt; 1);\n    return result_conv.f;\n}\n\n// vector ^ 0.5 (sqrtf-based)| float\n/**\n * @name tiny_vec_sqrtf_f32\n * @brief Computes the square root of each element in a vector using fast approximation.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function computes the square root of each element using fast bit manipulation.\n *       It returns TINY_ERR_MATH_NEGATIVE_SQRT immediately if any element is negative.\n *       WARNING: This is a fast approximation. Typical relative error is &lt; 1%.\n *       For high-precision applications, use tiny_vec_sqrt_f32() instead.\n *       NOTE: Ensure input/output arrays have sufficient size: at least len elements.\n *       When using step parameters, ensure: (len-1)*step &lt; array_size to avoid buffer overflow.\n */\ntiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    // Pre-check: validate all inputs before processing to avoid partial results\n    for (int i = 0; i &lt; len; i++)\n    {\n        if (input[i] &lt; 0.0f)\n        {\n            return TINY_ERR_MATH_NEGATIVE_SQRT;\n        }\n    }\n\n    // All inputs validated, now perform computation\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i] = tiny_sqrtf_f32(input[i]);\n    }\n\n    return TINY_OK;\n}\n\n// vector ^ -0.5 (sqrt-based) | float\n/**\n * @name tiny_vec_inv_sqrt_f32\n * @brief Computes the inverse square root of each element in a vector using standard sqrtf().\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function provides accurate inverse square root results using 1.0f / sqrtf(x).\n *       It returns TINY_ERR_NOT_ALLOWED immediately if any element is less than TINY_MATH_MIN_POSITIVE_INPUT_F32.\n */\ntiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    // Pre-check: validate all inputs before processing to avoid partial results\n    for (int i = 0; i &lt; len; i++)\n    {\n        if (input[i] &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            return TINY_ERR_NOT_ALLOWED;\n        }\n    }\n\n    // All inputs validated, now perform computation\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i] = 1.0f / sqrtf(input[i]);  // Accurate inverse square root\n    }\n\n    return TINY_OK;\n}\n\n\n// single value inv sqrt\n/**\n * @name tiny_inverted_sqrtf_f32\n * @brief Computes the inverse square root of a single float value using bit manipulation (Quake algorithm).\n * @param data Input float value.\n * @return Inverse square root of the input value.\n * @note This function uses the famous Quake III fast inverse square root algorithm.\n *       It returns 0.0f for negative or near-zero inputs to prevent division by zero.\n *       WARNING: This is a fast approximation. Typical relative error is &lt; 0.2%.\n *       For high-precision applications, use tiny_vec_inv_sqrt_f32() instead.\n */\nfloat tiny_inverted_sqrtf_f32(float data)\n{\n    if (data &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32) {\n        return 0.0f;  // Avoid division by near-zero or zero\n    }\n\n    const float x2 = data * 0.5F;\n    const float threehalfs = 1.5F;\n\n    union {\n        float f;\n        uint32_t i;\n    } conv = {data};\n\n    conv.i  = 0x5f3759df - (conv.i &gt;&gt; 1);\n    conv.f  = conv.f * (threehalfs - (x2 * conv.f * conv.f));\n\n    return conv.f;\n}\n\n// vector ^ -0.5 (sqrtf-based) | float\n/**\n * @name tiny_vec_inv_sqrtf_f32\n * @brief Computes the inverse square root of each element in a vector using fast approximation.\n * @param input Pointer to the input vector.\n * @param output Pointer to the output vector.\n * @param len Length of the vectors.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function computes the inverse square root using the Quake III fast algorithm.\n *       If any element is less than TINY_MATH_MIN_POSITIVE_INPUT_F32, the function returns TINY_ERR_NOT_ALLOWED.\n *       WARNING: This is a fast approximation. Typical relative error is &lt; 0.2%.\n *       For high-precision applications, use tiny_vec_inv_sqrt_f32() instead.\n *       NOTE: Ensure input/output arrays have sufficient size: at least len elements.\n *       When using step parameters, ensure: (len-1)*step &lt; array_size to avoid buffer overflow.\n */\ntiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len)\n{\n    if (NULL == input || NULL == output)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n\n    // Pre-check: validate all inputs before processing to avoid partial results\n    for (int i = 0; i &lt; len; i++)\n    {\n        if (input[i] &lt; TINY_MATH_MIN_POSITIVE_INPUT_F32)\n        {\n            return TINY_ERR_NOT_ALLOWED;\n        }\n    }\n\n    // All inputs validated, now perform computation\n    for (int i = 0; i &lt; len; i++)\n    {\n        output[i] = tiny_inverted_sqrtf_f32(input[i]);\n    }\n\n    return TINY_OK;\n}\n\n/* DOT PRODUCT */\n\n// vector * vector (dot product) | float\n/**\n * @name tiny_vec_dotprod_f32\n * @brief Computes the dot product of two vectors.\n * @param src1 Pointer to the first input vector.\n * @param src2 Pointer to the second input vector.\n * @param dest Pointer to the output scalar result.\n * @param len Length of the vectors.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function computes the dot product of two vectors and stores the result in a single float value.\n *       It returns TINY_ERR_MATH_NULL_POINTER if any pointer is NULL.\n *       The function uses the ESP-DSP library for optimized computation.\n */\ntiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len)\n{\n    if (NULL == src1 || NULL == src2 || NULL == dest)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized dot product\n    dsps_dotprod_f32(src1, src2, dest, len);\n#else\n    // Fallback to a simple loop for dot product\n    float acc = 0.0f;\n    for (int i = 0; i &lt; len; i++)\n    {\n        acc += src1[i] * src2[i];\n    }\n    *dest = acc;\n#endif\n    return TINY_OK;\n}\n\n// vector * vector (dot product - step support) | float\n/**\n * @name tiny_vec_dotprode_f32\n * @brief Computes the dot product of two vectors with step support.\n * @param src1 Pointer to the first input vector.\n * @param src2 Pointer to the second input vector.\n * @param dest Pointer to the output scalar result.\n * @param len Length of the vectors.\n * @param step1 Step size for the first input vector.\n * @param step2 Step size for the second input vector.\n * @return tiny_error_t Error code indicating success or failure.\n * @note This function computes the dot product of two vectors with specified step sizes and stores the result in a single float value.\n */\ntiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2)\n{\n    if (NULL == src1 || NULL == src2 || NULL == dest)\n    {\n        return TINY_ERR_MATH_NULL_POINTER;\n    }\n\n    if (len &lt;= 0 || step1 &lt;= 0 || step2 &lt;= 0)\n    {\n        return TINY_ERR_INVALID_ARG;\n    }\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // Use the ESP-DSP library for optimized dot product with step support\n    dsps_dotprode_f32(src1, src2, dest, len, step1, step2);\n#else\n    // Fallback to a simple loop for dot product with step support\n    float acc = 0.0f;\n    for (int i = 0; i &lt; len; i++)\n    {\n        acc += src1[i * step1] * src2[i * step2];\n    }\n    *dest = acc;\n#endif\n    return TINY_OK;\n}\n</code></pre>"},{"location":"MATH/VECTOR/test/","title":"VECTOR OPERATIONS TEST","text":"<p>Vector Operations Test</p> <p>This test is designed to evaluate the performance of vector-related functions.</p>"},{"location":"MATH/VECTOR/test/#test-code","title":"Test Code","text":""},{"location":"MATH/VECTOR/test/#tiny_vec_testh","title":"tiny_vec_test.h","text":"<pre><code>/**\n * @file tiny_vec_test.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the test header file for the submodule vec of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n */\n\n#pragma once\n\n#include \"tiny_math_config.h\"\n#include \"tiny_vec.h\"\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /**\n     * @name tiny_vec_test\n     * @brief Run unit tests and timing benchmarks for the tiny_vec module.\n     */\n    void tiny_vec_test(void);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"MATH/VECTOR/test/#tiny_vec_testc","title":"tiny_vec_test.c","text":"<pre><code>/**\n * @file tiny_vec_test.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file implements test functions for the submodule vec of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n */\n\n#include \"tiny_vec_test.h\"\n\n#define LEN 6\n#define ITERATIONS 10000 // Number of iterations for performance benchmarking\n\n#define RUN_VEC_TEST(FUNC, ...)                                                \\\n    do                                                                         \\\n    {                                                                          \\\n        tiny_error_t err = TINY_OK;                                            \\\n        int actual_iter = 0;                                                   \\\n        TinyTimeMark_t t0 = tiny_get_running_time();                           \\\n        for (int iter = 0; iter &lt; ITERATIONS; iter++)                          \\\n        {                                                                      \\\n            err = FUNC(__VA_ARGS__);                                           \\\n            actual_iter++;                                                     \\\n            if (err != TINY_OK)                                                \\\n                break;                                                         \\\n        }                                                                      \\\n        TinyTimeMark_t t1 = tiny_get_running_time();                           \\\n        double dt_total = (double)(t1 - t0);                                   \\\n        double dt_avg = (actual_iter &gt; 0) ? dt_total / actual_iter : 0.0;     \\\n        printf(\"%-24s | Output: \", #FUNC);                                     \\\n        for (int i = 0; i &lt; LEN; i++)                                          \\\n        {                                                                      \\\n            printf(\"%10.6f \", out[i]);                                         \\\n        }                                                                      \\\n        printf(\"| Total: %8.2f us | Avg: %6.2f us | Iter: %d | Error: %d\\n\\r\", \\\n               dt_total, dt_avg, actual_iter, err);                            \\\n    } while (0)\n\nvoid tiny_vec_test(void)\n{\n    float a[] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\n    float b[] = {6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f};\n    float out[LEN];\n    float C = 2.0f;\n    float dot_result = 0.0f;\n\n    printf(\"============ [tiny_vec_test] ============\\n\\r\");\n    printf(\"Benchmark Settings: %d iterations per test\\n\\r\", ITERATIONS);\n\n    printf(\"Input Vector a:        \");\n    for (int i = 0; i &lt; LEN; i++)\n        printf(\"%10.6f \", a[i]);\n    printf(\"\\n\\r\");\n\n    printf(\"Input Vector b:        \");\n    for (int i = 0; i &lt; LEN; i++)\n        printf(\"%10.6f \", b[i]);\n    printf(\"\\n\\r\");\n\n    printf(\"Constant C:            %10.6f\\n\\r\\n\\r\", C);\n\n    RUN_VEC_TEST(tiny_vec_add_f32, a, b, out, LEN, 1, 1, 1);\n    RUN_VEC_TEST(tiny_vec_addc_f32, a, out, LEN, C, 1, 1);\n    RUN_VEC_TEST(tiny_vec_sub_f32, a, b, out, LEN, 1, 1, 1);\n    RUN_VEC_TEST(tiny_vec_subc_f32, a, out, LEN, C, 1, 1);\n    RUN_VEC_TEST(tiny_vec_mul_f32, a, b, out, LEN, 1, 1, 1);\n    RUN_VEC_TEST(tiny_vec_mulc_f32, a, out, LEN, C, 1, 1);\n    RUN_VEC_TEST(tiny_vec_div_f32, a, b, out, LEN, 1, 1, 1, true);\n    RUN_VEC_TEST(tiny_vec_divc_f32, a, out, LEN, C, 1, 1, true);\n    RUN_VEC_TEST(tiny_vec_sqrt_f32, a, out, LEN);\n    RUN_VEC_TEST(tiny_vec_sqrtf_f32, a, out, LEN);\n    RUN_VEC_TEST(tiny_vec_inv_sqrt_f32, a, out, LEN);\n    RUN_VEC_TEST(tiny_vec_inv_sqrtf_f32, a, out, LEN);\n\n    // Dot product (non-strided)\n    {\n        tiny_error_t err = TINY_OK;\n        int actual_iter = 0;\n        TinyTimeMark_t t0 = tiny_get_running_time();\n        for (int iter = 0; iter &lt; ITERATIONS; iter++)\n        {\n            err = tiny_vec_dotprod_f32(a, b, &amp;dot_result, LEN);\n            actual_iter++;\n            if (err != TINY_OK)\n                break;\n        }\n        TinyTimeMark_t t1 = tiny_get_running_time();\n        double dt_total = (double)(t1 - t0);\n        double dt_avg = (actual_iter &gt; 0) ? dt_total / actual_iter : 0.0;\n        printf(\"%-24s | Output: %10.6f | Total: %8.2f us | Avg: %6.2f us | Iter: %d | Error: %d\\n\\r\",\n               \"tiny_vec_dotprod_f32\", dot_result, dt_total, dt_avg, actual_iter, err);\n    }\n\n    // Dot product (strided)\n    {\n        tiny_error_t err = TINY_OK;\n        int actual_iter = 0;\n        TinyTimeMark_t t0 = tiny_get_running_time();\n        for (int iter = 0; iter &lt; ITERATIONS; iter++)\n        {\n            err = tiny_vec_dotprode_f32(a, b, &amp;dot_result, LEN, 1, 1);\n            actual_iter++;\n            if (err != TINY_OK)\n                break;\n        }\n        TinyTimeMark_t t1 = tiny_get_running_time();\n        double dt_total = (double)(t1 - t0);\n        double dt_avg = (actual_iter &gt; 0) ? dt_total / actual_iter : 0.0;\n        printf(\"%-24s | Output: %10.6f | Total: %8.2f us | Avg: %6.2f us | Iter: %d | Error: %d\\n\\r\",\n               \"tiny_vec_dotprode_f32\", dot_result, dt_total, dt_avg, actual_iter, err);\n    }\n\n    printf(\"============ [test complete] ============\\n\\r\");\n}\n</code></pre>"},{"location":"MATH/VECTOR/test/#maincpp","title":"main.cpp","text":"<pre><code>#include \"tiny_vec_test.h\"\n\nextern \"C\" void app_main(void)\n{\n    tiny_vec_test();\n}\n</code></pre>"},{"location":"MATH/VECTOR/test/#test-output","title":"Test Output","text":"<p>Basic C computation results</p> <p></p> <p>ESP-DSP accelerated results</p> <p></p> <p>You can see that with ESP-DSP acceleration enabled, the performance of vector operations is significantly improved.</p>"},{"location":"PREREQUISITE/prerequisite/","title":"PREREQUISITES","text":""},{"location":"PREREQUISITE/prerequisite/#hardware-and-software-requirements","title":"HARDWARE AND SOFTWARE REQUIREMENTS","text":"<p>ESP32 development board, please refer to the following projects for details:</p> <ul> <li> <p> NexNode</p> <p>  Repo </p> <p>  Online Doc </p> </li> </ul> <p>We will build upon the code in this project for further development.</p>"},{"location":"PREREQUISITE/prerequisite/#dependency-components","title":"DEPENDENCY COMPONENTS","text":"<p>To enhance the computational efficiency of our framework, we first introduce the ESP-DSP library and ESP-DL library, which provide efficient implementations for digital signal processing and deep learning respectively.</p> <p>Tip</p> <p>Note that these two libraries seem to be developed by different teams, so many of their functions overlap.</p> <pre><code>- espressif__esp-dsp\n- espressif__esp-dl\n   - espressif__dl_fft\n   - espressif__esp_new_jpeg\n</code></pre> <p>We can find and download these components from the ESP-REGISTRY into our project. In this project, I moved the downloaded components and their dependencies into the <code>middleware</code> folder and removed the configuration files to avoid version locking and network dependencies.</p>"},{"location":"TOOLBOX/toolbox/","title":"TOOLBOX","text":"<p>tiny_toolbox</p> <p>tiny_toolbox is a library designed for platform adaptation and optimization, providing various practical tools to serve edge computing and application development. Note that the adaptation and tools are included in the same library because many tools utilize the functions provided by the platform at a lower level. Therefore, placing platform adaptation and various tools together facilitates usage and maintenance.</p> <p>Warning</p> <p>Currently, development is based on ESP32, and migration to platforms like STM32 requires some modifications to the adaptation layer.</p>"},{"location":"TOOLBOX/toolbox/#architecture-and-function-directory","title":"ARCHITECTURE AND FUNCTION DIRECTORY","text":"<pre><code>    tiny_toolbox\n    \u251c\u2500\u2500 CMakeLists.txt\n    \u251c\u2500\u2500 tiny_toolbox.h // serves as a directory, integrating all submodules\n    \u251c\u2500\u2500 time\n    \u2502   \u251c\u2500\u2500 tiny_time.h // submodule for time management - header file\n    \u2502   \u251c\u2500\u2500 tiny_time.c // submodule for time management - source file\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"TOOLBOX/toolbox/#time","title":"TIME","text":"<ul> <li>Get Running Time: <code>tiny_get_running_time()</code></li> <li>SNTP Time Synchronization: <code>sync_time_with_timezone(\"CST-8\")</code></li> <li>Get World Time: <code>tiny_get_current_datetime(1)</code></li> </ul> <p>TODO:</p> <ul> <li>Local Time Synchronization for Wireless Sensor Networks - Microsecond Level</li> </ul>"},{"location":"TOOLBOX/toolbox/#code","title":"CODE","text":"<p>Tip</p> <p>tiny_toolbox.h serves merely as a directory, integrating all submodules. The specific functionalities are implemented in each submodule. tiny_toolbox.c is just a formal source file without specific functionality.</p>"},{"location":"TOOLBOX/toolbox/#cmakeliststxt","title":"CMakeLists.txt","text":"<pre><code>set(src_dirs\n    .\n    time\n)\n\nset(include_dirs\n    .\n    time\n)\n\nset(requires\n    esp_timer\n    node_rtc\n    espressif__esp-dsp\n    espressif__esp_new_jpeg\n    espressif__dl_fft\n    espressif__esp-dl\n)\n\nidf_component_register(SRC_DIRS ${src_dirs} INCLUDE_DIRS ${include_dirs} REQUIRES ${requires})\n</code></pre>"},{"location":"TOOLBOX/toolbox/#tiny_toolboxh","title":"tiny_toolbox.h","text":"<pre><code>/**\n * @file tiny_toolbox.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the tiny_toolbox middleware.\n * @version 1.0\n * @date 2025-03-26\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* DEPENDENCIES */\n// system\n#include \"freertos/FreeRTOS.h\"\n#include \"freertos/task.h\"\n#include \"esp_log.h\"\n#include \"esp_timer.h\"\n#include \"esp_heap_caps.h\"\n\n// customized drivers\n#include \"node_rtc.h\"\n\n/* SUBMODULES */\n#include \"tiny_time.h\" // Time\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"TOOLBOX/TIME/code/","title":"TIME","text":""},{"location":"TOOLBOX/TIME/code/#tiny_timeh","title":"tiny_time.h","text":"<pre><code>/**\n * @file tiny_time.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief Submodule for TinyToolbox - header file\n * @version 1.0\n * @date 2025-04-10\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* CONFIGURATIONS */\n\n/* ================================ DEPENDENCIES ================================ */\n#include &lt;stdbool.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;time.h&gt;\n#include &lt;sys/time.h&gt;\n#include \"esp_log.h\"\n#include \"esp_timer.h\"\n#include \"esp_sntp.h\"\n// customized drivers\n#include \"node_rtc.h\"\n\n    /* ================================ DEFINITIONS ================================= */\n    // Use int64_t to match esp_timer_get_time() return type and avoid overflow\n    // esp_timer_get_time() returns microseconds since boot (int64_t)\n    typedef int64_t TinyTimeMark_t;\n\n    /**\n     * @brief Structure to hold date and time\n     */\n    typedef struct TinyDateTime_t\n    {\n        int year;\n        int month;\n        int day;\n        int hour;\n        int minute;\n        int second;\n        int32_t microsecond; // Microseconds (0-999999), using int32_t for portability\n    } TinyDateTime_t;\n\n    /* ================================ FUNCTIONS =================================== */\n    /* LOCAL RUNNING TIME IN MICROSECONDS */\n    /**\n     * @brief Get the running time in microseconds\n     * @return TinyTimeMark_t\n     */\n    TinyTimeMark_t tiny_get_running_time(void);\n\n    /* WORLD CURRENT TIME - SNTP */\n    /**\n     * @brief Obtain the current time with timezone\n     * @param timezone_str Timezone string (e.g., \"CST-8\" or \"GMT+8\")\n     * @note The timezone string format should be compatible with POSIX TZ format (e.g., \"CST-8\", \"GMT+8\")\n     * @note To use this function, in application, after internet connection, call sync_time_with_timezone(\"CST-8\")\n     * @return None\n     */\n    void sync_time_with_timezone(const char *timezone_str);\n\n    /* WORLD CURRENT TIME - GET TIME */\n    /**\n     * @name tiny_get_current_datetime\n     * @brief Get the current time as a TinyDateTime_t struct\n     * @param print_flag Flag to indicate whether to print the time\n     * @return TinyDateTime_t structure containing the current date and time\n     */\n    TinyDateTime_t tiny_get_current_datetime(bool print_flag);\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"TOOLBOX/TIME/code/#tiny_timec","title":"tiny_time.c","text":"<pre><code>/**\n * @file tiny_time.c\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief Submodule for TinyToolbox - source file\n * @version 1.0\n * @date 2025-04-10\n * @copyright Copyright (c) 2025\n *\n */\n\n/* ================================ DEPENDENCIES\n * ================================ */\n#include \"tiny_time.h\" // Time\n\n/* ================================ DEFINITIONS\n * ================================= */\n/* CONFIGURATIONS */\n#define MIN_VALID_YEAR_OFFSET \\\n    (2020 - 1900) // Minimum valid year offset (year 2020)\n\n/* TAGS */\nstatic const char *TAG_SNTP = \"NTP_SYNC\";\nstatic const char *TAG_TIME = \"TIME\";\n\n/* ================================ FUNCTIONS\n * =================================== */\n/* LOCAL RUNNING TIME IN MICROSECONDS */\n/**\n * @brief Get the running time in microseconds\n * @return TinyTimeMark_t\n */\nTinyTimeMark_t tiny_get_running_time(void) { return esp_timer_get_time(); }\n\n/* WORLD CURRENT TIME - SNTP */\n/**\n * @brief Callback function for time synchronization notification\n * @param tv Pointer to the timeval structure containing the synchronized time\n * @return None\n */\nstatic void time_sync_notification_cb(struct timeval *tv)\n{\n    ESP_LOGI(TAG_SNTP, \"Time synchronized!\");\n}\n\n/**\n * @brief Initialize SNTP\n * @note This function can be called multiple times if needed\n * @return None\n */\nstatic void initialize_sntp(void)\n{\n    ESP_LOGI(TAG_SNTP, \"Initializing SNTP\");\n    esp_sntp_setoperatingmode(SNTP_OPMODE_POLL);\n    esp_sntp_setservername(0, \"pool.ntp.org\"); // NTP server // pool.ntp.org // ntp.aliyun.com\n    esp_sntp_set_time_sync_notification_cb(time_sync_notification_cb);\n    esp_sntp_init();\n}\n\n/**\n * @brief Obtain the current time with timezone\n * @param timezone_str Timezone string (e.g., \"CST-8\" or \"GMT+8\")\n * @note The timezone string format should be compatible with POSIX TZ format\n * (e.g., \"CST-8\", \"GMT+8\")\n * @note To use this function, in application, after internet connection, call\n * sync_time_with_timezone(\"CST-8\")\n * @return None\n */\nvoid sync_time_with_timezone(const char *timezone_str)\n{\n    // Validate input parameter\n    if (timezone_str == NULL)\n    {\n        ESP_LOGE(TAG_SNTP, \"timezone_str is NULL\");\n        return;\n    }\n\n    // Set system timezone\n    if (setenv(\"TZ\", timezone_str, 1) != 0)\n    {\n        ESP_LOGE(TAG_SNTP, \"Failed to set timezone environment variable\");\n        return;\n    }\n    tzset();\n\n    // Initialize SNTP and start time sync\n    initialize_sntp();\n\n    // Wait for system time to be set\n    time_t now = 0;\n    struct tm timeinfo = {0};\n    int retry = 0;\n    const int retry_count = 15;\n\n    while (timeinfo.tm_year &lt; MIN_VALID_YEAR_OFFSET &amp;&amp; ++retry &lt; retry_count)\n    {\n        ESP_LOGI(TAG_SNTP, \"Waiting for system time to be set... (%d/%d)\", retry,\n                 retry_count);\n        vTaskDelay(2000 / portTICK_PERIOD_MS);\n        time(&amp;now);\n        if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n        {\n            ESP_LOGW(TAG_SNTP, \"Failed to convert time to local time\");\n            continue;\n        }\n    }\n\n    if (timeinfo.tm_year &gt;= MIN_VALID_YEAR_OFFSET)\n    {\n        rtc_set_time(timeinfo.tm_year + 1900, timeinfo.tm_mon + 1, timeinfo.tm_mday,\n                     timeinfo.tm_hour, timeinfo.tm_min,\n                     timeinfo.tm_sec); // defined in esp_rtc.c\n        ESP_LOGI(TAG_SNTP, \"System time is set.\");\n    }\n    else\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to sync time.\");\n        return;\n    }\n\n    // Log current local time (using thread-safe formatting)\n    char time_str[64];\n    if (strftime(time_str, sizeof(time_str), \"%a %b %d %H:%M:%S %Y\", &amp;timeinfo) ==\n        0)\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to format time string\");\n    }\n    else\n    {\n        ESP_LOGI(TAG_SNTP, \"Current time: %s\", time_str);\n    }\n\n    // vTaskDelay(10000 / portTICK_PERIOD_MS); // Wait for 10 second\n    // rtc_get_time(); // uncomment to check the RTC time\n    // ESP_LOGI(TAG_SNTP, \"Current RTC time: %04d-%02d-%02d %02d:%02d:%02d\",\n    //          calendar.year, calendar.month, calendar.date,\n    //          calendar.hour, calendar.min, calendar.sec); // uncomment to check\n    //          the RTC time\n}\n\n/* WORLD CURRENT TIME - GET TIME */\n/**\n * @name tiny_get_current_datetime\n * @brief Get the current time as a TinyDateTime_t struct\n * @param print_flag Flag to indicate whether to print the time\n * @return TinyDateTime_t structure containing the current date and time\n */\nTinyDateTime_t tiny_get_current_datetime(bool print_flag)\n{\n    TinyDateTime_t result = {0}; // Initialize to zero\n    struct timeval tv;\n\n    // Get current time (seconds + microseconds)\n    if (gettimeofday(&amp;tv, NULL) != 0)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to get time of day\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    time_t now = tv.tv_sec;\n    struct tm timeinfo;\n    if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to convert time to local time\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    result.year = timeinfo.tm_year + 1900;\n    result.month = timeinfo.tm_mon + 1;\n    result.day = timeinfo.tm_mday;\n    result.hour = timeinfo.tm_hour;\n    result.minute = timeinfo.tm_min;\n    result.second = timeinfo.tm_sec;\n    result.microsecond = (int32_t)tv.tv_usec; // Explicit cast for portability\n\n    if (print_flag)\n    {\n        ESP_LOGI(TAG_TIME, \"Current Time: %04d-%02d-%02d %02d:%02d:%02d.%06d\",\n                 result.year, result.month, result.day, result.hour, result.minute,\n                 result.second, result.microsecond);\n    }\n\n    return result;\n}\n</code></pre>"},{"location":"TOOLBOX/TIME/code/#maincpp","title":"main.cpp","text":"<pre><code>/**\n * @file main.cpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief Main program for testing tiny_time module\n * @version 1.0\n * @date 2025-10-22\n * \n * @copyright Copyright (c) 2024\n * \n */\n\n/* DEPENDENCIES */\n// ESP\n#include \"nvs_flash.h\"\n#include \"esp_log.h\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n// FreeRTOS (must be inside extern \"C\" for C++ files)\n#include \"freertos/FreeRTOS.h\"\n#include \"freertos/task.h\"\n#include \"freertos/event_groups.h\"\n#include \"freertos/semphr.h\"\n\n// ESP Timer (high-precision timer)\n#include \"esp_timer.h\"\n\n// WiFi (required for time sync)\n#include \"node_wifi.h\"\n\n// TinyToolbox\n#include \"tiny_time.h\"\n\n/* Variables */\nconst char *TAG = \"tiny_time_test\";\n\n/* Timer precision test variables */\n#define TIMESTAMP_COUNT 15\nstatic TinyTimeMark_t s_timestamps[TIMESTAMP_COUNT] = {0};\nstatic int s_timestamp_index = 0;\nstatic bool s_timer_test_complete = false;\nstatic esp_timer_handle_t s_timer_handle = NULL;\nstatic SemaphoreHandle_t s_timer_mutex = NULL;\n\n/**\n * @brief Timer callback function - records timestamp at the very beginning\n * @param arg Timer argument (not used)\n * @return None\n */\nstatic void timer_precision_callback(void *arg)\n{\n    // CRITICAL: Get timestamp IMMEDIATELY at the start of callback\n    // to avoid any execution overhead affecting the measurement\n    TinyTimeMark_t timestamp = tiny_get_running_time();\n\n    // Store timestamp in array (thread-safe access)\n    if (xSemaphoreTake(s_timer_mutex, portMAX_DELAY) == pdTRUE)\n    {\n        if (s_timestamp_index &lt; TIMESTAMP_COUNT)\n        {\n            s_timestamps[s_timestamp_index++] = timestamp;\n\n            // Stop timer when we have collected all timestamps\n            if (s_timestamp_index &gt;= TIMESTAMP_COUNT)\n            {\n                s_timer_test_complete = true;\n                esp_timer_stop(s_timer_handle);\n            }\n        }\n        xSemaphoreGive(s_timer_mutex);\n    }\n}\n\n/**\n * @brief Entry point of the program - Testing tiny_time module\n * @param None\n * @retval None\n */\nvoid app_main(void)\n{\n    esp_err_t ret;\n\n    // Initialize NVS (required for WiFi)\n    ret = nvs_flash_init();\n    if (ret == ESP_ERR_NVS_NO_FREE_PAGES || ret == ESP_ERR_NVS_NEW_VERSION_FOUND)\n    {\n        ESP_ERROR_CHECK(nvs_flash_erase());\n        ret = nvs_flash_init();\n    }\n    ESP_ERROR_CHECK(ret);\n\n    ESP_LOGI(TAG, \"========================================\");\n    ESP_LOGI(TAG, \"  tiny_time Module Test Program\");\n    ESP_LOGI(TAG, \"========================================\");\n\n    // Initialize WiFi (required for time synchronization)\n    ESP_LOGI(TAG, \"Initializing WiFi...\");\n    ret = wifi_sta_wpa2_init();\n    if (ret != ESP_OK)\n    {\n        ESP_LOGE(TAG, \"WiFi initialization failed!\");\n        return;\n    }\n    ESP_LOGI(TAG, \"WiFi initialized successfully\");\n\n    // Wait for WiFi connection\n    ESP_LOGI(TAG, \"Waiting for WiFi connection...\");\n    EventBits_t ev = xEventGroupWaitBits(wifi_event_group, CONNECTED_BIT, \n                                         pdTRUE, pdFALSE, portMAX_DELAY);\n    if (ev &amp; CONNECTED_BIT)\n    {\n        ESP_LOGI(TAG, \"WiFi connected!\");\n\n        // ============================================================\n        // Test 1: Get running time (microseconds since boot)\n        // ============================================================\n        ESP_LOGI(TAG, \"\\n--- Test 1: Get Running Time ---\");\n        TinyTimeMark_t start_time = tiny_get_running_time();\n        ESP_LOGI(TAG, \"Running time: %lld microseconds\", start_time);\n        ESP_LOGI(TAG, \"Running time: %.3f seconds\", start_time / 1000000.0);\n\n        // ============================================================\n        // Test 2: Sync time with timezone\n        // ============================================================\n        ESP_LOGI(TAG, \"\\n--- Test 2: Sync Time with Timezone ---\");\n        ESP_LOGI(TAG, \"Syncing time with timezone CST-8...\");\n        sync_time_with_timezone(\"CST-8\");\n\n        // Wait for time synchronization (SNTP may take a few seconds)\n        ESP_LOGI(TAG, \"Waiting for time synchronization...\");\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n\n        // ============================================================\n        // Test 3: Get current datetime\n        // ============================================================\n        ESP_LOGI(TAG, \"\\n--- Test 3: Get Current DateTime ---\");\n        (void)tiny_get_current_datetime(true);  // Function prints internally\n\n        // ============================================================\n        // Test 4: Measure time elapsed\n        // ============================================================\n        ESP_LOGI(TAG, \"\\n--- Test 4: Measure Time Elapsed ---\");\n        TinyTimeMark_t end_time = tiny_get_running_time();\n        TinyTimeMark_t elapsed = end_time - start_time;\n        ESP_LOGI(TAG, \"Time elapsed: %lld microseconds\", elapsed);\n        ESP_LOGI(TAG, \"Time elapsed: %.3f seconds\", elapsed / 1000000.0);\n\n        ESP_LOGI(TAG, \"\\n========================================\");\n        ESP_LOGI(TAG, \"  Initial Tests Completed\");\n        ESP_LOGI(TAG, \"========================================\\n\");\n    }\n    else\n    {\n        ESP_LOGE(TAG, \"WiFi connection failed!\");\n        return;\n    }\n\n    // ============================================================\n    // Timer precision test: Record 15 timestamps at 2-second intervals\n    // ============================================================\n    ESP_LOGI(TAG, \"\\n========================================\");\n    ESP_LOGI(TAG, \"  Timer Precision Test\");\n    ESP_LOGI(TAG, \"========================================\");\n    ESP_LOGI(TAG, \"Recording 15 timestamps at 2-second intervals...\");\n    ESP_LOGI(TAG, \"No printing during recording to avoid timing overhead.\\n\");\n\n    // Create mutex for thread-safe access to timestamp array\n    s_timer_mutex = xSemaphoreCreateMutex();\n    if (s_timer_mutex == NULL)\n    {\n        ESP_LOGE(TAG, \"Failed to create mutex!\");\n        return;\n    }\n\n    // Initialize timer\n    const uint64_t TIMER_PERIOD_US = 2000000;  // 2 seconds in microseconds\n    esp_timer_create_args_t timer_args;\n    timer_args.callback = &amp;timer_precision_callback;\n    timer_args.arg = NULL;\n    timer_args.dispatch_method = ESP_TIMER_TASK;  // Execute callback in timer task\n    timer_args.name = \"precision_timer\";\n    timer_args.skip_unhandled_events = false;  // Don't skip events\n\n    ret = esp_timer_create(&amp;timer_args, &amp;s_timer_handle);\n    if (ret != ESP_OK)\n    {\n        ESP_LOGE(TAG, \"Failed to create timer: %s\", esp_err_to_name(ret));\n        vSemaphoreDelete(s_timer_mutex);\n        return;\n    }\n\n    // Start periodic timer\n    ret = esp_timer_start_periodic(s_timer_handle, TIMER_PERIOD_US);\n    if (ret != ESP_OK)\n    {\n        ESP_LOGE(TAG, \"Failed to start timer: %s\", esp_err_to_name(ret));\n        esp_timer_delete(s_timer_handle);\n        vSemaphoreDelete(s_timer_mutex);\n        return;\n    }\n\n    // Wait for all timestamps to be collected\n    ESP_LOGI(TAG, \"Timer started. Waiting for %d timestamps...\", TIMESTAMP_COUNT);\n    while (!s_timer_test_complete)\n    {\n        vTaskDelay(100 / portTICK_PERIOD_MS);  // Check every 100ms\n    }\n\n    // Wait a bit more to ensure timer has stopped\n    vTaskDelay(100 / portTICK_PERIOD_MS);\n\n    // Print all results\n    ESP_LOGI(TAG, \"\\n========================================\");\n    ESP_LOGI(TAG, \"  Timer Precision Test Results\");\n    ESP_LOGI(TAG, \"========================================\");\n    ESP_LOGI(TAG, \"Expected interval: 2000000 microseconds (2.000000 seconds)\\n\");\n\n    for (int i = 0; i &lt; TIMESTAMP_COUNT; i++)\n    {\n        if (i == 0)\n        {\n            // First timestamp - show absolute time\n            ESP_LOGI(TAG, \"Timestamp #%2d: %lld microseconds (%.6f seconds) [baseline]\",\n                     i + 1, s_timestamps[i], s_timestamps[i] / 1000000.0);\n        }\n        else\n        {\n            // Calculate interval from previous timestamp\n            TinyTimeMark_t interval = s_timestamps[i] - s_timestamps[i - 1];\n            int64_t error = interval - 2000000;  // Expected 2 seconds = 2000000 microseconds\n            double error_ms = error / 1000.0;\n\n            ESP_LOGI(TAG, \"Timestamp #%2d: %lld microseconds (%.6f seconds) | \"\n                     \"Interval: %lld us (%.6f s) | Error: %lld us (%.3f ms)\",\n                     i + 1, \n                     s_timestamps[i], \n                     s_timestamps[i] / 1000000.0,\n                     interval,\n                     interval / 1000000.0,\n                     error,\n                     error_ms);\n        }\n    }\n\n    // Calculate statistics\n    ESP_LOGI(TAG, \"\\n--- Statistics ---\");\n    int64_t total_interval = s_timestamps[TIMESTAMP_COUNT - 1] - s_timestamps[0];\n    int64_t expected_total = 2000000 * (TIMESTAMP_COUNT - 1);\n    int64_t total_error = total_interval - expected_total;\n\n    ESP_LOGI(TAG, \"Total time: %lld microseconds (%.6f seconds)\", \n             total_interval, total_interval / 1000000.0);\n    ESP_LOGI(TAG, \"Expected total: %lld microseconds (%.6f seconds)\", \n             expected_total, expected_total / 1000000.0);\n    ESP_LOGI(TAG, \"Total error: %lld microseconds (%.3f milliseconds)\", \n             total_error, total_error / 1000.0);\n\n    // Calculate average interval\n    double avg_interval = (double)total_interval / (TIMESTAMP_COUNT - 1);\n    ESP_LOGI(TAG, \"Average interval: %.6f seconds (%.3f microseconds)\", \n             avg_interval / 1000000.0, avg_interval);\n\n    // Cleanup\n    esp_timer_delete(s_timer_handle);\n    vSemaphoreDelete(s_timer_mutex);\n\n    ESP_LOGI(TAG, \"\\n========================================\");\n    ESP_LOGI(TAG, \"  Test Complete\");\n    ESP_LOGI(TAG, \"========================================\\n\");\n\n    // Main loop: Keep running\n    while (1)\n    {\n        vTaskDelay(10000 / portTICK_PERIOD_MS);\n    }\n}\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"TOOLBOX/TIME/code/#output","title":"output","text":"<pre><code>I (25) boot: ESP-IDF v6.0-dev-1833-g758939caec 2nd stage bootloader\nI (25) boot: compile time Nov  4 2025 23:13:16\nI (25) boot: Multicore bootloader\nI (27) boot: chip revision: v0.2\nI (30) boot: efuse block revision: v1.3\nI (33) qio_mode: Enabling default flash chip QIO\nI (38) boot.esp32s3: Boot SPI Speed : 80MHz\nI (41) boot.esp32s3: SPI Mode       : QIO\nI (45) boot.esp32s3: SPI Flash Size : 16MB\nI (49) boot: Enabling RNG early entropy source...\nI (54) boot: Partition Table:\nI (56) boot: ## Label            Usage          Type ST Offset   Length\nI (62) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (69) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (75) boot:  2 factory          factory app      00 00 00010000 001f0000\nI (82) boot:  3 vfs              Unknown data     01 81 00200000 00a00000\nI (89) boot:  4 storage          Unknown data     01 82 00c00000 00400000\nI (95) boot: End of partition table\nI (98) esp_image: segment 0: paddr=00010020 vaddr=3c0b0020 size=1df80h (122752) map\nI (124) esp_image: segment 1: paddr=0002dfa8 vaddr=3fc99300 size=02070h (  8304) load\nI (126) esp_image: segment 2: paddr=00030020 vaddr=42000020 size=a26fch (665340) map\nI (227) esp_image: segment 3: paddr=000d2724 vaddr=3fc9b370 size=030e0h ( 12512) load\nI (229) esp_image: segment 4: paddr=000d580c vaddr=40374000 size=152ech ( 86764) load\nI (247) esp_image: segment 5: paddr=000eab00 vaddr=50000000 size=00020h (    32) load\nI (256) boot: Loaded app from partition at offset 0x10000\nI (256) boot: Disabling RNG early entropy source...\nI (266) octal_psram: vendor id    : 0x0d (AP)\nI (267) octal_psram: dev id       : 0x02 (generation 3)\nI (267) octal_psram: density      : 0x03 (64 Mbit)\nI (269) octal_psram: good-die     : 0x01 (Pass)\nI (273) octal_psram: Latency      : 0x01 (Fixed)\nI (277) octal_psram: VCC          : 0x01 (3V)\nI (281) octal_psram: SRF          : 0x01 (Fast Refresh)\nI (286) octal_psram: BurstType    : 0x01 (Hybrid Wrap)\nI (291) octal_psram: BurstLen     : 0x01 (32 Byte)\nI (296) octal_psram: Readlatency  : 0x02 (10 cycles@Fixed)\nI (301) octal_psram: DriveStrength: 0x00 (1/1)\nI (306) MSPI Timing: PSRAM timing tuning index: 5\nI (310) esp_psram: Found 8MB PSRAM device\nI (313) esp_psram: Speed: 80MHz\nI (316) cpu_start: Multicore app\nI (752) esp_psram: SPI SRAM memory test OK\nI (760) cpu_start: GPIO 44 and 43 are used as console UART I/O pins\nI (761) cpu_start: Pro cpu start user code\nI (761) cpu_start: cpu freq: 240000000 Hz\nI (762) app_init: Application information:\nI (766) app_init: Project name:     AIoTNode\nI (770) app_init: App version:      0a79117-dirty\nI (775) app_init: Compile time:     Nov  4 2025 23:13:38\nI (780) app_init: ELF file SHA256:  a5e0090b4...\nI (784) app_init: ESP-IDF:          v6.0-dev-1833-g758939caec\nI (789) efuse_init: Min chip rev:     v0.0\nI (793) efuse_init: Max chip rev:     v0.99 \nI (797) efuse_init: Chip rev:         v0.2\nI (801) heap_init: Initializing. RAM available for dynamic allocation:\nI (807) heap_init: At 3FCA2918 len 00046DF8 (283 KiB): RAM\nI (812) heap_init: At 3FCE9710 len 00005724 (21 KiB): RAM\nI (818) heap_init: At 3FCF0000 len 00008000 (32 KiB): DRAM\nI (823) heap_init: At 600FE000 len 00001FE8 (7 KiB): RTCRAM\nI (828) esp_psram: Adding pool of 8192K of PSRAM memory to heap allocator\nI (835) spi_flash: detected chip: boya\nI (838) spi_flash: flash io: qio\nI (841) sleep_gpio: Configure to isolate all GPIO pins in sleep state\nI (847) sleep_gpio: Enable automatic switching of GPIO sleep configuration\nI (854) main_task: Started on CPU0\nI (878) esp_psram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (878) main_task: Calling app_main()\nI (883) tiny_time_test: ========================================\nI (884) tiny_time_test:   tiny_time Module Test Program\nI (889) tiny_time_test: ========================================\nI (895) tiny_time_test: Initializing WiFi...\nI (900) pp: pp rom version: e7ae62f\nI (902) net80211: net80211 rom version: e7ae62f\nI (907) wifi:wifi driver task: 3fcaf644, prio:23, stack:6656, core=0\nI (915) wifi:wifi firmware version: 14da9b7\nI (916) wifi:wifi certification version: v7.0\nI (920) wifi:config NVS flash: enabled\nI (924) wifi:config nano formatting: disabled\nI (928) wifi:Init data frame dynamic rx buffer num: 32\nI (933) wifi:Init static rx mgmt buffer num: 5\nI (937) wifi:Init management short buffer num: 32\nI (941) wifi:Init dynamic tx buffer num: 32\nI (945) wifi:Init static tx FG buffer num: 2\nI (949) wifi:Init static rx buffer size: 1600\nI (953) wifi:Init static rx buffer num: 10\nI (957) wifi:Init dynamic rx buffer num: 32\nI (961) wifi_init: rx ba win: 6\nI (964) wifi_init: accept mbox: 6\nI (967) wifi_init: tcpip mbox: 32\nI (970) wifi_init: udp mbox: 6\nI (973) wifi_init: tcp mbox: 6\nI (975) wifi_init: tcp tx win: 5760\nI (979) wifi_init: tcp rx win: 5760\nI (982) wifi_init: tcp mss: 1440\nI (985) wifi_init: WiFi IRAM OP enabled\nI (988) wifi_init: WiFi RX IRAM OP enabled\nI (992) NODE-WIFI: Setting WiFi configuration SSID NTUSECURE...\nI (999) phy_init: phy_version 701,f4f1da3a,Mar  3 2025,15:50:10\nI (1037) wifi:mode : sta (cc:ba:97:09:a7:50)\nI (1038) wifi:enable tsf\nI (1039) tiny_time_test: WiFi initialized successfully\nI (1040) tiny_time_test: Waiting for WiFi connection...\nI (1107) wifi:new:&lt;1,0&gt;, old:&lt;1,0&gt;, ap:&lt;255,255&gt;, sta:&lt;1,0&gt;, prof:1, snd_ch_cfg:0x0\nI (1108) wifi:state: init -&gt; auth (0xb0)\nI (1111) wifi:state: auth -&gt; assoc (0x0)\nI (1115) wifi:state: assoc -&gt; run (0x10)\nI (1430) wifi:connected with NTUSECURE, aid = 2, channel 1, BW20, bssid = a8:9d:21:3c:12:b1\nI (1430) wifi:security: WPA2-ENT, phy: bgn, rssi: -66\nI (1432) wifi:pm start, type: 1\n\nI (1435) wifi:dp: 1, bi: 104448, li: 2, scale listen interval from 307200 us to 208896 us\nI (1443) wifi:set rx beacon pti, rx_bcn_pti: 0, bcn_timeout: 25000, mt_pti: 0, mt_time: 10000\nI (1459) wifi:&lt;ba-add&gt;idx:0 (ifx:0, a8:9d:21:3c:12:b1), tid:0, ssn:1200, winSize:64\nI (1488) wifi:AP's beacon interval = 104448 us, DTIM period = 1\nI (2467) esp_netif_handlers: sta ip: 10.91.180.236, mask: 255.255.0.0, gw: 10.91.255.254\nI (2467) tiny_time_test: WiFi connected!\nI (2467) tiny_time_test: \n--- Test 1: Get Running Time ---\nI (2473) tiny_time_test: Running time: 1644833 microseconds\nI (2478) tiny_time_test: Running time: 1.645 seconds\nI (2483) tiny_time_test: \n--- Test 2: Sync Time with Timezone ---\nI (2489) tiny_time_test: Syncing time with timezone CST-8...\nI (2494) NTP_SYNC: Initializing SNTP\nI (2498) NTP_SYNC: Waiting for system time to be set... (1/15)\nI (4503) NTP_SYNC: Waiting for system time to be set... (2/15)\nI (4715) NTP_SYNC: Time synchronized!\nI (6503) NTP_SYNC: System time is set.\nI (6503) NTP_SYNC: Current time: Tue Nov 04 23:15:34 2025\nI (6503) tiny_time_test: Waiting for time synchronization...\nI (11506) tiny_time_test: \n--- Test 3: Get Current DateTime ---\nI (11506) TIME: Current Time: 2025-11-04 23:15:39.003179\nI (11506) tiny_time_test: \n--- Test 4: Measure Time Elapsed ---\nI (11511) tiny_time_test: Time elapsed: 9038406 microseconds\nI (11517) tiny_time_test: Time elapsed: 9.038 seconds\nI (11521) tiny_time_test: \n========================================\nI (11527) tiny_time_test:   Initial Tests Completed\nI (11532) tiny_time_test: ========================================\n\nI (11538) tiny_time_test: \n========================================\nI (11544) tiny_time_test:   Timer Precision Test\nI (11548) tiny_time_test: ========================================\nI (11554) tiny_time_test: Recording 15 timestamps at 2-second intervals...\nI (11561) tiny_time_test: No printing during recording to avoid timing overhead.\n\nI (11568) tiny_time_test: Timer started. Waiting for 15 timestamps...\nI (41674) tiny_time_test: \n========================================\nI (41674) tiny_time_test:   Timer Precision Test Results\nI (41674) tiny_time_test: ========================================\nI (41680) tiny_time_test: Expected interval: 2000000 microseconds (2.000000 seconds)\n\nI (41687) tiny_time_test: Timestamp # 1: 12740383 microseconds (12.740383 seconds) [baseline]\nI (41696) tiny_time_test: Timestamp # 2: 14740381 microseconds (14.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41708) tiny_time_test: Timestamp # 3: 16740383 microseconds (16.740383 seconds) | Interval: 2000002 us (2.000002 s) | Error: 2 us (0.002 ms)\nI (41721) tiny_time_test: Timestamp # 4: 18740383 microseconds (18.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41733) tiny_time_test: Timestamp # 5: 20740383 microseconds (20.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41746) tiny_time_test: Timestamp # 6: 22740383 microseconds (22.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41759) tiny_time_test: Timestamp # 7: 24740382 microseconds (24.740382 seconds) | Interval: 1999999 us (1.999999 s) | Error: -1 us (-0.001 ms)\nI (41771) tiny_time_test: Timestamp # 8: 26740383 microseconds (26.740383 seconds) | Interval: 2000001 us (2.000001 s) | Error: 1 us (0.001 ms)\nI (41784) tiny_time_test: Timestamp # 9: 28740383 microseconds (28.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41797) tiny_time_test: Timestamp #10: 30740383 microseconds (30.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41809) tiny_time_test: Timestamp #11: 32740383 microseconds (32.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41822) tiny_time_test: Timestamp #12: 34740381 microseconds (34.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41834) tiny_time_test: Timestamp #13: 36740383 microseconds (36.740383 seconds) | Interval: 2000002 us (2.000002 s) | Error: 2 us (0.002 ms)\nI (41847) tiny_time_test: Timestamp #14: 38740383 microseconds (38.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41860) tiny_time_test: Timestamp #15: 40740381 microseconds (40.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41872) tiny_time_test: \n--- Statistics ---\nI (41877) tiny_time_test: Total time: 27999998 microseconds (27.999998 seconds)\nI (41884) tiny_time_test: Expected total: 28000000 microseconds (28.000000 seconds)\nI (41891) tiny_time_test: Total error: -2 microseconds (-0.002 milliseconds)\nI (41898) tiny_time_test: Average interval: 2.000000 seconds (1999999.857 microseconds)\nI (41906) tiny_time_test: \n========================================\nI (41912) tiny_time_test:   Test Complete\nI (41915) tiny_time_test: ========================================\n</code></pre>"},{"location":"TOOLBOX/TIME/log/","title":"LOG","text":"<p>2025-04-10</p> <ul> <li>Get Running Time: <code>tiny_get_running_time()</code></li> <li>SNTP Time Synchronization: <code>sync_time_with_timezone(\"CST-8\")</code></li> <li>Get World Time: <code>tiny_get_current_datetime(1)</code></li> </ul> <p>TODO:</p> <ul> <li>Local Time Synchronization for Wireless Sensor Networks - Microsecond Level</li> </ul>"},{"location":"TOOLBOX/TIME/notes/","title":"TIME","text":"<p>Time</p> <p>Time related functions are of vital importance for MCU devices. This section provides a series of time related definitions and functions for developers to use.</p> <p>In MCU, time can be divided into the following types:</p> <ul> <li> <p>Running Time: The time from the power-on of the MCU to now.</p> </li> <li> <p>World Time: The time of the time zone where the MCU is located. World time can be represented by standard year, month, day, hour, minute, and second, or it can be represented as a UNIX timestamp.</p> </li> </ul>"},{"location":"TOOLBOX/TIME/notes/#running-time","title":"RUNNING TIME","text":"<p>ESP has its own function to get the running time, <code>esp_timer_get_time</code>, which depends on the <code>esp_timer</code> library. This function returns the time from power-on to now, in microseconds.</p> <p>To facilitate usage, TinyToolbox redefines the data type <code>TinyTimeMark_t</code> and provides a function <code>tiny_get_running_time</code> to get the running time. The time returned by this function is in the unit of int64_t, which is long enough to avoid overflow.</p> <pre><code>typedef int64_t TinyTimeMark_t;\n</code></pre> <pre><code>/**\n * @brief Get the running time in microseconds\n * @return TinyTimeMark_t\n */\nTinyTimeMark_t tiny_get_running_time(void) { return esp_timer_get_time(); }\n</code></pre> <p>Usage reference:</p> <pre><code>void app_main(void)\n{\n    // Get running time\n    TinyTimeMark_t running_time = tiny_get_running_time();\n    ESP_LOGI(TAG_TIME, \"Running Time: %lld us\", running_time);\n}\n</code></pre>"},{"location":"TOOLBOX/TIME/notes/#world-time","title":"WORLD TIME","text":"<p>Warning</p> <p>Note that obtaining world time requires a successful network connection. In other words, the function to obtain world time needs to be called after the network connection is successfully established.</p>"},{"location":"TOOLBOX/TIME/notes/#ntp-time-synchronization","title":"NTP TIME SYNCHRONIZATION","text":"<p>NTP Time Synchronization</p> <p>NTP (Network Time Protocol) is a protocol used to synchronize time in computer networks. It can obtain accurate time information through the Internet or local area network. NTP protocol uses UDP for communication, with the default port being 123. NTP servers periodically send time information to clients, and clients adjust their system time based on this information.</p> <pre><code>    Client                      Server\n      |-------------------&gt;      |     T1\uff1aRequest sent\n      |                          |\n      |         &lt;--------------- |     T2/T3\uff1aServer received &amp; replied\n      |                          |\n      |-------------------&gt;      |     T4\uff1aClient received response\n</code></pre> <p>NTP Time Synchronization Principle</p> <p>NTP time synchronization is based on four timestamps: 1. Timestamp T1 when the client sends the request 2. Timestamp T2 when the server receives the request 3. Timestamp T3 when the server sends the response 4. Timestamp T4 when the client receives the response. Based on these four timestamps, we can calculate Network Delay Delay = (T4 - T1) - (T3 - T2), and Time Offset Offset = ((T2 - T1) + (T3 - T4)) / 2.</p> <p>ESP32 SNTP Time Synchronization</p> <p>In ESP32, SNTP (Simple Network Time Protocol) is used. SNTP is a simplified version of NTP, suitable for scenarios where time accuracy is not critical. The time synchronization in ESP32 relies on the <code>esp_sntp</code> library. The working principle of SNTP is similar to that of NTP, but the implementation of SNTP is relatively simple, making it suitable for embedded devices. Its accuracy is usually at the millisecond level, which is sufficient for most application scenarios.</p> <p>First, define a callback function to receive time synchronization notifications:</p> <pre><code>/* WORLD CURRENT TIME - SNTP */\n/**\n * @brief Callback function for time synchronization notification\n * @param tv Pointer to the timeval structure containing the synchronized time\n * @return None\n */\nstatic void time_sync_notification_cb(struct timeval *tv)\n{\n    ESP_LOGI(TAG_SNTP, \"Time synchronized!\");\n}\n</code></pre> <p>Next is the SNTP initialization function, which is also the core function of time synchronization. It is usually called when the system is initialized and the network is connected. Note that the time synchronization server address can be modified as needed. After the time synchronization is completed, ESP32 will set the local time at the bottom layer.</p> <pre><code>/**\n * @brief Initialize SNTP\n * @note This function can be called multiple times if needed\n * @return None\n */\nstatic void initialize_sntp(void)\n{\n    ESP_LOGI(TAG_SNTP, \"Initializing SNTP\");\n    esp_sntp_setoperatingmode(SNTP_OPMODE_POLL);\n    esp_sntp_setservername(0, \"pool.ntp.org\"); // NTP server // pool.ntp.org // ntp.aliyun.com\n    esp_sntp_set_time_sync_notification_cb(time_sync_notification_cb);\n    esp_sntp_init();\n}\n</code></pre> <p>Next is a further encapsulation of the above functions, including time zone settings. Note that the following function includes the RTC setting <code>rtc_set_time</code>, which depends on the RTC driver at the driver layer. Here I use my custom rtc driver, if there is no related function, you can comment it out directly.</p> <pre><code>/**\n * @brief Obtain the current time with timezone\n * @param timezone_str Timezone string (e.g., \"CST-8\" or \"GMT+8\")\n * @note The timezone string format should be compatible with POSIX TZ format\n * (e.g., \"CST-8\", \"GMT+8\")\n * @note To use this function, in application, after internet connection, call\n * sync_time_with_timezone(\"CST-8\")\n * @return None\n */\nvoid sync_time_with_timezone(const char *timezone_str)\n{\n    // Validate input parameter\n    if (timezone_str == NULL)\n    {\n        ESP_LOGE(TAG_SNTP, \"timezone_str is NULL\");\n        return;\n    }\n\n    // Set system timezone\n    if (setenv(\"TZ\", timezone_str, 1) != 0)\n    {\n        ESP_LOGE(TAG_SNTP, \"Failed to set timezone environment variable\");\n        return;\n    }\n    tzset();\n\n    // Initialize SNTP and start time sync\n    initialize_sntp();\n\n    // Wait for system time to be set\n    time_t now = 0;\n    struct tm timeinfo = {0};\n    int retry = 0;\n    const int retry_count = 15;\n\n    while (timeinfo.tm_year &lt; MIN_VALID_YEAR_OFFSET &amp;&amp; ++retry &lt; retry_count)\n    {\n        ESP_LOGI(TAG_SNTP, \"Waiting for system time to be set... (%d/%d)\", retry,\n                 retry_count);\n        vTaskDelay(2000 / portTICK_PERIOD_MS);\n        time(&amp;now);\n        if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n        {\n            ESP_LOGW(TAG_SNTP, \"Failed to convert time to local time\");\n            continue;\n        }\n    }\n\n    if (timeinfo.tm_year &gt;= MIN_VALID_YEAR_OFFSET)\n    {\n        rtc_set_time(timeinfo.tm_year + 1900, timeinfo.tm_mon + 1, timeinfo.tm_mday,\n                     timeinfo.tm_hour, timeinfo.tm_min,\n                     timeinfo.tm_sec); // defined in esp_rtc.c\n        ESP_LOGI(TAG_SNTP, \"System time is set.\");\n    }\n    else\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to sync time.\");\n        return;\n    }\n\n    // Log current local time (using thread-safe formatting)\n    char time_str[64];\n    if (strftime(time_str, sizeof(time_str), \"%a %b %d %H:%M:%S %Y\", &amp;timeinfo) ==\n        0)\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to format time string\");\n    }\n    else\n    {\n        ESP_LOGI(TAG_SNTP, \"Current time: %s\", time_str);\n    }\n\n    // vTaskDelay(10000 / portTICK_PERIOD_MS); // Wait for 10 second\n    // rtc_get_time(); // uncomment to check the RTC time\n    // ESP_LOGI(TAG_SNTP, \"Current RTC time: %04d-%02d-%02d %02d:%02d:%02d\",\n    //          calendar.year, calendar.month, calendar.date,\n    //          calendar.hour, calendar.min, calendar.sec); // uncomment to check\n    //          the RTC time\n}\n</code></pre>"},{"location":"TOOLBOX/TIME/notes/#world-time-getting","title":"WORLD TIME GETTING","text":"<p>In order to facilitate the acquisition of world time, we first define a data structure <code>DateTime_t</code> to store information such as year, month, day, hour, minute, and second. Then we define a function <code>tiny_get_current_datetime</code> to obtain the current world time. This function returns a <code>DateTime_t</code> structure, which contains the current year, month, day, hour, minute, and second information. When using it, pass in a Boolean value <code>print_flag</code> to control whether to print the current time.</p> <pre><code>/**\n * @brief Structure to hold date and time\n */\ntypedef struct TinyDateTime_t\n{\n    int year;\n    int month;\n    int day;\n    int hour;\n    int minute;\n    int second;\n    long microsecond;\n} TinyDateTime_t; \n</code></pre> <pre><code>/* WORLD CURRENT TIME - GET TIME */\n/**\n * @name tiny_get_current_datetime\n * @brief Get the current time as a TinyDateTime_t struct\n * @param print_flag Flag to indicate whether to print the time\n * @return TinyDateTime_t structure containing the current date and time\n */\nTinyDateTime_t tiny_get_current_datetime(bool print_flag)\n{\n    TinyDateTime_t result = {0}; // Initialize to zero\n    struct timeval tv;\n\n    // Get current time (seconds + microseconds)\n    if (gettimeofday(&amp;tv, NULL) != 0)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to get time of day\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    time_t now = tv.tv_sec;\n    struct tm timeinfo;\n    if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to convert time to local time\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    result.year = timeinfo.tm_year + 1900;\n    result.month = timeinfo.tm_mon + 1;\n    result.day = timeinfo.tm_mday;\n    result.hour = timeinfo.tm_hour;\n    result.minute = timeinfo.tm_min;\n    result.second = timeinfo.tm_sec;\n    result.microsecond = (int32_t)tv.tv_usec; // Explicit cast for portability\n\n    if (print_flag)\n    {\n        ESP_LOGI(TAG_TIME, \"Current Time: %04d-%02d-%02d %02d:%02d:%02d.%06d\",\n                 result.year, result.month, result.day, result.hour, result.minute,\n                 result.second, result.microsecond);\n    }\n\n    return result;\n}\n</code></pre> <p>Usage</p> <pre><code>void app_main(void)\n{\n    // Initialize SNTP and sync time\n    sync_time_with_timezone(\"CST-8\");\n\n    // Get current time\n    TinyDateTime_t current_time = tiny_get_current_datetime(true);\n\n    // Print current time\n    ESP_LOGI(TAG_TIME, \"Current Time: %04d-%02d-%02d %02d:%02d:%02d.%06ld\",\n             current_time.year, current_time.month, current_time.day,\n             current_time.hour, current_time.minute, current_time.second, current_time.microsecond);\n}\n</code></pre> <p>Example Output</p> <p></p> <p>Danger</p> <p>The SNTP accuracy when syncing to RTC is at the second level, so the microsecond part when obtaining world time may not be accurate and is for reference only.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"zh/","title":"TINYAUTON: \u9762\u5411\u5fae\u63a7\u5236\u5668\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u8d4b\u80fd\u6846\u67b6","text":""},{"location":"zh/#_1","title":"\u5173\u4e8e\u672c\u9879\u76ee","text":"<p>\u8fd9\u4e2a\u9879\u76ee\u81f4\u529b\u4e8e\u5f00\u53d1\u4e00\u4e2a\u8fd0\u884c\u5728 MCU \u8bbe\u5907\u4e0a\u7684\u5c0f\u578b\u667a\u80fd\u4f53\u76f8\u5173\u7684\u8ba1\u7b97\u5e93\uff0c\u4ee5\u670d\u52a1\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6db5\u76d6\u6570\u5b66\u8fd0\u7b97\u3001\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u548c TinyML\u3002</p> <p>\u540d\u5b57\u7684\u7531\u6765</p> <p>\"TinyAuton\" \u662f \"Tiny\" \u548c \"Auton\" \u7684\u7ec4\u5408\u3002\"Tiny\" \u610f\u5473\u7740\u667a\u80fd\u4f53\u88ab\u8bbe\u8ba1\u4e3a\u8fd0\u884c\u5728 MCU \u8bbe\u5907\u4e0a\uff0c\u800c \"Auton\" \u662f \"Autonomous Agent\" \u7684\u7f29\u5199\u3002</p>"},{"location":"zh/#_2","title":"\u76ee\u6807\u786c\u4ef6","text":"<ul> <li>MCU \u8bbe\u5907\uff08\u76ee\u524d\u4ee5 ESP32 \u4e3a\u4e3b\u8981\u76ee\u6807\uff09</li> </ul>"},{"location":"zh/#_3","title":"\u8986\u76d6\u8303\u56f4","text":"<ul> <li>\u5e73\u53f0\u9002\u914d\u4e0e\u5404\u7c7b\u5404\u7c7b\u5de5\u5177\uff08\u65f6\u95f4\u3001\u901a\u8baf\u7b49\uff09</li> <li>\u57fa\u672c\u6570\u5b66\u8fd0\u7b97</li> <li>\u6570\u5b57\u4fe1\u53f7\u5904\u7406</li> <li>TinyML / \u8fb9\u7f18\u4eba\u5de5\u667a\u80fd</li> </ul>"},{"location":"zh/#_4","title":"\u5f00\u53d1\u8f7d\u4f53","text":"<p>Tip</p> <p>\u4ee5\u4e0b\u786c\u4ef6\u4ec5\u505a\u5c55\u793a\u7528\u9014\uff0c\u672c\u9879\u76ee\u5e76\u4e0d\u5c40\u9650\u4e8e\u6b64\uff0c\u53ef\u4ee5\u79fb\u690d\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u786c\u4ef6\u4e0a\u3002</p> <ul> <li>Alientek \u7684 DNESP32S3M\uff08ESP32-S3\uff09</li> </ul> <p></p> <p></p> <ul> <li> <p> NexNode</p> <p>  \u4ee3\u7801 </p> <p>  \u6587\u6863 </p> </li> </ul>"},{"location":"zh/#_5","title":"\u9879\u76ee\u67b6\u6784","text":"<pre><code>+------------------------------+\n| \u5e94\u7528\u5c42                        |\n+------------------------------+\n|   - TinyAI                   | &lt;-- AI \u51fd\u6570\n|   - TinyDSP                  | &lt;-- DSP \u51fd\u6570\n|   - TinyMath                 | &lt;-- \u5e38\u7528\u6570\u5b66\u51fd\u6570\n|   - TinyToolbox              | &lt;-- \u5e73\u53f0\u5e95\u5c42\u4f18\u5316 + \u5404\u79cd\u5de5\u5177\n| \u4e2d\u95f4\u4ef6                        |\n+------------------------------+\n| \u9a71\u52a8\u5c42                        |\n+------------------------------+\n| \u786c\u4ef6\u5c42                        |\n+------------------------------+\n</code></pre>"},{"location":"zh/AI/ai/","title":"\u4eba\u5de5\u667a\u80fd","text":""},{"location":"zh/ARCHITECTURE/architecture/","title":"\u67b6\u6784","text":""},{"location":"zh/ARCHITECTURE/architecture/#_2","title":"\u5206\u5c42\u67b6\u6784","text":"<pre><code>+------------------------------+\n| AI                           | &lt;-- \u57fa\u4e8e\u4f4e\u7ea7\u51fd\u6570\u7684\u8fb9\u7f18\u8bbe\u5907 AI/ML \u51fd\u6570\n+------------------------------+\n| DSP                          | &lt;-- \u6570\u5b57\u4fe1\u53f7\u5904\u7406\u51fd\u6570\n+------------------------------+\n| Math Operations              | &lt;-- \u5404\u79cd\u5e94\u7528\u7684\u5e38\u7528\u6570\u5b66\u51fd\u6570\n+------------------------------+\n| Adaptation/Toolbox Layer     | &lt;-- \u7528\u5e73\u53f0\u4f18\u5316/\u7279\u5b9a\u51fd\u6570\u66ff\u6362\u6807\u51c6 C \u4e2d\u7684\u51fd\u6570\n+------------------------------+\n</code></pre>"},{"location":"zh/DSP/dsp/","title":"\u6570\u5b57\u4fe1\u53f7\u5904\u7406","text":"<p>Note</p> <p>\u8be5\u7ec4\u4ef6\u7528\u4e8e\u65e8\u5728\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4fe1\u53f7\u5904\u7406\u7684\u4e00\u7cfb\u5217\u51fd\u6570\uff0c\u8bbe\u8ba1\u4e3b\u65e8\u4e3a\u8f7b\u91cf\u9ad8\u6548\uff0c\u8303\u56f4\u4e3a\u5e38\u7528\u91cd\u8981\u7684\u4fe1\u53f7\u5904\u7406\u7b97\u6cd5\u3002</p> <p>Note</p> <p>\u8be5\u7ec4\u4ef6\u57fa\u4e8eESP32\u5b98\u65b9\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u5e93 ESP-DSP \u8fdb\u884c\u5c01\u88c5\u548c\u6269\u5c55\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u6b21\u7684API\u63a5\u53e3\u3002\u5148\u524dTinyMath\u5df2\u7ecf\u5bf9\u5e94\u4e86ESP-DSP\u4e2d\u7684Math, Matrix, DotProduct\u6a21\u5757\uff0cESP-DSP\u4e2d\u7684\u5176\u4f59\u6a21\u5757\u5bf9\u5e94\u672c\u7ec4\u4ef6TinyDSP\u5e93\u3002\u9664\u6b64\u4ee5\u5916\uff0cTinyDSP\u8fd8\u63d0\u4f9b\u4e86ESP-DSP\u4e2d\u672a\u66fe\u63d0\u4f9b\u7684\u4e00\u4e9b\u529f\u80fd\uff0c\u91cd\u70b9\u8986\u76d6\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u7b49\u573a\u666f\u3002</p>"},{"location":"zh/DSP/dsp/#_2","title":"\u7ec4\u4ef6\u4f9d\u8d56","text":"<pre><code>set(src_dirs\n    .\n    signal\n    filter\n    transform\n    support\n)\n\nset(include_dirs\n    .\n    include\n    signal\n    filter\n    transform\n    support\n)\n\nset(requires\n    tiny_math\n)\n\nidf_component_register(SRC_DIRS ${src_dirs} INCLUDE_DIRS ${include_dirs} REQUIRES ${requires})\n</code></pre>"},{"location":"zh/DSP/dsp/#_3","title":"\u67b6\u6784\u4e0e\u529f\u80fd\u76ee\u5f55","text":""},{"location":"zh/DSP/dsp/#_4","title":"\u4f9d\u8d56\u5173\u7cfb\u793a\u610f\u56fe","text":""},{"location":"zh/DSP/dsp/#_5","title":"\u4ee3\u7801\u6811","text":"<pre><code>tiny_dsp/\n\u251c\u2500\u2500 include/                     \n\u2502   \u251c\u2500\u2500 tiny_dsp.h               # entrance header file\n\u2502   \u2514\u2500\u2500 tiny_dsp_config.h        # dsp module configuration file\n\u2502\n\u251c\u2500\u2500 signal/\n\u2502   \u251c\u2500\u2500 tiny_conv.h              # convolution - header file\n\u2502   \u251c\u2500\u2500 tiny_conv.c              # convolution - source file\n|   \u251c\u2500\u2500 tiny_conv_test.h         # convolution - test header file\n|   \u251c\u2500\u2500 tiny_conv_test.c         # convolution - test source file\n\u2502   \u251c\u2500\u2500 tiny_corr.h              # correlation - header file\n\u2502   \u251c\u2500\u2500 tiny_corr.c              # correlation - source file\n|   \u251c\u2500\u2500 tiny_corr_test.h         # correlation - test header file\n|   \u251c\u2500\u2500 tiny_corr_test.c         # correlation - test source file\n|   \u251c\u2500\u2500 tiny_resample.h          # resampling - header file\n|   \u251c\u2500\u2500 tiny_resample.c          # resampling - source file\n|   \u251c\u2500\u2500 tiny_resample_test.h     # resampling - test header file\n|   \u2514\u2500\u2500 tiny_resample_test.c     # resampling - test source file\n\u2502\n\u251c\u2500\u2500 filter/\n\u2502\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 tiny_dwt.h               # discrete wavelet transform - header file\n\u2502   \u251c\u2500\u2500 tiny_dwt.c               # discrete wavelet transform - source file\n\u2502   \u251c\u2500\u2500 tiny_dwt_test.h          # discrete wavelet transform - test header file\n\u2502   \u2514\u2500\u2500 tiny_dwt_test.c          # discrete wavelet transform - test source\n\u2502\n\u2514\u2500\u2500 support/\n</code></pre>"},{"location":"zh/DSP/HEADER-FILE/tiny_dsp/","title":"TinyDSP \u5934\u6587\u4ef6","text":"<p>Info</p> <p>\u8fd9\u662fTinyDSP\u5e93\u7684\u4e3b\u5934\u6587\u4ef6\u3002\u5b83\u5305\u542b\u6240\u6709\u5fc5\u8981\u7684\u5934\u6587\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u53e3\u6765\u4f7f\u7528\u5e93\u7684\u529f\u80fd\u3002\u5728\u9879\u76ee\u4e2d\u5b8c\u6210\u8be5\u5e93\u7684\u79fb\u690d\u540e\uff0c\u5728\u9700\u8981\u4f7f\u7528\u76f8\u5173\u51fd\u6570\u7684\u5730\u65b9\u63d2\u5165\u8be5\u5934\u6587\u4ef6\u5373\u53ef\u4f7f\u7528\u5e93\u5185\u7684\u6240\u6709\u51fd\u6570\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u4e00\u81f4\uff0c\u8bf7\u4ee5\u5b9e\u9645\u4ee3\u7801\u4e3a\u51c6\u3002</p> <pre><code>/**\n * @file tiny_dsp.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief tiny_dsp | entrance file\n * @version 1.0\n * @date 2025-04-28\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n\n// tiny_dsp configuration file\n#include \"tiny_dsp_config.h\"\n\n// [signal]\n\n// signal - convolution\n#include \"tiny_conv.h\"\n#include \"tiny_conv_test.h\"\n\n// signal - correlation &amp; crosss correlation\n#include \"tiny_corr.h\"\n#include \"tiny_corr_test.h\"\n\n// signal - resample &amp; decimate\n#include \"tiny_resample.h\"\n#include \"tiny_resample_test.h\"\n\n// [filter]\n\n// [transform]\n\n// transform - discrete wavelet transform\n#include \"tiny_dwt.h\"\n#include \"tiny_dwt_test.h\"\n\n// [support]\n\n\n\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/DSP/HEADER-FILE/tiny_dsp_config/","title":"TinyDSP \u914d\u7f6e","text":"<p>Info</p> <p>\u8fd9\u4e2a\u5934\u6587\u4ef6\u8d77\u5230\u914d\u7f6e\u6574\u4e2aTinyDSP\u6a21\u5757\u7684\u4f5c\u7528\uff0c\u6bcf\u4e2a\u5b50\u6a21\u5757\u90fd\u5305\u542b\u4e86\u6b64\u5934\u6587\u4ef6\u3002\u5b83\u5b9a\u4e49\u4e86TinyDSP\u7684\u914d\u7f6e\u9009\u9879\u548c\u5b8f\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u9700\u8981\u8fdb\u884c\u81ea\u5b9a\u4e49\u8bbe\u7f6e\u3002\u901a\u8fc7\u4fee\u6539\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9009\u9879\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u5730\u8c03\u6574TinyDSP\u7684\u884c\u4e3a\u548c\u529f\u80fd\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4f1a\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u4e00\u81f4\uff0c\u8bf7\u4ee5\u4ee3\u7801\u4e3a\u51c6\u3002</p> <p>Tip</p> <p>\u5e73\u53f0\u52a0\u901f\u9009\u9879\u8bf7\u5230TinyMath\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u8bbe\u7f6e\u3002</p> <pre><code>/**\n * @file tiny_dsp_config.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_dsp middleware.\n * @version 1.0\n * @date 2025-04-27\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* DEPENDENCIES */\n#include \"tiny_math.h\"\n\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/code/","title":"\u4ee3\u7801","text":""},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/","title":"\u8bf4\u660e","text":""},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#_2","title":"\u5377\u79ef\u7684\u6570\u5b66\u539f\u7406","text":"<p>\u5377\u79ef\u662f\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4e00\u79cd\u91cd\u8981\u64cd\u4f5c\uff0c\u7528\u4e8e\u63cf\u8ff0\u4e24\u4e2a\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5b83\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u4e2a\u4fe1\u53f7\u4e0e\u53e6\u4e00\u4e2a\u4fe1\u53f7\u7684\u52a0\u6743\u5e73\u5747\u3002\u5377\u79ef\u7684\u6570\u5b66\u5b9a\u4e49\u5982\u4e0b\uff1a</p> \\[y(t) = \\int_{-\\infty}^{\\infty} x(\\tau) h(t - \\tau) d\\tau\\] <p>\u5176\u4e2d\uff0c\\(x(t)\\) \u662f\u8f93\u5165\u4fe1\u53f7\uff0c\\(h(t)\\) \u662f\u7cfb\u7edf\u7684\u8109\u51b2\u54cd\u5e94\uff0c\\(y(t)\\) \u662f\u8f93\u51fa\u4fe1\u53f7\u3002\u5377\u79ef\u7684\u7ed3\u679c\u662f\u4e00\u4e2a\u65b0\u7684\u4fe1\u53f7\uff0c\u5b83\u5305\u542b\u4e86\u8f93\u5165\u4fe1\u53f7\u548c\u7cfb\u7edf\u8109\u51b2\u54cd\u5e94\u4e4b\u95f4\u7684\u6240\u6709\u4fe1\u606f\u3002</p> <p></p> <ul> <li> <p> \u5377\u79ef\u7684\u7269\u7406\u610f\u4e49</p> <p>  \u4f20\u9001\u95e8 </p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#_3","title":"\u7f16\u7a0b\u601d\u8def","text":"<p>\u672c\u5e93\u4e2d\u7684\u5377\u79ef\u64cd\u4f5c\u5b9e\u9645\u4e0a\u662f\u5c06\u5377\u79ef\u6838\u8c03\u8f6c\u65b9\u5411\u7136\u540e\u4e0e\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u9010\u70b9\u76f8\u4e58\u5e76\u6c42\u548c\u3002</p>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_f32","title":"tiny_conv_f32","text":"<pre><code>/**\n * @name: tiny_conv_f32\n * @brief Convolution function\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *convout)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    // ESP32 DSP library\n    dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n#else\n    float *sig = (float *)Signal;\n    float *kern = (float *)Kernel;\n    int lsig = siglen;\n    int lkern = kernlen;\n\n    // stage I\n    for (int n = 0; n &lt; lkern; n++)\n    {\n        size_t k;\n\n        convout[n] = 0;\n\n        for (k = 0; k &lt;= n; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage II\n    for (int n = lkern; n &lt; lsig; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = n;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n\n    // stage III\n    for (int n = lsig; n &lt; lsig + lkern - 1; n++)\n    {\n        size_t kmin, kmax, k;\n\n        convout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = lsig - 1;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            convout[n] += sig[k] * kern[n - k];\n        }\n    }\n#endif\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0\uff1a\u8be5\u51fd\u6570\u5b9e\u73b0\u4e86\u5bf9\u8f93\u5165\u4fe1\u53f7\u548c\u5377\u79ef\u6838\u7684\u5377\u79ef\u64cd\u4f5c\u3002\u5b83\u9996\u5148\u68c0\u67e5\u8f93\u5165\u53c2\u6570\u662f\u5426\u4e3aNULL\uff0c\u7136\u540e\u6839\u636e\u5e73\u53f0\u9009\u62e9\u4f7f\u7528ESP32 DSP\u5e93\u6216\u6807\u51c6C\u5b9e\u73b0\u8fdb\u884c\u5377\u79ef\u8ba1\u7b97\u3002\u51fd\u6570\u8fd4\u56de\u5377\u79ef\u7ed3\u679c\u3002</p> <p>\u7279\u70b9\uff1a</p> <ul> <li> <p>\u652f\u6301ESP32 DSP\u5e93\u52a0\u901f</p> </li> <li> <p>\u652f\u6301\u5377\u79ef\u6838\u548c\u4fe1\u53f7\u4e92\u6362\u4ee5\u4fdd\u8bc1\u4fe1\u53f7\u957f\u5ea6\u5927\u4e8e\u5377\u79ef\u6838\u957f\u5ea6</p> </li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>Signal</code>\uff1a\u8f93\u5165\u4fe1\u53f7\u6570\u7ec4</p> </li> <li> <p><code>siglen</code>\uff1a\u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>Kernel</code>\uff1a\u8f93\u5165\u5377\u79ef\u6838\u6570\u7ec4</p> </li> <li> <p><code>kernlen</code>\uff1a\u8f93\u5165\u5377\u79ef\u6838\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>convout</code>\uff1a\u8f93\u51fa\u6570\u7ec4\uff0c\u7528\u4e8e\u5b58\u50a8\u5377\u79ef\u7ed3\u679c</p> </li> </ul> <p>\u8fd4\u56de\u503c\uff1a</p> <ul> <li> <p><code>TINY_OK</code>\uff1a\u5377\u79ef\u6210\u529f</p> </li> <li> <p><code>TINY_ERR_DSP_NULL_POINTER</code>\uff1a\u8f93\u5165\u53c2\u6570\u4e3aNULL</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_ex_f32","title":"tiny_conv_ex_f32","text":"<pre><code>/**\n * @name: tiny_conv_ex_f32\n * @brief Extended convolution function with padding and mode options\n *\n * @param Signal The input signal array\n * @param siglen The length of the input signal array\n * @param Kernel The input kernel array\n * @param kernlen The length of the input kernel array\n * @param convout The output array for the convolution result\n * @param padding_mode Padding mode (zero, symmetric, periodic)\n * @param conv_mode Convolution mode (full, head, center, tail)\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_conv_ex_f32(const float *Signal, const int siglen,\n                              const float *Kernel, const int kernlen,\n                              float *convout,\n                              tiny_padding_mode_t padding_mode,\n                              tiny_conv_mode_t conv_mode)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == convout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n    if (siglen &lt;= 0 || kernlen &lt;= 0)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n    if (siglen &lt; kernlen)\n    {\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    if (padding_mode == TINY_PADDING_ZERO &amp;&amp; conv_mode == TINY_CONV_FULL)\n    {\n        dsps_conv_f32(Signal, siglen, Kernel, kernlen, convout);\n        return TINY_OK;\n    }\n#endif\n\n    int pad_len = kernlen - 1;\n    int padded_len = siglen + 2 * pad_len;\n    float *padded_signal = (float *)calloc(padded_len, sizeof(float));\n    if (padded_signal == NULL)\n    {\n        return TINY_ERR_DSP_MEMORY_ALLOC;\n    }\n\n    // Fill padded signal\n    switch (padding_mode)\n    {\n    case TINY_PADDING_ZERO:\n        // Middle copy only, left and right are zeros (calloc already zeroed)\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen);\n        break;\n\n    case TINY_PADDING_SYMMETRIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[i];                   // Mirror left\n            padded_signal[pad_len + siglen + i] = Signal[siglen - 1 - i]; // Mirror right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    case TINY_PADDING_PERIODIC:\n        for (int i = 0; i &lt; pad_len; i++)\n        {\n            padded_signal[pad_len - 1 - i] = Signal[(siglen - pad_len + i) % siglen]; // Wrap left\n            padded_signal[pad_len + siglen + i] = Signal[i % siglen];                 // Wrap right\n        }\n        memcpy(padded_signal + pad_len, Signal, sizeof(float) * siglen); // Copy center\n        break;\n\n    default:\n        free(padded_signal);\n        return TINY_ERR_DSP_INVALID_PARAM;\n    }\n\n    // Full convolution\n    int convlen_full = siglen + kernlen - 1;\n    for (int n = 0; n &lt; convlen_full; n++)\n    {\n        float sum = 0.0f;\n        for (int k = 0; k &lt; kernlen; k++)\n        {\n            sum += padded_signal[n + k] * Kernel[kernlen - 1 - k]; // Convolution is flip+slide\n        }\n        convout[n] = sum;\n    }\n\n    free(padded_signal);\n\n    // Handle output mode\n    if (conv_mode == TINY_CONV_FULL)\n    {\n        return TINY_OK;\n    }\n    else\n    {\n        int start_idx = 0;\n        int out_len = 0;\n\n        switch (conv_mode)\n        {\n        case TINY_CONV_HEAD:\n            start_idx = 0;\n            out_len = kernlen;\n            break;\n        case TINY_CONV_CENTER:\n            start_idx = (kernlen - 1) / 2;\n            out_len = siglen;\n            break;\n        case TINY_CONV_TAIL:\n            start_idx = siglen - 1;\n            out_len = kernlen;\n            break;\n        default:\n            return TINY_ERR_DSP_INVALID_MODE;\n        }\n\n        // Copy the selected part to the beginning\n        for (int i = 0; i &lt; out_len; i++)\n        {\n            convout[i] = convout[start_idx + i];\n        }\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0\uff1a\u8be5\u51fd\u6570\u5b9e\u73b0\u4e86\u5bf9\u8f93\u5165\u4fe1\u53f7\u548c\u5377\u79ef\u6838\u7684\u6269\u5c55\u5377\u79ef\u64cd\u4f5c\uff0c\u652f\u6301\u4e0d\u540c\u7684\u586b\u5145\u6a21\u5f0f\u548c\u8f93\u51fa\u6a21\u5f0f\u3002\u5b83\u9996\u5148\u68c0\u67e5\u8f93\u5165\u53c2\u6570\u662f\u5426\u4e3aNULL\uff0c\u7136\u540e\u6839\u636e\u5e73\u53f0\u9009\u62e9\u4f7f\u7528ESP32 DSP\u5e93\u6216\u6807\u51c6C\u5b9e\u73b0\u8fdb\u884c\u5377\u79ef\u8ba1\u7b97\u3002\u51fd\u6570\u8fd4\u56de\u5377\u79ef\u7ed3\u679c\u3002</p> <p>\u7279\u70b9\uff1a</p> <ul> <li> <p>\u652f\u6301ESP32 DSP\u5e93\u52a0\u901f</p> </li> <li> <p>\u652f\u6301\u591a\u79cd\u586b\u5145\u6a21\u5f0f\uff08\u96f6\u586b\u5145\u3001\u5bf9\u79f0\u586b\u5145\u3001\u5468\u671f\u586b\u5145\uff09</p> </li> <li> <p>\u652f\u6301\u591a\u79cd\u8f93\u51fa\u6a21\u5f0f\uff08\u5b8c\u6574\u5377\u79ef\u3001\u5934\u90e8\u5377\u79ef\u3001\u4e2d\u5fc3\u5377\u79ef\u3001\u5c3e\u90e8\u5377\u79ef\uff09</p> </li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>Signal</code>\uff1a\u8f93\u5165\u4fe1\u53f7\u6570\u7ec4</p> </li> <li> <p><code>siglen</code>\uff1a\u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>Kernel</code>\uff1a\u8f93\u5165\u5377\u79ef\u6838\u6570\u7ec4</p> </li> <li> <p><code>kernlen</code>\uff1a\u8f93\u5165\u5377\u79ef\u6838\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>convout</code>\uff1a\u8f93\u51fa\u6570\u7ec4\uff0c\u7528\u4e8e\u5b58\u50a8\u5377\u79ef\u7ed3\u679c</p> </li> <li> <p><code>padding_mode</code>\uff1a\u586b\u5145\u6a21\u5f0f\uff08\u96f6\u586b\u5145\u3001\u5bf9\u79f0\u586b\u5145\u3001\u5468\u671f\u586b\u5145\uff09</p> </li> <li> <p><code>conv_mode</code>\uff1a\u5377\u79ef\u6a21\u5f0f\uff08\u5b8c\u6574\u5377\u79ef\u3001\u5934\u90e8\u5377\u79ef\u3001\u4e2d\u5fc3\u5377\u79ef\u3001\u5c3e\u90e8\u5377\u79ef\uff09</p> </li> </ul> <p>\u8fd4\u56de\u503c\uff1a</p> <ul> <li> <p><code>TINY_OK</code>\uff1a\u5377\u79ef\u6210\u529f</p> </li> <li> <p><code>TINY_ERR_DSP_NULL_POINTER</code>\uff1a\u8f93\u5165\u53c2\u6570\u4e3aNULL</p> </li> <li> <p><code>TINY_ERR_DSP_INVALID_PARAM</code>\uff1a\u8f93\u5165\u53c2\u6570\u65e0\u6548</p> </li> <li> <p><code>TINY_ERR_DSP_MEMORY_ALLOC</code>\uff1a\u5185\u5b58\u5206\u914d\u5931\u8d25</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#_4","title":"\u51fd\u6570\u5bf9\u6bd4","text":"<p>\u4e3a\u4e86\u5e2e\u52a9\u8bfb\u8005\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u51fd\u6570\uff0c\u4ee5\u4e0b\u662f <code>tiny_conv_f32</code> \u548c <code>tiny_conv_ex_f32</code> \u7684\u5bf9\u6bd4\uff1a</p> \u7279\u6027 <code>tiny_conv_f32</code> <code>tiny_conv_ex_f32</code> \u586b\u5145\u6a21\u5f0f \u4ec5\u96f6\u586b\u5145\uff08\u9690\u5f0f\uff09 \u96f6\u586b\u5145\u3001\u5bf9\u79f0\u586b\u5145\u6216\u5468\u671f\u586b\u5145\uff08\u663e\u5f0f\uff09 \u8f93\u51fa\u6a21\u5f0f \u4ec5\u5b8c\u6574\u5377\u79ef \u5b8c\u6574\u3001\u5934\u90e8\u3001\u4e2d\u5fc3\u6216\u5c3e\u90e8\u6a21\u5f0f \u8f93\u51fa\u957f\u5ea6 <code>siglen + kernlen - 1</code> \u6839\u636e <code>conv_mode</code> \u53ef\u914d\u7f6e \u5185\u5b58\u4f7f\u7528 \u65e0\u52a8\u6001\u5206\u914d \u9700\u8981\u4e3a\u586b\u5145\u4fe1\u53f7\u52a8\u6001\u5206\u914d\u5185\u5b58 \u6027\u80fd \u4f18\u5316\uff08ESP32 \u652f\u6301\u786c\u4ef6\u52a0\u901f\uff09 \u4ec5\u5728 ESP32 \u4e0a\u4f7f\u7528\u96f6\u586b\u5145+\u5b8c\u6574\u6a21\u5f0f\u65f6\u4f18\u5316 \u4f7f\u7528\u573a\u666f \u7b80\u5355\u7684\u96f6\u586b\u5145\u5b8c\u6574\u5377\u79ef \u9700\u8981\u81ea\u5b9a\u4e49\u586b\u5145\u548c\u8f93\u51fa\u6a21\u5f0f\u7684\u9ad8\u7ea7\u5377\u79ef"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_f32_1","title":"\u4f55\u65f6\u4f7f\u7528 <code>tiny_conv_f32</code>","text":"<p>\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e0b\u4f7f\u7528 <code>tiny_conv_f32</code>\uff1a</p> <ul> <li> <p>\u9700\u8981\u7b80\u5355\u7684\u5b8c\u6574\u5377\u79ef\u7ed3\u679c</p> </li> <li> <p>\u96f6\u586b\u5145\u5904\u7406\u8fb9\u754c\u662f\u53ef\u63a5\u53d7\u7684</p> </li> <li> <p>\u9700\u8981\u6700\u4f73\u6027\u80fd\uff08\u7279\u522b\u662f\u5728\u652f\u6301\u786c\u4ef6\u52a0\u901f\u7684 ESP32 \u4e0a\uff09</p> </li> <li> <p>\u5e0c\u671b\u907f\u514d\u52a8\u6001\u5185\u5b58\u5206\u914d</p> </li> <li> <p>\u8f93\u51fa\u957f\u5ea6 <code>siglen + kernlen - 1</code> \u662f\u53ef\u63a5\u53d7\u7684</p> </li> </ul> <p>\u793a\u4f8b\u573a\u666f\uff1a</p> <ul> <li> <p>\u57fa\u672c\u4fe1\u53f7\u6ee4\u6ce2</p> </li> <li> <p>\u7b80\u5355\u7684\u76f8\u5173\u8fd0\u7b97</p> </li> <li> <p>\u9700\u8981\u907f\u514d\u5185\u5b58\u5206\u914d\u7684\u5b9e\u65f6\u5904\u7406</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/notes/#tiny_conv_ex_f32_1","title":"\u4f55\u65f6\u4f7f\u7528 <code>tiny_conv_ex_f32</code>","text":"<p>\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e0b\u4f7f\u7528 <code>tiny_conv_ex_f32</code>\uff1a</p> <ul> <li> <p>\u9700\u8981\u4e0d\u540c\u7684\u586b\u5145\u7b56\u7565\uff08\u5bf9\u79f0\u6216\u5468\u671f\u586b\u5145\uff09\u6765\u5904\u7406\u4fe1\u53f7\u8fb9\u754c</p> </li> <li> <p>\u9700\u8981\u63d0\u53d6\u5377\u79ef\u7ed3\u679c\u7684\u7279\u5b9a\u90e8\u5206\uff08\u5934\u90e8\u3001\u4e2d\u5fc3\u6216\u5c3e\u90e8\uff09</p> </li> <li> <p>\u9700\u8981\u8f93\u51fa\u957f\u5ea6\u7b49\u4e8e\u8f93\u5165\u4fe1\u53f7\u957f\u5ea6\uff08\u4e2d\u5fc3\u6a21\u5f0f\uff09</p> </li> <li> <p>\u5904\u7406\u7684\u4fe1\u53f7\u8fb9\u754c\u6548\u5e94\u5f88\u91cd\u8981\uff08\u4f8b\u5982\u56fe\u50cf\u5904\u7406\u3001\u5468\u671f\u4fe1\u53f7\uff09</p> </li> <li> <p>\u53ef\u4ee5\u63a5\u53d7\u52a8\u6001\u5185\u5b58\u5206\u914d</p> </li> </ul> <p>\u793a\u4f8b\u573a\u666f\uff1a</p> <ul> <li> <p>\u4f7f\u7528\u5bf9\u79f0\u586b\u5145\u8fdb\u884c\u56fe\u50cf\u6ee4\u6ce2\u4ee5\u51cf\u5c11\u8fb9\u754c\u4f2a\u5f71</p> </li> <li> <p>\u4f7f\u7528\u5468\u671f\u586b\u5145\u5904\u7406\u5468\u671f\u4fe1\u53f7</p> </li> <li> <p>\u5f53\u8f93\u51fa\u957f\u5ea6\u9700\u8981\u5339\u914d\u8f93\u5165\u65f6\uff0c\u4ec5\u63d0\u53d6\u6709\u6548\u5377\u79ef\u533a\u57df\uff08\u4e2d\u5fc3\u6a21\u5f0f\uff09</p> </li> <li> <p>\u8fb9\u754c\u5904\u7406\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u7ea7\u4fe1\u53f7\u5904\u7406</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/CONVOLUTION/test/","title":"\u6d4b\u8bd5","text":""},{"location":"zh/DSP/SIGNAL/CONVOLUTION/test/#_2","title":"\u6d4b\u8bd5\u7ed3\u679c","text":"<pre><code>===== tiny_conv_f32 and tiny_conv_ex_f32 Test Start =====\nSignal: 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11.00 \nKernel: 0.20 0.50 0.30 \n\n[Test] tiny_conv_f32 basic full convolution...\n[PASS] tiny_conv_f32 full convolution completed.\n\n[Test] tiny_conv_ex_f32 padding=zero, mode=full...\n[PASS] tiny_conv_ex_f32 matches tiny_conv_f32 (zero padding, full mode).\n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=FULL).\nOutput:\n0.20000 0.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.50000 3.30000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=HEAD).\nOutput:\n0.20000 0.90000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=CENTER).\nOutput:\n0.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.50000 \n\n[Test] tiny_conv_ex_f32 padding=ZERO, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=ZERO, mode=TAIL).\nOutput:\n9.90000 8.50000 3.30000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=FULL).\nOutput:\n1.30000 1.20000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 10.70000 10.80000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=HEAD).\nOutput:\n1.30000 1.20000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=CENTER).\nOutput:\n1.20000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 10.70000 \n\n[Test] tiny_conv_ex_f32 padding=SYMMETRIC, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=SYMMETRIC, mode=TAIL).\nOutput:\n9.90000 10.70000 10.80000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=FULL...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=FULL).\nOutput:\n8.50000 3.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.70000 4.20000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=HEAD...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=HEAD).\nOutput:\n8.50000 3.90000 1.90000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=CENTER...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=CENTER).\nOutput:\n3.90000 1.90000 2.90000 3.90000 4.90000 5.90000 6.90000 7.90000 8.90000 9.90000 8.70000 \n\n[Test] tiny_conv_ex_f32 padding=PERIODIC, mode=TAIL...\n[PASS] tiny_conv_ex_f32 completed (padding=PERIODIC, mode=TAIL).\nOutput:\n9.90000 8.70000 4.20000 \n\n===== tiny_conv_f32 and tiny_conv_ex_f32 Test End =====\n</code></pre>"},{"location":"zh/DSP/SIGNAL/CORRELATION/code/","title":"\u4ee3\u7801","text":""},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/","title":"\u8bf4\u660e","text":"<p>\u8bf4\u660e</p> <p>\u76f8\u5173\u6027\u662f\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6982\u5ff5\uff0c\u901a\u5e38\u7528\u4e8e\u5206\u6790\u4fe1\u53f7\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6216\u4f9d\u8d56\u6027\u3002\u5b83\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u90fd\u5f88\u6709\u7528\uff0c\u4f8b\u5982\u6a21\u5f0f\u8bc6\u522b\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u548c\u4fe1\u53f7\u68c0\u6d4b\u3002</p>"},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#_2","title":"\u6ed1\u52a8\u76f8\u5173","text":""},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#_3","title":"\u6570\u5b66\u539f\u7406","text":"<p>\u76f8\u5173\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a</p> \\[ \\text{Correlation}[n] = \\sum_{m=0}^{L_p - 1} S[n + m] \\cdot P[m] \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p>\\( S \\) \u4e3a\u8f93\u5165\u4fe1\u53f7\uff0c\u957f\u5ea6\u4e3a \\( L_s \\)</p> </li> <li> <p>\\( P \\) \u4e3a\u6a21\u5f0f\u5e8f\u5217\uff08Pattern\uff09\uff0c\u957f\u5ea6\u4e3a \\( L_p \\)</p> </li> <li> <p>\\( n \\in [0, L_s - L_p] \\)</p> </li> </ul> <p>\u8f93\u51fa\u957f\u5ea6\u8ba1\u7b97\uff1a</p> \\[ L_{\\text{out}} = L_s - L_p + 1 \\]"},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#tiny_corr_f32","title":"tiny_corr_f32","text":"<pre><code>/**\n * @name: tiny_corr_f32\n * @brief Correlation function\n *\n * @param Signal: input signal array\n * @param siglen: length of the signal array\n * @param Pattern: input pattern array\n * @param patlen: length of the pattern array\n * @param dest: output array for the correlation result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_corr_f32(const float *Signal, const int siglen, const float *Pattern, const int patlen, float *dest)\n{\n    if (NULL == Signal || NULL == Pattern || NULL == dest)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n\n    if (siglen &lt; patlen) // signal length shoudl be greater than pattern length\n    {\n        return TINY_ERR_DSP_MISMATCH;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_corr_f32(Signal, siglen, Pattern, patlen, dest);\n#else\n\n    for (size_t n = 0; n &lt;= (siglen - patlen); n++)\n    {\n        float k_corr = 0;\n        for (size_t m = 0; m &lt; patlen; m++)\n        {\n            k_corr += Signal[n + m] * Pattern[m];\n        }\n        dest[n] = k_corr;\n    }\n\n#endif\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u8ba1\u7b97\u4fe1\u53f7\u548c\u6a21\u5f0f\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002</p> <p>\u7279\u70b9</p> <ul> <li>\u652f\u6301\u5e73\u53f0\u52a0\u901f</li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>Signal</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4</p> </li> <li> <p><code>siglen</code>: \u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>Pattern</code>: \u8f93\u5165\u6a21\u5f0f\u6570\u7ec4</p> </li> <li> <p><code>patlen</code>: \u6a21\u5f0f\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>dest</code>: \u8f93\u51fa\u6570\u7ec4\uff0c\u7528\u4e8e\u5b58\u50a8\u76f8\u5173\u6027\u7ed3\u679c</p> </li> </ul> <p>\u8fd4\u56de\u503c: \u8fd4\u56de\u6210\u529f\u6216\u9519\u8bef\u4ee3\u7801\u3002</p>"},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#_4","title":"\u4ea4\u53c9\u76f8\u5173\u51fd\u6570","text":""},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#_5","title":"\u6570\u5b66\u539f\u7406","text":"<p>\u4e92\u76f8\u5173\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a</p> \\[ R_{xy}[n] = \\sum_{k} x[k] \\cdot y[k + n] \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p>\\( x \\) \u4e3a\u4fe1\u53f7\u5e8f\u5217\uff0c\u957f\u5ea6\u4e3a \\( L_x \\)</p> </li> <li> <p>\\( y \\) \u4e3a\u5377\u79ef\u6838\uff08Kernel\uff09\uff0c\u957f\u5ea6\u4e3a \\( L_y \\)</p> </li> <li> <p>\\( n \\in [0, L_x + L_y - 2] \\)</p> </li> </ul> <p>\u8f93\u51fa\u957f\u5ea6\u8ba1\u7b97\uff1a</p> \\[ L_{\\text{out}} = L_x + L_y - 1 \\]"},{"location":"zh/DSP/SIGNAL/CORRELATION/notes/#tiny_ccorr_f32","title":"tiny_ccorr_f32","text":"<pre><code>/**\n * @name: tiny_ccorr_f32\n * @brief Cross-correlation function\n *\n * @param Signal: input signal array\n * @param siglen: length of the signal array\n * @param Kernel: input kernel array\n * @param kernlen: length of the kernel array\n * @param corrvout: output array for the cross-correlation result\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_ccorr_f32(const float *Signal, const int siglen, const float *Kernel, const int kernlen, float *corrvout)\n{\n    if (NULL == Signal || NULL == Kernel || NULL == corrvout)\n    {\n        return TINY_ERR_DSP_NULL_POINTER;\n    }\n\n    float *sig = (float *)Signal;\n    float *kern = (float *)Kernel;\n    int lsig = siglen;\n    int lkern = kernlen;\n\n    // swap signal and kernel if needed\n    if (siglen &lt; kernlen)\n    {\n        sig = (float *)Kernel;\n        kern = (float *)Signal;\n        lsig = kernlen;\n        lkern = siglen;\n    }\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n    dsps_ccorr_f32(Signal, siglen, Kernel, kernlen, corrvout);\n#else\n    // stage I\n    for (int n = 0; n &lt; lkern; n++)\n    {\n        size_t k;\n        size_t kmin = lkern - 1 - n;\n        corrvout[n] = 0;\n\n        for (k = 0; k &lt;= n; k++)\n        {\n            corrvout[n] += sig[k] * kern[kmin + k];\n        }\n    }\n\n    // stage II\n    for (int n = lkern; n &lt; lsig; n++)\n    {\n        size_t kmin, kmax, k;\n\n        corrvout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = n;\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            corrvout[n] += sig[k] * kern[k - kmin];\n        }\n    }\n\n    // stage III\n    for (int n = lsig; n &lt; lsig + lkern - 1; n++)\n    {\n        size_t kmin, kmax, k;\n\n        corrvout[n] = 0;\n\n        kmin = n - lkern + 1;\n        kmax = lsig - 1;\n\n        for (k = kmin; k &lt;= kmax; k++)\n        {\n            corrvout[n] += sig[k] * kern[k - kmin];\n        }\n    }\n#endif\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0: \u8ba1\u7b97\u4fe1\u53f7\u548c\u5377\u79ef\u6838\u4e4b\u95f4\u7684\u4e92\u76f8\u5173\u6027\u3002</p> <p>\u7279\u70b9</p> <ul> <li>\u652f\u6301\u5e73\u53f0\u52a0\u901f</li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>Signal</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4</p> </li> <li> <p><code>siglen</code>: \u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>Kernel</code>: \u8f93\u5165\u5377\u79ef\u6838\u6570\u7ec4</p> </li> <li> <p><code>kernlen</code>: \u5377\u79ef\u6838\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>corrvout</code>: \u8f93\u51fa\u6570\u7ec4\uff0c\u7528\u4e8e\u5b58\u50a8\u4e92\u76f8\u5173\u6027\u7ed3\u679c</p> </li> </ul> <p>\u8fd4\u56de\u503c: \u8fd4\u56de\u6210\u529f\u6216\u9519\u8bef\u4ee3\u7801\u3002</p>"},{"location":"zh/DSP/SIGNAL/CORRELATION/test/","title":"\u6d4b\u8bd5","text":""},{"location":"zh/DSP/SIGNAL/CORRELATION/test/#_2","title":"\u6d4b\u8bd5\u7ed3\u679c","text":"<pre><code>========== Correlation &amp; Cross-Correlation Test ==========\n\n--- Test 1: tiny_corr_f32 ---\nInput Signal : [1.00, 2.00, 3.00, 4.00, 2.00, 1.00]\nPattern      : [2.00, 1.00, 0.00]\nOutput vs Expected:\n  [0] Output = 4.000 | Expected = 4.000\n  [1] Output = 7.000 | Expected = 7.000\n  [2] Output = 10.000 | Expected = 10.000\n  [3] Output = 10.000 | Expected = 10.000\n[tiny_corr_f32 Test] [PASS]\n\n--- Test 2: tiny_ccorr_f32 ---\nInput Signal X: [1.00, 3.00, 2.00, 0.00, 1.00, 2.00]\nInput Signal Y: [2.00, 1.00, 0.00, -1.00]\nOutput vs Expected:\n  [0] Output = -1.000 | Expected = -1.000\n  [1] Output = -3.000 | Expected = -3.000\n  [2] Output = -1.000 | Expected = -1.000\n  [3] Output = 5.000 | Expected = 5.000\n  [4] Output = 7.000 | Expected = 7.000\n  [5] Output = 2.000 | Expected = 2.000\n  [6] Output = 1.000 | Expected = 1.000\n  [7] Output = 4.000 | Expected = 4.000\n  [8] Output = 4.000 | Expected = 4.000\n[tiny_ccorr_f32 Test] [PASS]\n==========================================================\n</code></pre>"},{"location":"zh/DSP/SIGNAL/RESAMPLE/code/","title":"\u4ee3\u7801","text":""},{"location":"zh/DSP/SIGNAL/RESAMPLE/notes/","title":"\u8bf4\u660e","text":"<p>\u8bf4\u660e</p> <p>\u91cd\u91c7\u6837\u662f\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\uff0c\u901a\u5e38\u7528\u4e8e\u6539\u53d8\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u3002\u5b83\u53ef\u4ee5\u7528\u4e8e\u97f3\u9891\u3001\u89c6\u9891\u548c\u5176\u4ed6\u7c7b\u578b\u7684\u4fe1\u53f7\u5904\u7406\u3002</p>"},{"location":"zh/DSP/SIGNAL/RESAMPLE/notes/#-","title":"\u4fe1\u53f7\u4e0b\u91c7\u6837 - \u8df3\u8dc3","text":"<p>\u4fe1\u53f7\u8df3\u8dc3\u4e0b\u91c7\u6837\u662f\u6307\u5728\u539f\u59cb\u4fe1\u53f7\u4e2d\u4ee5\u4e00\u5b9a\u95f4\u9694\u9009\u62e9\u6837\u672c\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u964d\u4f4e\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u3002\u6ce8\u610f\u533a\u522b\u4e8edecimation\uff0cdecimation\u662f\u5148\u6ee4\u6ce2\u518d\u4e0b\u91c7\u6837\u3002</p> <pre><code>/**\n * @name tiny_downsample_skip_f32\n * @brief Downsample a signal by a given factor using skipping\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param output_len pointer to the length of the output signal array\n * @param keep number of samples to keep\n * @param skip number of samples to skip\n *\n * @return tiny_error_t\n */\ntiny_error_t tiny_downsample_skip_f32(const float *input, int input_len, float *output, int *output_len, int keep, int skip)\n{\n    if (!input || !output || !output_len)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || keep &lt;= 0 || skip &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int out_len = input_len / skip;\n    *output_len = out_len;\n\n    for (int i = 0; i &lt; out_len; i++)\n    {\n        output[i] = input[i * skip];\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0\uff1a</p> <p>\u4fe1\u53f7\u8df3\u8dc3\u4e0b\u91c7\u6837\u51fd\u6570\uff0c\u901a\u8fc7\u4e24\u4e2a\u6574\u6570\u53c2\u6570<code>keep</code>\u548c<code>skip</code>\u6765\u63a7\u5236\u8df3\u8dc3\u4e0b\u91c7\u6837\u7684\u8fc7\u7a0b\u3002<code>keep</code>\u8868\u793a\u8981\u4fdd\u7559\u7684\u6837\u672c\u6570\uff0c<code>skip</code>\u8868\u793a\u8981\u8df3\u8fc7\u7684\u6837\u672c\u6570\u3002</p> <p>\u7279\u70b9\uff1a</p> <ul> <li> <p>\u6574\u6570\u500d\u4e0b\u91c7\u6837</p> </li> <li> <p>\u8df3\u8dc3\u4e0b\u91c7\u6837</p> </li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>input_len</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>output_len</code>: \u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6\u7684\u6307\u9488</p> </li> <li> <p><code>keep</code>: \u8981\u4fdd\u7559\u7684\u6837\u672c\u6570</p> </li> <li> <p><code>skip</code>: \u8981\u8df3\u8fc7\u7684\u6837\u672c\u6570</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/RESAMPLE/notes/#-0","title":"\u4fe1\u53f7\u4e0a\u91c7\u6837 - 0\u586b\u5145","text":"<p>\u4fe1\u53f7\u4e0a\u91c7\u6837\u662f\u6307\u5728\u539f\u59cb\u4fe1\u53f7\u4e2d\u63d2\u5165\u96f6\u4ee5\u589e\u52a0\u91c7\u6837\u7387\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u63d0\u9ad8\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u3002\u6ce8\u610f\u533a\u522b\u4e8einterpolation\uff0c\u4e0a\u91c7\u6837\u662f\u5148\u586b\u5145\u518d\u63d2\u503c\u3002</p> <pre><code>/**\n * @name tiny_upsample_zero_f32\n * @brief Upsample a signal using zero-insertion between samples\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_upsample_zero_f32(const float *input, int input_len, float *output, int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    int factor = target_len / input_len;\n    if (factor &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        output[i] = (i % factor == 0) ? input[i / factor] : 0.0f;\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0\uff1a</p> <p>\u4fe1\u53f7\u4e0a\u91c7\u6837\u51fd\u6570\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u4fe1\u53f7\u4e2d\u63d2\u5165\u96f6\u6765\u589e\u52a0\u91c7\u6837\u7387\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u63d0\u9ad8\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u3002</p> <p>\u7279\u70b9\uff1a</p> <ul> <li> <p>\u6574\u6570\u500d\u4e0a\u91c7\u6837</p> </li> <li> <p>0\u586b\u5145</p> </li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>input_len</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>target_len</code>: \u76ee\u6807\u957f\u5ea6\uff0c\u7528\u4e8e\u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/RESAMPLE/notes/#-_1","title":"\u4fe1\u53f7\u91cd\u91c7\u6837 - \u4efb\u610f\u500d\u6570\u4e0a\u4e0b\u91c7\u6837 - \u7ebf\u6027\u63d2\u503c","text":"<p>\u91cd\u91c7\u6837\u662f\u6307\u5c06\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u4ece\u4e00\u4e2a\u503c\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a\u503c\u3002\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u6700\u76f4\u63a5\u7684\u7ebf\u6027\u63d2\u503c\u6cd5\u3002\u9996\u5148\u8ba1\u7b97\u65b0\u4fe1\u53f7\u4e2d\u70b9\u5728\u8001\u4fe1\u53f7\u4e2d\u7684\u5927\u6982\u4f4d\u7f6e\uff0c\u7ed3\u5408\u8d77\u4f4d\u7f6e\u5e73\u8861\u5de6\u53f3\u70b9\u6570\u503c\u8fdb\u884c\u751f\u6210\u3002</p> <pre><code>/**\n * @name: tiny_resample_f32\n * @brief Resample a signal to a target length\n *\n * @param input pointer to the input signal array\n * @param input_len length of the input signal array\n * @param output pointer to the output signal array\n * @param target_len target length for the output signal array\n * @return tiny_error_t\n */\ntiny_error_t tiny_resample_f32(const float *input,\n                               int input_len,\n                               float *output,\n                               int target_len)\n{\n    if (!input || !output)\n        return TINY_ERR_DSP_NULL_POINTER;\n\n    if (input_len &lt;= 0 || target_len &lt;= 0)\n        return TINY_ERR_DSP_INVALID_PARAM;\n\n    float ratio = (float)(target_len) / (float)(input_len);\n\n    for (int i = 0; i &lt; target_len; i++)\n    {\n        float pos = i / ratio;\n        int index = (int)floorf(pos);\n        float frac = pos - index;\n\n        if (index &gt;= input_len - 1)\n            output[i] = input[input_len - 1]; // Clamp at end\n        else\n            output[i] = input[index] * (1.0f - frac) + input[index + 1] * frac;\n    }\n\n    return TINY_OK;\n}\n</code></pre> <p>\u63cf\u8ff0\uff1a</p> <p>\u4fe1\u53f7\u91cd\u91c7\u6837\u662f\u6307\u5c06\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u4ece\u4e00\u4e2a\u503c\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a\u503c\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u97f3\u9891\u548c\u89c6\u9891\u5904\u7406\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u64ad\u653e\u8bbe\u5907\u6216\u7f51\u7edc\u5e26\u5bbd\u3002</p> <p>\u7279\u70b9\uff1a</p> <ul> <li> <p>\u4efb\u610f\u500d\u6570\u4e0a\u4e0b\u91c7\u6837</p> </li> <li> <p>\u7ebf\u6027\u63d2\u503c</p> </li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>input_len</code>: \u8f93\u5165\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u6307\u9488</p> </li> <li> <p><code>target_len</code>: \u76ee\u6807\u957f\u5ea6\uff0c\u7528\u4e8e\u8f93\u51fa\u4fe1\u53f7\u6570\u7ec4\u7684\u957f\u5ea6</p> </li> </ul>"},{"location":"zh/DSP/SIGNAL/RESAMPLE/test/","title":"\u6d4b\u8bd5","text":""},{"location":"zh/DSP/SIGNAL/RESAMPLE/test/#_2","title":"\u6d4b\u8bd5\u7ed3\u679c","text":"<pre><code>========== TinyResample Test ==========\nDownsampled (skip 2):  1.00 3.00 5.00 7.00\nZero-Upsampled:      1.00 0.00 0.00 0.00 3.00 0.00 0.00 0.00 5.00 0.00 0.00 0.00 7.00 0.00 0.00 0.00\nInterpolated:        1.00 1.67 2.33 3.00 3.67 4.33 5.00 5.67 6.33 7.00 7.67 8.00\nResample validation at midpoints (expected: 1.5, 2.5, ..., 7.5):\n  midpoint[1] = 1.67\n  midpoint[3] = 3.00\n  midpoint[5] = 4.33\n  midpoint[7] = 5.67\n  midpoint[9] = 7.00\n  midpoint[11] = 8.00\n========================================\n</code></pre>"},{"location":"zh/DSP/TRANSFORM/DWT/code/","title":"\u4ee3\u7801","text":""},{"location":"zh/DSP/TRANSFORM/DWT/notes/","title":"\u8bf4\u660e","text":""},{"location":"zh/DSP/TRANSFORM/DWT/test/","title":"\u6d4b\u8bd5","text":""},{"location":"zh/DSP/USAGE/usage/","title":"\u4f7f\u7528\u8bf4\u660e","text":"<p>\u4f7f\u7528\u8bf4\u660e</p> <p>\u8be5\u6587\u6863\u662f\u5bf9 <code>tiny_dsp</code> \u6a21\u5757\u7684\u4f7f\u7528\u8bf4\u660e\u3002</p>"},{"location":"zh/DSP/USAGE/usage/#tinydsp","title":"\u6574\u4f53\u5f15\u5165TinyDSP","text":"<p>Info</p> <p>\u9002\u7528\u4e8eC\u9879\u76ee\uff0c\u6216\u8005\u7ed3\u6784\u8f83\u4e3a\u7b80\u5355\u7684C++\u9879\u76ee\u3002</p> <pre><code>#include \"tiny_dsp.h\"\n</code></pre>"},{"location":"zh/DSP/USAGE/usage/#tinydsp_1","title":"\u5206\u6a21\u5757\u5f15\u5165TinyDSP","text":"<p>Info</p> <p>\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u63a7\u5236\u5f15\u5165\u6a21\u5757\u7684\u9879\u76ee\uff0c\u6216\u8005\u590d\u6742\u7684C++\u9879\u76ee\u3002</p> <pre><code>#include \"tiny_conv.h\" // \u5f15\u5165\u5377\u79ef\u6a21\u5757\n#include \"tiny_corr.h\" // \u5f15\u5165\u76f8\u5173\u6a21\u5757\n...\n</code></pre> <p>Tip</p> <p>\u5177\u4f53\u7684\u4f7f\u7528\u65b9\u6cd5\u8bf7\u53c2\u8003\u6d4b\u8bd5\u4ee3\u7801\u3002</p>"},{"location":"zh/MATH/math/","title":"\u6570\u5b66\u8fd0\u7b97","text":"<p>Note</p> <p>\u8be5\u7ec4\u4ef6\u7528\u4e8e \u6570\u5b66\u8fd0\u7b97 \uff0c\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5e93\uff0c\u63d0\u4f9b\u57fa\u672c\u7684\u6570\u5b66\u51fd\u6570\uff0c\u4ee5\u4fbf\u4e8e\u677f\u8f7d\u8ba1\u7b97\u548cAI\u6a21\u578b\u63a8\u7406\u3002\u8be5\u5e93\u8bbe\u8ba1\u4e3a \u8f7b\u91cf\u9ad8\u6548 \uff0c\u9002\u5408\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002</p> <p>Note</p> <p>\u8be5\u7ec4\u4ef6\u57fa\u4e8eESP32\u5b98\u65b9\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u5e93 ESP-DSP \u8fdb\u884c\u5c01\u88c5\u548c\u6269\u5c55\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u6b21\u7684API\u63a5\u53e3\u3002\u7b80\u5355\u6765\u8bf4\uff0cTinyMath\u5e93\u5bf9\u5e94ESP-DSP\u4e2d\u7684Math, Matrix, DotProduct\u6a21\u5757\uff0cESP-DSP\u4e2d\u7684\u5176\u4f59\u6a21\u5757\u5bf9\u5e94TinyDSP\u5e93\u3002</p>"},{"location":"zh/MATH/math/#_2","title":"\u7ec4\u4ef6\u4f9d\u8d56","text":"<pre><code>set(src_dirs\n    .\n    vec\n    mat\n)\n\nset(include_dirs\n    .\n    include\n    vec\n    mat\n)\n\nset(requires\n    tiny_toolbox\n)\n\nidf_component_register(SRC_DIRS ${src_dirs} INCLUDE_DIRS ${include_dirs} REQUIRES ${requires})\n</code></pre>"},{"location":"zh/MATH/math/#_3","title":"\u67b6\u6784\u4e0e\u529f\u80fd\u76ee\u5f55","text":""},{"location":"zh/MATH/math/#_4","title":"\u4f9d\u8d56\u5173\u7cfb\u793a\u610f\u56fe","text":""},{"location":"zh/MATH/math/#_5","title":"\u4ee3\u7801\u6811","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500 CMakeLists.txt\n    \u251c\u2500\u2500 include\n    |   \u251c\u2500\u2500 tiny_error_type.h // error type header file\n    |   \u251c\u2500\u2500 tiny_constant.h // constant header file\n    |   \u251c\u2500\u2500 tiny_math_config.h // configuration header file\n    |   \u2514\u2500\u2500 tiny_math.h // main header file, include this file where you want to use the library\n    \u251c\u2500\u2500 vec\n    |   \u251c\u2500\u2500 tiny_vec.h // vector header file\n    |   \u251c\u2500\u2500 tiny_vec.c // vector source file\n    |   \u251c\u2500\u2500 tiny_vec_test.c // vector test file\n    |   \u2514\u2500\u2500 tiny_vec_test.h // vector test header file\n    \u251c\u2500\u2500 mat\n    |   \u251c\u2500\u2500 tiny_mat.h // matrix header file - c\n    |   \u251c\u2500\u2500 tiny_mat.c // matrix source file - c\n    |   \u251c\u2500\u2500 tiny_mat_test.c // matrix test file - c \n    |   \u251c\u2500\u2500 tiny_mat_test.h // matrix test header file - c\n    |   \u251c\u2500\u2500 tiny_matrix.hpp // matrix header file - cpp\n    |   \u251c\u2500\u2500 tiny_matrix.cpp // matrix source file - cpp\n    |   \u251c\u2500\u2500 tiny_matrix_test.cpp // matrix test file - cpp\n    |   \u2514\u2500\u2500 tiny_matrix_test.hpp // matrix test header file - cpp\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/","title":"ESP-DSP \u6570\u5b57\u4fe1\u53f7\u5904\u7406\u5e93","text":"<ul> <li> <p> ESP-DSP</p> <p>\u4e00\u4e2a Espressif DSP \u5e93 (esp-dsp)\uff0c\u5b83\u662f\u4e00\u4e2a\u51fd\u6570\u3001\u6a21\u5757\u548c\u7ec4\u4ef6\u7684\u5e93\uff0c\u63d0\u4f9b\u4e86\u4ee5\u9ad8\u6548\u7684\u65b9\u5f0f\u4f7f\u7528 Espressif \u7684 CPU \u4f5c\u4e3a DSP \u7684\u53ef\u80fd\u6027\u3002</p> <p>  \u5728\u7ebf\u6587\u6863 </p> </li> </ul>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_1","title":"\u51fd\u6570\u547d\u540d","text":"<p>\u547d\u540d\u7ea6\u5b9a\u9002\u7528\u4e8e\u6240\u6709\u8986\u76d6\u7684\u9886\u57df\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7 dsps \u524d\u7f00\u533a\u5206\u4fe1\u53f7\u5904\u7406\u51fd\u6570\uff0c\u800c\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\u51fd\u6570\u5177\u6709 dspi \u524d\u7f00\uff0c\u7279\u5b9a\u4e8e\u5c0f\u77e9\u9635\u64cd\u4f5c\u7684\u51fd\u6570\u5728\u5176\u540d\u79f0\u4e2d\u5177\u6709 dspm \u524d\u7f00\u3002\u5e93\u4e2d\u7684\u51fd\u6570\u540d\u79f0\u5177\u6709\u4ee5\u4e0b\u901a\u7528\u683c\u5f0f\uff1a</p> <pre><code>dsp&lt;data-domain&gt;_&lt;name&gt;_&lt;datatype1&gt;&lt;datatype_ext&gt;_&lt;datatype2&gt;&lt;datatype_ext&gt;[_&lt;descriptor&gt;]&lt;_impl&gt;(&lt;parameters&gt;);\n</code></pre> <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p><code>&lt;data-domain&gt;</code> \u662f\u51fd\u6570\u7684\u57df\uff0c\u4f8b\u5982 <code>s</code> \u8868\u793a\u4fe1\u53f7\u5904\u7406\uff0c<code>i</code> \u8868\u793a\u56fe\u50cf\u5904\u7406\uff0c<code>v</code> \u8868\u793a\u89c6\u9891\u5904\u7406\uff0c<code>m</code> \u8868\u793a\u5c0f\u77e9\u9635\u64cd\u4f5c\u3002</p> </li> <li> <p><code>&lt;name&gt;</code> \u662f\u51fd\u6570\u7684\u540d\u79f0\u3002</p> </li> <li> <p><code>&lt;datatype1&gt;</code> \u662f\u7b2c\u4e00\u4e2a\u8f93\u5165\u53c2\u6570\u7684\u7c7b\u578b\u3002</p> </li> <li> <p><code>&lt;datatype_ext&gt;</code> \u662f\u7b2c\u4e00\u4e2a\u8f93\u5165\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u540e\u7f00\u8868\u793a\u6570\u636e\u7684\u7c7b\u578b\uff0c\u4f8b\u5982 <code>f</code> \u8868\u793a\u6d6e\u70b9\u6570\uff0c<code>i</code> \u8868\u793a\u6574\u6570\uff0c<code>c</code> \u8868\u793a\u590d\u6570\u7b49\u3002</p> </li> <li> <p><code>&lt;datatype2&gt;</code> \u662f\u7b2c\u4e8c\u4e2a\u8f93\u5165\u53c2\u6570\u7684\u7c7b\u578b\u3002</p> </li> <li> <p><code>&lt;descriptor&gt;</code> \u662f\u4e00\u4e2a\u53ef\u9009\u63cf\u8ff0\u7b26\uff0c\u63d0\u4f9b\u6709\u5173\u51fd\u6570\u7684\u9644\u52a0\u4fe1\u606f\u3002</p> </li> <li> <p><code>&lt;impl&gt;</code> \u662f\u4e00\u4e2a\u53ef\u9009\u5b9e\u73b0\u63cf\u8ff0\u7b26\uff0c\u63d0\u4f9b\u6709\u5173\u51fd\u6570\u5b9e\u73b0\u7684\u9644\u52a0\u4fe1\u606f\u3002</p> </li> <li> <p><code>&lt;parameters&gt;</code> \u662f\u51fd\u6570\u7684\u53c2\u6570\u3002</p> </li> </ul>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_2","title":"\u6570\u636e\u57df","text":"<p>\u6570\u636e\u57df\u662f\u4e00\u4e2a\u5355\u5b57\u7b26\uff0c\u8868\u793a\u7ed9\u5b9a\u51fd\u6570\u6240\u5c5e\u7684\u529f\u80fd\u5b50\u96c6\u3002\u5e93\u8bbe\u8ba1\u4e3a\u652f\u6301\u4ee5\u4e0b\u6570\u636e\u57df\uff1a</p> <ul> <li> <p>s - \u4fe1\u53f7\uff08\u9884\u671f\u6570\u636e\u7c7b\u578b\u4e3a 1D \u4fe1\u53f7\uff09</p> </li> <li> <p>i - \u56fe\u50cf\u548c\u89c6\u9891\uff08\u9884\u671f\u6570\u636e\u7c7b\u578b\u4e3a 2D \u56fe\u50cf\uff09</p> </li> <li> <p>m - \u77e9\u9635\uff08\u9884\u671f\u6570\u636e\u7c7b\u578b\u4e3a\u77e9\u9635\uff09</p> </li> <li> <p>r - \u903c\u771f\u6e32\u67d3\u529f\u80fd\u548c 3D \u6570\u636e\u5904\u7406\uff08\u9884\u671f\u6570\u636e\u7c7b\u578b\u53d6\u51b3\u4e8e\u652f\u6301\u7684\u6e32\u67d3\u6280\u672f\uff09</p> </li> <li> <p>q - \u56fa\u5b9a\u957f\u5ea6\u4fe1\u53f7</p> </li> </ul> <p>\u4f8b\u5982\uff0c\u4ee5 dspi \u5f00\u5934\u7684\u51fd\u6570\u540d\u79f0\u8868\u793a\u76f8\u5e94\u7684\u51fd\u6570\u7528\u4e8e\u56fe\u50cf\u6216\u89c6\u9891\u5904\u7406\u3002</p>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_3","title":"\u540d\u79f0","text":"<p>\u51fd\u6570\u540d\u79f0\u662f\u51fd\u6570\u5b9e\u9645\u6267\u884c\u7684\u6838\u5fc3\u64cd\u4f5c\u7684\u7f29\u5199\uff0c\u4f8b\u5982 Add\u3001Sqrt\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u540e\u8ddf\u51fd\u6570\u7279\u5b9a\u7684\u4fee\u9970\u7b26\uff1a= [_modifier]</p> <p>\u5982\u679c\u5b58\u5728\u6b64\u4fee\u9970\u7b26\uff0c\u5219\u8868\u793a\u5bf9\u7ed9\u5b9a\u51fd\u6570\u8fdb\u884c\u4e86\u7ec6\u5fae\u7684\u4fee\u6539\u6216\u53d8\u4f53\u3002</p>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_4","title":"\u6570\u636e\u7c7b\u578b","text":"<p>\u8be5\u5e93\u652f\u6301\u4e24\u79cd\u4e3b\u8981\u6570\u636e\u7c7b\u578b\uff1a\u7528\u4e8e\u5b9a\u70b9\u8fd0\u7b97\u7684 int16 \u548c\u7528\u4e8e\u6d6e\u70b9\u8fd0\u7b97\u7684 float\u3002\u6570\u636e\u7c7b\u578b\u63cf\u8ff0\u5982\u4e0b\uff1a</p>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_5","title":"\u6570\u636e\u7c7b\u578b\u540e\u7f00","text":"<ul> <li> <p>s - \u6709\u7b26\u53f7</p> </li> <li> <p>u - \u65e0\u7b26\u53f7</p> </li> <li> <p>f - \u6d6e\u70b9\u6570</p> </li> </ul>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_6","title":"\u6570\u636e\u7c7b\u578b\u6269\u5c55","text":"<ul> <li>c - \u590d\u6570</li> </ul>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_7","title":"\u6570\u636e\u7c7b\u578b\u6bd4\u7279\u5206\u8fa8\u7387","text":"<ul> <li> <p>16</p> </li> <li> <p>32</p> </li> </ul> <p>\u4f8b\u5982\uff1adsps_mac_sc16 \u5b9a\u4e49\u5c06\u4f7f\u7528 16 \u4f4d\u6709\u7b26\u53f7\u590d\u6570\u6570\u636e\u5bf9 1d \u6570\u7ec4\u8fdb\u884c m\u200b\u200bac \u8fd0\u7b97\u3002</p>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_8","title":"\u5b9e\u73b0\u65b9\u5f0f\u7c7b\u578b","text":"<p>\u6bcf\u4e2a\u51fd\u6570\u53ef\u4ee5\u9488\u5bf9\u4e0d\u540c\u7684\u5e73\u53f0\u8fdb\u884c\u4e0d\u540c\u7684\u5b9e\u73b0\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u6837\u5f0f\u548c\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u6bcf\u4e2a\u5b9e\u73b0\u7684\u51fd\u6570\u90fd\u4f1a\u6709\u4e00\u4e2a\u6269\u5c55\u540d &lt;_impl&gt;\uff0c\u7528\u4e8e\u5b9a\u4e49\u5176\u5b9e\u73b0\u7c7b\u578b\u3002\u7528\u6237\u65e0\u9700\u6269\u5c55\u540d\u5373\u53ef\u4f7f\u7528\u901a\u7528\u51fd\u6570\u3002</p>"},{"location":"zh/MATH/ESP-DSP/esp-dsp/#_9","title":"\u5b9e\u73b0\u65b9\u5f0f\u7c7b\u578b\u540e\u7f00","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u51fd\u6570\u65e0\u9700\u6269\u5c55\u5373\u53ef\u4f7f\u7528\u3002\u60a8\u53ef\u4ee5\u5728 menuconfig \u4e2d\u9009\u62e9\u201coptimized/ansi\u201d\u9009\u9879\u3002</p> <p>\u5e93\u4e2d\u7684\u6269\u5c55\u5305\u62ec\uff1a</p> <ul> <li> <p>_ansi - \u901a\u7528\u51fd\u6570\uff0c\u5176\u51fd\u6570\u4f53\u4f7f\u7528 ANSI C \u5b9e\u73b0\u3002\u6b64\u5b9e\u73b0\u4e0d\u5305\u542b\u4efb\u4f55\u786c\u4ef6\u4f18\u5316\u3002</p> </li> <li> <p>_ae32 - \u4f7f\u7528 ESP32 \u6c47\u7f16\u5668\u7f16\u5199\uff0c\u5e76\u9488\u5bf9 ESP32 \u8fdb\u884c\u4e86\u4f18\u5316\u3002</p> </li> <li> <p>_aes3 - \u4f7f\u7528 ESP32S3 \u6c47\u7f16\u5668\u7f16\u5199\uff0c\u5e76\u9488\u5bf9 ESP32S3 \u8fdb\u884c\u4e86\u4f18\u5316\u3002</p> </li> <li> <p>_arp4 - \u4f7f\u7528 ESP32P4 \u6c47\u7f16\u5668\u7f16\u5199\uff0c\u5e76\u9488\u5bf9 ESP32P4 \u8fdb\u884c\u4e86\u4f18\u5316\u3002</p> </li> <li> <p>_platform - \u5934\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u9488\u5bf9\u4e0d\u540c\u51fd\u6570\u7684\u53ef\u7528 CPU \u6307\u4ee4\u5b9a\u4e49\u3002</p> </li> <li> <p>\u5176\u4ed6 - \u53d6\u51b3\u4e8e\u652f\u6301\u7684 CPU \u6570\u91cf\u3002\u6b64\u5217\u8868\u672a\u6765\u5c06\u4e0d\u65ad\u6269\u5c55\u3002</p> </li> </ul>"},{"location":"zh/MATH/ESP-DSP/examples/","title":"ESP-DSP \u6848\u4f8b","text":""},{"location":"zh/MATH/ESP-DSP/examples/#esp-dsp_1","title":"esp-dsp \u793a\u4f8b\u5217\u8868","text":"<p>\u4fe1\u53f7\u5904\u7406 API \u4f7f\u7528 dsps \u524d\u7f00\u3002\u4ee5\u4e0b\u6a21\u5757\u53ef\u7528\uff1a</p> <ul> <li> <p>\u57fa\u7840\u6570\u5b66 - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u57fa\u672c\u5411\u91cf\u6570\u5b66\u8fd0\u7b97</p> </li> <li> <p>\u70b9\u79ef - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u70b9\u79ef\u51fd\u6570</p> </li> <li> <p>\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362 - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 FFT \u529f\u80fd</p> </li> <li> <p>\u7a97\u53e3 FFT - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u7a97\u53e3\u548c FFT \u529f\u80fd</p> </li> <li> <p>\u5b9e\u6570 FFT - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 FFT \u529f\u80fd\u5904\u7406\u5b9e\u6570\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u65e0\u9650\u8109\u51b2\u54cd\u5e94 (IIR) - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 IIR \u6ee4\u6ce2\u5668\u529f\u80fd</p> </li> <li> <p>\u6709\u9650\u8109\u51b2\u54cd\u5e94 (FIR) - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 FIR \u6ee4\u6ce2\u5668\u529f\u80fd</p> </li> <li> <p>\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668 - \u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668 (EKF) \u793a\u4f8b</p> </li> <li> <p>\u77e9\u9635 - \u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Mat \u7c7b\u529f\u80fd</p> </li> </ul>"},{"location":"zh/MATH/ESP-DSP/examples/#_1","title":"\u57fa\u7840\u6570\u5b66","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 esp-dsp \u5e93\u4e2d\u7684\u57fa\u672c\u6570\u5b66\u51fd\u6570\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u5e93</p> </li> <li> <p>\u7528 1024 \u4e2a\u6837\u672c\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u4f7f\u7528\u6807\u51c6 C \u5faa\u73af\u5bf9\u8f93\u5165\u4fe1\u53f7\u52a0\u7a97\u3002</p> </li> <li> <p>\u8ba1\u7b97 1024 \u4e2a\u590d\u6570\u6837\u672c\u7684 FFT \u5e76\u663e\u793a\u7ed3\u679c</p> </li> <li> <p>\u5728\u56fe\u8868\u4e0a\u663e\u793a\u7ed3\u679c</p> </li> <li> <p>\u4f7f\u7528\u57fa\u672c\u6570\u5b66\u51fd\u6570 dsps_mul_f32 \u548c dsps_mulc_f32 \u5bf9\u8f93\u5165\u4fe1\u53f7\u52a0\u7a97\u3002</p> </li> <li> <p>\u8ba1\u7b97 1024 \u4e2a\u590d\u6570\u6837\u672c\u7684 FFT</p> </li> <li> <p>\u5728\u56fe\u8868\u4e0a\u663e\u793a\u7ed3\u679c</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/basic_math/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#_2","title":"\u70b9\u79ef","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 esp-dsp \u5e93\u4e2d\u7684 dotprod dsps_dotprod_f32 \u51fd\u6570\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u8f93\u5165\u6570\u7ec4</p> </li> <li> <p>\u8ba1\u7b97\u4e24\u4e2a\u6570\u7ec4\u7684\u70b9\u79ef</p> </li> <li> <p>\u6bd4\u8f83\u7ed3\u679c\u5e76\u8ba1\u7b97\u6267\u884c\u65f6\u95f4\uff08\u4ee5\u5468\u671f\u4e3a\u5355\u4f4d\uff09\u3002</p> </li> </ul> <p>\u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 examples/dotprod/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#fft","title":"FFT","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 ESP-DSP \u5e93\u4e2d\u7684 FFT \u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u5e93</p> </li> <li> <p>\u7528 1024 \u4e2a\u6837\u672c\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7\uff1a\u7b2c\u4e00\u4e2a 0 dB\uff0c\u7b2c\u4e8c\u4e2a -20 dB</p> </li> <li> <p>\u5c06\u4e24\u4e2a\u4fe1\u53f7\u5408\u5e76\u4e3a\u4e00\u4e2a\u590d\u6570\u8f93\u5165\u4fe1\u53f7\uff0c\u5e76\u5bf9\u8f93\u5165\u4fe1\u53f7\u5bf9\u5e94\u7528\u7a97\u53e3\u3002</p> </li> <li> <p>\u8ba1\u7b97 1024 \u4e2a\u590d\u6570\u6837\u672c\u7684 FFT</p> </li> <li> <p>\u5bf9\u8f93\u51fa\u590d\u6570\u5411\u91cf\u5e94\u7528\u4f4d\u53cd\u8f6c\u64cd\u4f5c</p> </li> <li> <p>\u5c06\u4e00\u4e2a\u590d\u6570 FFT \u8f93\u51fa\u9891\u8c31\u62c6\u5206\u4e3a\u4e24\u4e2a\u5b9e\u6570\u4fe1\u53f7\u9891\u8c31</p> </li> <li> <p>\u5728\u56fe\u8868\u4e0a\u663e\u793a\u7ed3\u679c</p> </li> <li> <p>\u663e\u793a FFT \u7684\u6267\u884c\u65f6\u95f4</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/fft/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#fft_1","title":"FFT \u7a97\u53e3","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 esp-dsp \u5e93\u4e2d\u7684\u7a97\u53e3\u548c FFT \u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u5e93</p> </li> <li> <p>\u7528 1024 \u4e2a\u6837\u672c\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u5bf9\u8f93\u5165\u4fe1\u53f7\u5e94\u7528\u7a97\u53e3\u3002</p> </li> <li> <p>\u5bf9 1024 \u4e2a\u590d\u6570\u6837\u672c\u8ba1\u7b97 FFT</p> </li> <li> <p>\u5bf9\u8f93\u51fa\u590d\u6570\u5411\u91cf\u5e94\u7528\u4f4d\u53cd\u8f6c\u64cd\u4f5c</p> </li> <li> <p>\u5c06\u4e00\u4e2a\u590d\u6570 FFT \u8f93\u51fa\u9891\u8c31\u62c6\u5206\u4e3a\u4e24\u4e2a\u5b9e\u6570\u4fe1\u53f7\u9891\u8c31</p> </li> <li> <p>\u5728\u56fe\u8868\u4e0a\u663e\u793a\u7ed3\u679c</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/fft_window/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#fft-4-real","title":"FFT 4 Real","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 ESP-DSP \u5e93\u4e2d\u7684 FFT \u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u5e93</p> </li> <li> <p>\u7528 1024 \u4e2a\u6837\u672c\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7\uff1a\u7b2c\u4e00\u4e2a 0 dB\uff0c\u7b2c\u4e8c\u4e2a -20 dB</p> </li> <li> <p>\u8ba1\u7b97 1024 \u4e2a\u590d\u6570\u6837\u672c\u7684 FFT \u57fa\u6570 2</p> </li> <li> <p>\u8ba1\u7b97 1024 \u4e2a\u590d\u6570\u6837\u672c\u7684 FFT \u57fa\u6570 4</p> </li> <li> <p>\u5bf9\u8f93\u51fa\u590d\u6570\u5411\u91cf\u5e94\u7528\u4f4d\u53cd\u8f6c\u8fd0\u7b97</p> </li> <li> <p>\u7ed8\u56fe\u663e\u793a\u7ed3\u679c</p> </li> <li> <p>\u663e\u793a FFT \u6267\u884c\u65f6\u95f4</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/fft4real/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#iir","title":"IIR","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 ESP-DSP \u5e93\u4e2d\u7684 IIR \u6ee4\u6ce2\u5668\u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u5e93</p> </li> <li> <p>\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u663e\u793a Q \u56e0\u5b50\u4e3a 1 \u7684\u4f4e\u901a\u6ee4\u6ce2\u5668 (LPF)</p> </li> <li> <p>\u8ba1\u7b97 IIR \u6ee4\u6ce2\u5668\u7cfb\u6570</p> </li> <li> <p>\u6ee4\u6ce2\u8f93\u5165\u6d4b\u8bd5\u4fe1\u53f7\uff08Delta \u51fd\u6570\uff09</p> </li> <li> <p>\u5728\u56fe\u4e2d\u663e\u793a\u8109\u51b2\u54cd\u5e94</p> </li> <li> <p>\u5728\u56fe\u4e2d\u663e\u793a\u9891\u7387\u54cd\u5e94</p> </li> <li> <p>\u8ba1\u7b97\u6267\u884c\u6027\u80fd</p> </li> <li> <p>\u5bf9\u4e8e Q \u56e0\u5b50\u4e3a 10 \u7684\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u540c\u6837\u5982\u6b64</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/fir/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#fir","title":"FIR","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528 ESP-DSP \u5e93\u4e2d\u7684 FIR \u6ee4\u6ce2\u5668\u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316 FFT \u5e93</p> </li> <li> <p>\u521d\u59cb\u5316\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u663e\u793a\u8f93\u5165\u4fe1\u53f7</p> </li> <li> <p>\u663e\u793a\u6ee4\u6ce2\u540e\u7684\u4fe1\u53f7</p> </li> </ul> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/fir/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#_3","title":"\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668","text":"<p>\u672c\u793a\u4f8b\u6a21\u62df\u4e86\u5e26\u6709 IMU \u4f20\u611f\u5668\u7684\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u5177\u6709 13 \u4e2a\u72b6\u6001\u5411\u91cf\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668 (EKF) \u6765\u4f30\u8ba1\u9640\u87ba\u4eea\u8bef\u5dee\u5e76\u8ba1\u7b97\u7cfb\u7edf\u59ff\u6001\u3002\u6b64\u5916\uff0c\u672c\u793a\u4f8b\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 esp-dsp \u5e93\u5bf9\u77e9\u9635\u548c\u5411\u91cf\u8fdb\u884c\u8fd0\u7b97\u3002</p> <p>\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\uff0c\u5e94\u5c06\u6a21\u62df\u4f20\u611f\u5668\u503c\u66ff\u6362\u4e3a\u5b9e\u9645\u4f20\u611f\u5668\u503c\u3002\u7136\u540e\uff0c\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\uff0c\u5e94\u6267\u884c\u6821\u51c6\u9636\u6bb5\u3002\u6821\u51c6\u9636\u6bb5\u7ed3\u675f\u540e\uff0c\u5e94\u4fdd\u5b58\u72b6\u6001\u5411\u91cf X \u548c\u534f\u65b9\u5dee\u77e9\u9635 P\uff0c\u5e76\u5728\u4e0b\u6b21\u8c03\u7528\u6ee4\u6ce2\u5668\u65f6\u6062\u590d\u3002\u8fd9\u5c06\u8282\u7701\u521d\u59cb\u9636\u6bb5\u7684\u65f6\u95f4\u3002</p> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 examples/kalman/README.md</p>"},{"location":"zh/MATH/ESP-DSP/examples/#_4","title":"\u77e9\u9635","text":"<p>\u672c\u793a\u4f8b\u6f14\u793a\u5982\u4f55\u4f7f\u7528 esp-dsp \u5e93\u4e2d\u7684 Mat \u7c7b\u529f\u80fd\u3002\u793a\u4f8b\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\u77e9\u9635 A \u548c \u77e9\u9635 x</p> </li> <li> <p>\u8ba1\u7b97\u77e9\u9635 b\uff1ab = A*x</p> </li> <li> <p>\u4f7f\u7528\u4e0d\u540c\u65b9\u6cd5\u6c42 x1 \u7684\u6839\uff1aA*x1 = b</p> </li> <li> <p>\u6253\u5370\u7ed3\u679c</p> </li> </ul>"},{"location":"zh/MATH/HEADER-FILE/tiny_constants/","title":"\u5e38\u91cf\u5b9a\u4e49","text":"<p>Info</p> <p>\u8be5\u6587\u4ef6\u5305\u542b\u4e86\u4e00\u4e9b\u5e38\u91cf\u7684\u5b9a\u4e49\u7528\u4e8e\u4e0a\u5c42\u8ba1\u7b97\u548c\u5e94\u7528\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u4e00\u81f4\uff0c\u8bf7\u4ee5\u4ee3\u7801\u4e3a\u51c6\u3002</p> <pre><code>/**\n * @file tiny_constants.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file contains the constants used in the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n// =======================================\n//  Logical Constants\n// =======================================\n#ifndef TRUE\n#define TRUE 1\n#endif\n\n#ifndef FALSE\n#define FALSE 0\n#endif\n\n#ifndef NULL\n#define NULL ((void *)0)\n#endif\n\n// =======================================\n//  Math Constants (float/double safe)\n// =======================================\n#define TINY_PI 3.14159265358979323846f\n#define TINY_TWO_PI 6.28318530717958647692f\n#define TINY_HALF_PI 1.57079632679489661923f\n#define TINY_E 2.71828182845904523536f\n#define TINY_SQRT2 1.41421356237309504880f\n#define TINY_INV_SQRT2 0.70710678118654752440f\n\n#define TINY_DEG2RAD(x) ((x) * TINY_PI / 180.0f)\n#define TINY_RAD2DEG(x) ((x) * 180.0f / TINY_PI)\n\n// =======================================\n//  Bitmask &amp; Bit Manipulation\n// =======================================\n\n// Bitwise operations\n#define TINY_BIT(n) (1U &lt;&lt; (n)) // e.g. TINY_BIT(3) = 0b00001000\n#define TINY_BIT_SET(x, n) ((x) |= TINY_BIT(n))\n#define TINY_BIT_CLEAR(x, n) ((x) &amp;= ~TINY_BIT(n))\n#define TINY_BIT_TOGGLE(x, n) ((x) ^= TINY_BIT(n))\n#define TINY_BIT_CHECK(x, n) (((x) &gt;&gt; (n)) &amp; 0x1U)\n\n// Common bit masks\n#define TINY_MASK_4BIT 0x0FU\n#define TINY_MASK_8BIT 0xFFU\n#define TINY_MASK_16BIT 0xFFFFU\n#define TINY_MASK_32BIT 0xFFFFFFFFU\n\n// =======================================\n//  Fixed-Point Scaling Factors\n// =======================================\n#define TINY_Q7_SCALE 128          // 2^7\n#define TINY_Q15_SCALE 32768       // 2^15\n#define TINY_Q31_SCALE 2147483648U // 2^31\n\n// =======================================\n//  User-Defined Constants (Optional)\n// =======================================\n#define TINY_MATH_MIN_DENOMINATOR 1e-6f         // Minimum denominator for safe division\n#define TINY_MATH_MIN_POSITIVE_INPUT_F32 1e-12f // Minimum positive input for float operations\n#define TINY_MATH_LARGE_VALUE_F32 1e38f         // Large value used to represent infinity-like results (safe for IEEE 754 float, max ~3.4e38)\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/MATH/HEADER-FILE/tiny_error_type/","title":"\u9519\u8bef\u7c7b\u578b\u5b9a\u4e49","text":"<p>Info</p> <p>\u8be5\u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e9b\u8ba1\u7b97\u4e2d\u5e38\u89c1\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u7528\u4e8e\u8f85\u52a9\u5224\u65ad\u9519\u8bef\u539f\u56e0\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u7b26\uff0c\u8bf7\u4ee5\u4ee3\u7801\u4e3a\u51c6\u3002</p> <pre><code>/**\n * @file tiny_error_type.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-15\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /* TYPE DEFINITIONS */\n    typedef int tiny_error_t; // Error type for the tiny_math middleware\n\n/* MACROS */\n/* Definitions for error constants. */\n#define TINY_OK 0    /*!&lt; tiny_err_t value indicating success (no error) */\n#define TINY_FAIL -1 /*!&lt; Generic tiny_err_t code indicating failure */\n\n#define TINY_ERR_NO_MEM 0x101           /*!&lt; Out of memory */\n#define TINY_ERR_INVALID_ARG 0x102      /*!&lt; Invalid argument */\n#define TINY_ERR_INVALID_STATE 0x103    /*!&lt; Invalid state */\n#define TINY_ERR_INVALID_SIZE 0x104     /*!&lt; Invalid size */\n#define TINY_ERR_NOT_FOUND 0x105        /*!&lt; Requested resource not found */\n#define TINY_ERR_NOT_SUPPORTED 0x106    /*!&lt; Operation or feature not supported */\n#define TINY_ERR_TIMEOUT 0x107          /*!&lt; Operation timed out */\n#define TINY_ERR_INVALID_RESPONSE 0x108 /*!&lt; Received response was invalid */\n#define TINY_ERR_INVALID_CRC 0x109      /*!&lt; CRC or checksum was invalid */\n#define TINY_ERR_INVALID_VERSION 0x10A  /*!&lt; Version was invalid */\n#define TINY_ERR_INVALID_MAC 0x10B      /*!&lt; MAC address was invalid */\n#define TINY_ERR_NOT_FINISHED 0x10C     /*!&lt; Operation has not fully completed */\n#define TINY_ERR_NOT_ALLOWED 0x10D      /*!&lt; Operation is not allowed */\n\n#define TINY_ERR_WIFI_BASE 0x3000      /*!&lt; Starting number of WiFi error codes */\n#define TINY_ERR_MESH_BASE 0x4000      /*!&lt; Starting number of MESH error codes */\n#define TINY_ERR_FLASH_BASE 0x6000     /*!&lt; Starting number of flash error codes */\n#define TINY_ERR_HW_CRYPTO_BASE 0xc000 /*!&lt; Starting number of HW cryptography module error codes */\n#define TINY_ERR_MEMPROT_BASE 0xd000   /*!&lt; Starting number of Memory Protection API error codes */\n\n#define TINY_ERR_MATH_BASE 0x70000\n#define TINY_ERR_MATH_INVALID_LENGTH (TINY_ERR_MATH_BASE + 1)\n#define TINY_ERR_MATH_INVALID_PARAM (TINY_ERR_MATH_BASE + 2)\n#define TINY_ERR_MATH_PARAM_OUTOFRANGE (TINY_ERR_MATH_BASE + 3)\n#define TINY_ERR_MATH_UNINITIALIZED (TINY_ERR_MATH_BASE + 4)\n#define TINY_ERR_MATH_REINITIALIZED (TINY_ERR_MATH_BASE + 5)\n#define TINY_ERR_MATH_ARRAY_NOT_ALIGNED (TINY_ERR_MATH_BASE + 6)\n#define TINY_ERR_MATH_NULL_POINTER (TINY_ERR_MATH_BASE + 7)\n#define TINY_ERR_MATH_ZERO_DIVISION (TINY_ERR_MATH_BASE + 8)\n#define TINY_ERR_MATH_NEGATIVE_SQRT (TINY_ERR_MATH_BASE + 9)\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/MATH/HEADER-FILE/tiny_math/","title":"TinyMath\u5934\u6587\u4ef6","text":"<p>Info</p> <p>\u8fd9\u662fTinyMath\u5e93\u7684\u4e3b\u5934\u6587\u4ef6\u3002\u5b83\u5305\u542b\u6240\u6709\u5fc5\u8981\u7684\u5934\u6587\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u53e3\u6765\u4f7f\u7528\u5e93\u7684\u529f\u80fd\u3002\u5728\u9879\u76ee\u4e2d\u5b8c\u6210\u8be5\u5e93\u7684\u79fb\u690d\u540e\uff0c\u5728\u9700\u8981\u4f7f\u7528\u76f8\u5173\u51fd\u6570\u7684\u5730\u65b9\u63d2\u5165\u8be5\u5934\u6587\u4ef6\u5373\u53ef\u4f7f\u7528\u5e93\u5185\u7684\u6240\u6709\u51fd\u6570\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u4e00\u81f4\uff0c\u8bf7\u4ee5\u5b9e\u9645\u4ee3\u7801\u4e3a\u51c6\u3002</p> <pre><code>/**\n * @file tiny_math.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-03-26\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n\n// this layer\n#include \"tiny_math_config.h\"\n\n/* SUBMODULES */\n\n// vector operations\n#include \"tiny_vec.h\"\n\n// matrix operations\n#include \"tiny_mat.h\"\n\n// advanced matrix operations\n#ifdef __cplusplus\n\n#include \"tiny_matrix.hpp\"\n\n#endif\n\n/* TEST */ // NOTE: test files are platform specific and should not be included in the library\n\n// vector operations\n#include \"tiny_vec_test.h\"\n\n// matrix operations\n#include \"tiny_mat_test.h\"\n\n// advanced matrix operations\n#ifdef __cplusplus\n\n#include \"tiny_matrix_test.hpp\"\n\n#endif\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/MATH/HEADER-FILE/tiny_math_config/","title":"TinyMath \u914d\u7f6e","text":"<p>Info</p> <p>\u8fd9\u4e2a\u5934\u6587\u4ef6\u8d77\u5230\u914d\u7f6e\u6574\u4e2aTinyMath\u6a21\u5757\u7684\u4f5c\u7528\uff0c\u6bcf\u4e2a\u5b50\u6a21\u5757\u90fd\u5305\u542b\u4e86\u6b64\u5934\u6587\u4ef6\u3002\u5b83\u5b9a\u4e49\u4e86TinyMath\u7684\u914d\u7f6e\u9009\u9879\u548c\u5b8f\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u9700\u8981\u8fdb\u884c\u81ea\u5b9a\u4e49\u8bbe\u7f6e\u3002\u901a\u8fc7\u4fee\u6539\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9009\u9879\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u5730\u8c03\u6574TinyMath\u7684\u884c\u4e3a\u548c\u529f\u80fd\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u3002\u6587\u6863\u66f4\u65b0\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u80fd\u4f1a\u4e0e\u5b9e\u9645\u4ee3\u7801\u4e0d\u4e00\u81f4\uff0c\u8bf7\u4ee5\u4ee3\u7801\u4e3a\u51c6\u3002</p> <p>Tip</p> <p>\u8be5\u7ec4\u4ef6\u5185\u5305\u62ec\u9009\u62e9\u5e73\u53f0\u7684\u5b8f\u5b9a\u4e49\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u8981\u9009\u62e9\u4e0d\u540c\u7684\u5e73\u53f0\u8fdb\u884c\u7f16\u8bd1\u3002\u5207\u6362\u5230\u5bf9\u5e94\u5e73\u53f0\u7684\u5b8f\u540e\uff0c\u53ef\u4ee5\u5229\u7528\u5e73\u53f0\u52a0\u901f\u7684\u7279\u6027\u6765\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8eESP32\u5e73\u53f0\uff0cTinyMath\u4f1a\u81ea\u52a8\u9009\u62e9ESP32\u7684DSP\u5e93\u8fdb\u884c\u7f16\u8bd1\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6570\u5b66\u8fd0\u7b97\u3002</p> <pre><code>/**\n * @file tiny_math_config.h\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief The configuration file for the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-14\n * @copyright Copyright (c) 2025\n *\n */\n\n#pragma once\n\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n/* DEPENDENCIES */\n\n// ANSI C\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdint.h&gt;\n\n// lower level\n#include \"tiny_toolbox.h\"\n\n// this level\n#include \"tiny_error_type.h\"\n#include \"tiny_constants.h\"\n\n/* PLATFORM SELECTION */\n\n// available platforms\n#define MCU_PLATFORM_GENERIC 0\n#define MCU_PLATFORM_ESP32 1 // here, we utilize the ESP built-in DSP library, it will automatically select the optimized version\n#define MCU_PLATFORM_STM32 2\n#define MCU_PLATFORM_RISCV 3\n\n// choose one platform\n#define MCU_PLATFORM_SELECTED MCU_PLATFORM_ESP32\n\n#ifdef __cplusplus\n}\n#endif\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/","title":"\u77e9\u9635\u64cd\u4f5c - TINY_MAT","text":"<p>\u5173\u4e8etiny_mat\u5e93</p> <p>tiny_mat\u662f\u4e00\u4e2aC\u8bed\u8a00\u5b9e\u73b0\u7684\u77e9\u9635\u5e93\uff0c\u63d0\u4f9b\u4e86\u57fa\u672c\u7684\u77e9\u9635\u64cd\u4f5c\u51fd\u6570\u3002\u5b83\u652f\u6301\u6d6e\u70b9\u6570\u77e9\u9635\u7684\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u4e58\u6cd5\u7b49\u64cd\u4f5c\u3002\u8be5\u5e93\u9002\u7528\u4e8e\u9700\u8981\u8fdb\u884c\u77e9\u9635\u8ba1\u7b97\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u5b9e\u65f6\u5e94\u7528\u3002\u8be5\u5e93\u57fa\u4e8eANSIC C\u6807\u51c6\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u79fb\u690d\u6027\u548c\u6027\u80fd,\u540c\u65f6\u53c8\u652f\u6301\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u914d\u7f6e\u4ece\u800c\u652f\u6301\u5e73\u53f0\u52a0\u901f\uff08ESP32\uff09\u3002</p> <p>\u5173\u4e8etiny_mat\u5e93\u7684\u4f7f\u7528</p> <p>tiny_mat\u7684\u529f\u80fd\u88abtiny_matrix\u5b8c\u5168\u8986\u76d6\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728tiny_matrix\u4e2d\u7684\u529f\u80fd\u5305\u542b\u4e86tiny_mat\u7684\u6240\u6709\u529f\u80fd\u3002\u5bf9\u4e8e\u7b80\u5355\u7684\u77e9\u9635\u64cd\u4f5c\uff0c\u53ef\u4ee5\u4ec5\u5f15\u5165tiny_mat\u5e93\uff1b\u5bf9\u4e8e\u590d\u6742\u7684\u77e9\u9635\u64cd\u4f5c\uff0c\u5efa\u8bae\u4f7f\u7528tiny_matrix\u5e93\u3002tiny_matrix\u5e93\u662f\u4e00\u4e2aC++\u5b9e\u73b0\u7684\u77e9\u9635\u5e93\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u529f\u80fd\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002\u5b83\u652f\u6301\u6d6e\u70b9\u6570\u548c\u6574\u6570\u77e9\u9635\u7684\u52a0\u6cd5\u3001\u51cf\u6cd5\u3001\u4e58\u6cd5\u3001\u8f6c\u7f6e\u3001\u6c42\u9006\u7b49\u64cd\u4f5c\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_1","title":"\u76ee\u5f55","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500Vector\n    \u2514\u2500\u2500Matrix\n        \u251c\u2500\u2500 tiny_mat (c) &lt;---\n        \u2514\u2500\u2500 tiny_matrix (c++)\n</code></pre> <pre><code>// print matrix\nvoid print_matrix(const char *name, const float *mat, int rows, int cols);\n// print matrix padded (row-major)\nvoid print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step);\n// addition\ntiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// subtraction\ntiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\ntiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n// multiplication\ntiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k);\ntiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding);\ntiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_2","title":"\u5de5\u5177\u51fd\u6570","text":""},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_3","title":"\u6253\u5370\u77e9\u9635","text":"<pre><code>void print_matrix(const char *name, const float *mat, int rows, int cols);\n</code></pre> <p>\u51fd\u6570: \u4ee5\u884c\u4e3b\u5e8f\u6253\u5370\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>name</code>: \u77e9\u9635\u540d\u79f0\u3002</p> </li> <li> <p><code>mat</code>: \u77e9\u9635\u6570\u636e\u6307\u9488\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u65e0\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_4","title":"\u6253\u5370\u5e26\u586b\u5145\u7684\u77e9\u9635","text":"<pre><code>void print_matrix_padded(const char *name, const float *mat, int rows, int cols, int step);\n</code></pre> <p>\u51fd\u6570: \u4ee5\u884c\u4e3b\u5e8f\u6253\u5370\u5e26\u586b\u5145\u7684\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>name</code>: \u77e9\u9635\u540d\u79f0\u3002</p> </li> <li> <p><code>mat</code>: \u77e9\u9635\u6570\u636e\u6307\u9488\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>step</code>: \u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u65e0\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_5","title":"\u77e9\u9635\u52a0\u6cd5","text":"<pre><code>tiny_error_t tiny_mat_add_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u52a0\u6cd5\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>input1</code>: \u8f93\u5165\u77e9\u96351\u3002</p> </li> <li> <p><code>input2</code>: \u8f93\u5165\u77e9\u96352\u3002</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u77e9\u9635\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>padd1</code>: \u8f93\u5165\u77e9\u96351\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd2</code>: \u8f93\u5165\u77e9\u96352\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>step1</code>: \u8f93\u5165\u77e9\u96351\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step2</code>: \u8f93\u5165\u77e9\u96352\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_6","title":"\u77e9\u9635\u52a0\u5e38\u6570","text":"<pre><code>tiny_error_t tiny_mat_addc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u52a0\u5e38\u6570\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u77e9\u9635\u3002</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u77e9\u9635\u3002</p> </li> <li> <p><code>C</code>: \u5e38\u6570\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>padd_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>step_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_7","title":"\u77e9\u9635\u51cf\u6cd5","text":"<pre><code>tiny_error_t tiny_mat_sub_f32(const float *input1, const float *input2, float *output, int rows, int cols, int padd1, int padd2, int padd_out, int step1, int step2, int step_out);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u51cf\u6cd5\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>input1</code>: \u8f93\u5165\u77e9\u96351\u3002</p> </li> <li> <p><code>input2</code>: \u8f93\u5165\u77e9\u96352\u3002</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u77e9\u9635\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>padd1</code>: \u8f93\u5165\u77e9\u96351\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd2</code>: \u8f93\u5165\u77e9\u96352\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>step1</code>: \u8f93\u5165\u77e9\u96351\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step2</code>: \u8f93\u5165\u77e9\u96352\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_8","title":"\u77e9\u9635\u51cf\u5e38\u6570","text":"<pre><code>tiny_error_t tiny_mat_subc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u51cf\u5e38\u6570\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u77e9\u9635\u3002</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u77e9\u9635\u3002</p> </li> <li> <p><code>C</code>: \u5e38\u6570\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>padd_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>step_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_9","title":"\u77e9\u9635\u4e58\u6cd5","text":"<pre><code>tiny_error_t tiny_mat_mult_f32(const float *A, const float *B, float *C, int m, int n, int k);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u4e58\u6cd5\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>A</code>: \u8f93\u5165\u77e9\u9635A\u3002</p> </li> <li> <p><code>B</code>: \u8f93\u5165\u77e9\u9635B\u3002</p> </li> <li> <p><code>C</code>: \u8f93\u51fa\u77e9\u9635C\u3002</p> </li> <li> <p><code>m</code>: \u77e9\u9635A\u7684\u884c\u6570\u3002</p> </li> <li> <p><code>n</code>: \u77e9\u9635A\u7684\u5217\u6570\u3002</p> </li> <li> <p><code>k</code>: \u77e9\u9635B\u7684\u5217\u6570\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_10","title":"\u6269\u5c55\u77e9\u9635\u4e58\u6cd5","text":"<pre><code>tiny_error_t tiny_mat_mult_ex_f32(const float *A, const float *B, float *C, int A_rows, int A_cols, int B_cols, int A_padding, int B_padding, int C_padding);\n</code></pre> <p>\u51fd\u6570: \u6269\u5c55\u77e9\u9635\u4e58\u6cd5\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>A</code>: \u8f93\u5165\u77e9\u9635A\u3002</p> </li> <li> <p><code>B</code>: \u8f93\u5165\u77e9\u9635B\u3002</p> </li> <li> <p><code>C</code>: \u8f93\u51fa\u77e9\u9635C\u3002</p> </li> <li> <p><code>A_rows</code>: \u77e9\u9635A\u7684\u884c\u6570\u3002</p> </li> <li> <p><code>A_cols</code>: \u77e9\u9635A\u7684\u5217\u6570\u3002</p> </li> <li> <p><code>B_cols</code>: \u77e9\u9635B\u7684\u5217\u6570\u3002</p> </li> <li> <p><code>A_padding</code>: \u77e9\u9635A\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>B_padding</code>: \u77e9\u9635B\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>C_padding</code>: \u77e9\u9635C\u7684\u586b\u5145\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-api/#_11","title":"\u77e9\u9635\u4e58\u5e38\u6570","text":"<pre><code>tiny_error_t tiny_mat_multc_f32(const float *input, float *output, float C, int rows, int cols, int padd_in, int padd_out, int step_in, int step_out);\n</code></pre> <p>\u51fd\u6570: \u77e9\u9635\u4e58\u5e38\u6570\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>input</code>: \u8f93\u5165\u77e9\u9635\u3002</p> </li> <li> <p><code>output</code>: \u8f93\u51fa\u77e9\u9635\u3002</p> </li> <li> <p><code>C</code>: \u5e38\u6570\u3002</p> </li> <li> <p><code>rows</code>: \u77e9\u9635\u884c\u6570\u3002</p> </li> <li> <p><code>cols</code>: \u77e9\u9635\u5217\u6570\u3002</p> </li> <li> <p><code>padd_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>padd_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u586b\u5145\u3002</p> </li> <li> <p><code>step_in</code>: \u8f93\u5165\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>step_out</code>: \u8f93\u51fa\u77e9\u9635\u7684\u6b65\u957f\u3002</p> </li> </ul> <p>\u8fd4\u56de: \u9519\u8bef\u7801\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-mat-code/","title":"\u4ee3\u7801","text":""},{"location":"zh/MATH/MATRIX/tiny-mat-test/","title":"TINY_MAT \u6d4b\u8bd5","text":""},{"location":"zh/MATH/MATRIX/tiny-mat-test/#_1","title":"\u6d4b\u8bd5\u4ee3\u7801","text":""},{"location":"zh/MATH/MATRIX/tiny-mat-test/#maincpp","title":"main.cpp","text":"<pre><code>#include \"tiny_mat_test.hpp\"\n\nextern \"C\" void app_main(void)\n{\n    tiny_mat_test();\n}\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-mat-test/#_2","title":"\u6d4b\u8bd5\u7ed3\u679c","text":"<pre><code>============ [tiny_mat_test] ============\n\n================================================================================\nTest Case 1: tiny_mat_add_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1\n\nInput1 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nInput2 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5 \n  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\n          [4.5  5.5  6.5  7.5]  &lt;- Row 1\n          [8.5  9.5 10.5 11.5]  &lt;- Row 2\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.5   3.5   5.5   7.5   9.5  11.5  13.5  15.5  17.5  19.5  21.5  23.5 \n  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\n          [9.5 11.5 13.5 15.5]  &lt;- Row 1\n          [17.5 19.5 21.5 23.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.5   3.5   5.5   7.5   9.5  11.5  13.5  15.5  17.5  19.5  21.5  23.5 \n  Matrix: [1.5  3.5  5.5  7.5]  &lt;- Row 0\n          [9.5 11.5 13.5 15.5]  &lt;- Row 1\n          [17.5 19.5 21.5 23.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 2: tiny_mat_add_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad1=2, pad2=1, pad_out=2, step1=2, step2=3, step_out=2\nIndex formula: index = row * (cols + padding) + col * step\n\nInput1 Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nInput2 Memory Layout (16 elements, pad=1, step=3):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\n  Value:   0.5  0.0  0.0  1.5  3.5  0.0  2.5  4.5  0.0  0.0  5.5  0.0 ...\n  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\n          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\n          (X = padding/unused)\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.5  0.0  3.5  0.0  5.5  7.5  0.0  9.5  0.0 11.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.5  0.0  3.5  0.0  5.5  7.5  0.0  9.5  0.0 11.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.5  X  3.5  X  5.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [7.5  X  9.5  X 11.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 3: tiny_mat_addc_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1, C=2.5\n\nInput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nConstant C =   2.5\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5  12.5  13.5  14.5 \n  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\n          [7.5  8.5  9.5 10.5]  &lt;- Row 1\n          [11.5 12.5 13.5 14.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5  12.5  13.5  14.5 \n  Matrix: [3.5  4.5  5.5  6.5]  &lt;- Row 0\n          [7.5  8.5  9.5 10.5]  &lt;- Row 1\n          [11.5 12.5 13.5 14.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 4: tiny_mat_addc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad_in=2, pad_out=2, step_in=2, step_out=2, C=  1.5\nIndex formula: index = row * (cols + padding) + col * step\n\nInput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nConstant C =   1.5\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   2.5  0.0  3.5  0.0  4.5  5.5  0.0  6.5  0.0  7.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   2.5  0.0  3.5  0.0  4.5  5.5  0.0  6.5  0.0  7.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [2.5  X  3.5  X  4.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [5.5  X  6.5  X  7.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 5: tiny_mat_sub_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1\n\nInput1 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nInput2 Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5  11.5 \n  Matrix: [0.5  1.5  2.5  3.5]  &lt;- Row 0\n          [4.5  5.5  6.5  7.5]  &lt;- Row 1\n          [8.5  9.5 10.5 11.5]  &lt;- Row 2\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5 \n  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\n          [0.5  0.5  0.5  0.5]  &lt;- Row 1\n          [0.5  0.5  0.5  0.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5 \n  Matrix: [0.5  0.5  0.5  0.5]  &lt;- Row 0\n          [0.5  0.5  0.5  0.5]  &lt;- Row 1\n          [0.5  0.5  0.5  0.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 6: tiny_mat_sub_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad1=2, pad2=1, pad_out=2, step1=2, step2=3, step_out=2\nIndex formula: index = row * (cols + padding) + col * step\n\nInput1 Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nInput2 Memory Layout (16 elements, pad=1, step=3):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] ...\n  Value:   0.5  0.0  0.0  1.5  3.5  0.0  2.5  4.5  0.0  0.0  5.5  0.0 ...\n  Matrix: [0.5  X  X  1.5  X  X  2.5]  &lt;- Row 0 (indices: 0, 3, 6)\n          [3.5  X  X  4.5  X  X  5.5]  &lt;- Row 1 (indices: 4, 7, 10)\n          (X = padding/unused)\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   0.5  0.0  0.5  0.0  0.5  0.5  0.0  0.5  0.0  0.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   0.5  0.0  0.5  0.0  0.5  0.5  0.0  0.5  0.0  0.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [0.5  X  0.5  X  0.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [0.5  X  0.5  X  0.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 7: tiny_mat_subc_f32 - Contiguous Memory Layout (pad=0, step=1)\n================================================================================\nParameters: rows=3, cols=4, pad=0, step=1, C=2.5\n\nInput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nConstant C =   2.5\n\nExpected Output Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:   -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5 \n  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\n          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\n          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\n\nOutput Memory Layout (12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:   -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5 \n  Matrix: [-1.5 -0.5  0.5  1.5]  &lt;- Row 0\n          [ 2.5  3.5  4.5  5.5]  &lt;- Row 1\n          [ 6.5  7.5  8.5  9.5] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 8: tiny_mat_subc_f32 - Non-Contiguous Memory Layout (pad!=0, step&gt;1)\n================================================================================\nParameters: rows=2, cols=3, pad_in=2, pad_out=2, step_in=2, step_out=2, C=  1.5\nIndex formula: index = row * (cols + padding) + col * step\n\nInput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nConstant C =   1.5\n\nExpected Output Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:  -0.5  0.0  0.5  0.0  1.5  2.5  0.0  3.5  0.0  4.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\nOutput Memory Layout (20 elements, pad=2, step=2):\n  Index:  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11] [12] [13] [14] ...\n  Value:  -0.5  0.0  0.5  0.0  1.5  2.5  0.0  3.5  0.0  4.5  0.0  0.0  0.0  0.0  0.0 ...\n  Matrix: [-0.5  X  0.5  X  1.5]  &lt;- Row 0 (indices: 0, 2, 4)\n          [ 2.5  X  3.5  X  4.5]  &lt;- Row 1 (indices: 5, 7, 9)\n          (X = padding/unused)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 9: tiny_mat_mult_f32 - Basic Matrix Multiplication\n================================================================================\nParameters: m=3, n=4, k=2 (A is 3x4, B is 4x2, C is 3x2)\nNote: This function always uses ESP-DSP on ESP32, standard implementation otherwise\n\nMatrix A Memory Layout (3x4, 12 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2\n\nMatrix B Memory Layout (4x2, 8 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5 \n  Matrix: [0.5  1.5]  &lt;- Row 0\n          [2.5  3.5]  &lt;- Row 1\n          [4.5  5.5]  &lt;- Row 2\n          [6.5  7.5]  &lt;- Row 3\n\nExpected Output Matrix C Memory Layout (3x2, 6 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0\n          [101.0  127.0]  &lt;- Row 1\n          [157.0  199.0] &lt;- Row 2\n  Calculation:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 =  45.0\n    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1] + A[0][3]*B[3][1]\n            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 + 4.0*7.5 =  55.0\n\nOutput Matrix C Memory Layout (3x2, 6 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0\n          [101.0  127.0]  &lt;- Row 1\n          [157.0  199.0] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 10: tiny_mat_mult_f32 - Square Matrix Multiplication\n================================================================================\nParameters: m=3, n=3, k=3 (A is 3x3, B is 3x3, C is 3x3)\nNote: This function always uses ESP-DSP on ESP32, standard implementation otherwise\n\nMatrix A Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0 \n  Matrix: [1.0  2.0  3.0]  &lt;- Row 0\n          [4.0  5.0  6.0]  &lt;- Row 1\n          [7.0  8.0  9.0]  &lt;- Row 2\n\nMatrix B Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    0.5   1.0   1.5   2.0   2.5   3.0   3.5   4.0   4.5 \n  Matrix: [0.5  1.0  1.5]  &lt;- Row 0\n          [2.0  2.5  3.0]  &lt;- Row 1\n          [3.5  4.0  4.5]  &lt;- Row 2\n\nExpected Output Matrix C Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    15.0   18.0   21.0   33.0   40.5   48.0   51.0   63.0   75.0 \n  Matrix: [ 15.0   18.0   21.0]  &lt;- Row 0\n          [ 33.0   40.5   48.0]  &lt;- Row 1\n          [ 51.0   63.0   75.0] &lt;- Row 2\n\nOutput Matrix C Memory Layout (3x3, 9 elements, contiguous):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    15.0   18.0   21.0   33.0   40.5   48.0   51.0   63.0   75.0 \n  Matrix: [ 15.0   18.0   21.0]  &lt;- Row 0\n          [ 33.0   40.5   48.0]  &lt;- Row 1\n          [ 51.0   63.0   75.0] &lt;- Row 2\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 11: tiny_mat_mult_ex_f32 - Contiguous Matrix Multiplication\n================================================================================\nParameters: A_rows=3, A_cols=4, B_cols=2, A_padding=0, B_padding=0, C_padding=0\nMatrix dimensions: A is 3x4, B is 4x2, C is 3x2\nNote: This should use ESP-DSP on ESP32 when all paddings are 0\n\nMatrix A Memory Layout (3x4, 12 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]  [11]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0  10.0  11.0  12.0 \n  Matrix: [1.0  2.0  3.0  4.0]  &lt;- Row 0 (indices: 0-3)\n          [5.0  6.0  7.0  8.0]  &lt;- Row 1 (indices: 4-7)\n          [9.0 10.0 11.0 12.0]  &lt;- Row 2 (indices: 8-11)\n  Step size: 4 (A_cols + A_padding = 4 + 0)\n\nMatrix B Memory Layout (4x2, 8 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:    0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5 \n  Matrix: [0.5  1.5]  &lt;- Row 0 (indices: 0-1)\n          [2.5  3.5]  &lt;- Row 1 (indices: 2-3)\n          [4.5  5.5]  &lt;- Row 2 (indices: 4-5)\n          [6.5  7.5]  &lt;- Row 3 (indices: 6-7)\n  Step size: 2 (B_cols + B_padding = 2 + 0)\n\nExpected Output Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0 (indices: 0-1)\n          [101.0  127.0]  &lt;- Row 1 (indices: 2-3)\n          [157.0  199.0] &lt;- Row 2 (indices: 4-5)\n  Step size: 2 (B_cols + C_padding = 2 + 0)\n  Calculation example:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0] + A[0][3]*B[3][0]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 + 4.0*6.5 =  45.0\n\nOutput Matrix C Memory Layout (3x2, 6 elements, contiguous, pad=0):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    45.0   55.0  101.0  127.0  157.0  199.0 \n  Matrix: [ 45.0   55.0]  &lt;- Row 0 (indices: 0-1)\n          [101.0  127.0]  &lt;- Row 1 (indices: 2-3)\n          [157.0  199.0] &lt;- Row 2 (indices: 4-5)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 12: tiny_mat_mult_ex_f32 - Padded Matrix Multiplication\n================================================================================\nParameters: A_rows=2, A_cols=3, B_cols=2, A_padding=2, B_padding=1, C_padding=1\nMatrix dimensions: A is 2x3, B is 3x2, C is 2x2\nNote: This should use own implementation when padding is non-zero\n\nMatrix A Memory Layout (2x3, pad=2, step=5, 10 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\n  Value:   1.0  2.0  3.0  0.0  0.0  4.0  5.0  6.0  0.0  0.0 \n  Matrix: [1.0  2.0  3.0  X   X]  &lt;- Row 0 (indices: 0, 1, 2, 3, 4)\n          [4.0  5.0  6.0  X   X]  &lt;- Row 1 (indices: 5, 6, 7, 8, 9)\n          (X = padding/unused)\n  Index calculation: A[i][j] = A[i * 5 + j]\n    Row 0: indices 0, 1, 2 (data), 3, 4 (padding)\n    Row 1: indices 5, 6, 7 (data), 8, 9 (padding)\n\nMatrix B Memory Layout (3x2, pad=1, step=3, 9 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:   0.5  1.5  0.0  2.5  3.5  0.0  4.5  5.5  0.0 \n  Matrix: [0.5  1.5  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [2.5  3.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n          [4.5  5.5  X]  &lt;- Row 2 (indices: 6, 7, 8)\n          (X = padding/unused)\n  Index calculation: B[i][j] = B[i * 3 + j]\n    Row 0: indices 0, 1 (data), 2 (padding)\n    Row 1: indices 3, 4 (data), 5 (padding)\n    Row 2: indices 6, 7 (data), 8 (padding)\n\nExpected Output Matrix C Memory Layout (2x2, pad=1, step=3, 6 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    19.0   25.0    0.0   41.5   56.5    0.0 \n  Matrix: [ 19.0   25.0  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 41.5   56.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n          (X = padding/unused)\n  Index calculation: C[i][j] = C[i * 3 + j]\n  Calculation:\n    C[0][0] = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0]\n            = A[0]*B[0] + A[1]*B[3] + A[2]*B[6]\n            = 1.0*0.5 + 2.0*2.5 + 3.0*4.5 =  19.0\n    C[0][1] = A[0][0]*B[0][1] + A[0][1]*B[1][1] + A[0][2]*B[2][1]\n            = 1.0*1.5 + 2.0*3.5 + 3.0*5.5 =  25.0\n    C[1][0] = A[1][0]*B[0][0] + A[1][1]*B[1][0] + A[1][2]*B[2][0]\n            = 4.0*0.5 + 5.0*2.5 + 6.0*4.5 =  41.5\n    C[1][1] = A[1][0]*B[0][1] + A[1][1]*B[1][1] + A[1][2]*B[2][1]\n            = 4.0*1.5 + 5.0*3.5 + 6.0*5.5 =  56.5\n\nOutput Matrix C Memory Layout (2x2, pad=1, step=3, 6 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]\n  Value:    19.0   25.0    0.0   41.5   56.5    0.0 \n  Matrix: [ 19.0   25.0  X]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 41.5   56.5  X]  &lt;- Row 1 (indices: 3, 4, 5)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 13: tiny_mat_multc_f32 - Contiguous Matrix Multiply Constant\n================================================================================\nParameters: rows=3, cols=3, padd_in=0, padd_out=0, step_in=1, step_out=1\nMatrix dimensions: 3x3\nConstant C: 2.5\nNote: This should use ESP-DSP on ESP32 when all paddings are 0 and all steps are 1\n\nInput Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:    1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0 \n  Matrix: [1.0  2.0  3.0]  &lt;- Row 0 (indices: 0, 1, 2)\n          [4.0  5.0  6.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [7.0  8.0  9.0]  &lt;- Row 2 (indices: 6, 7, 8)\n  Row stride: 3 (cols + padd_in = 3 + 0)\n  Index calculation: input[i][j] = input[i * 3 + j * 1]\n\nExpected Output Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:     2.5    5.0    7.5   10.0   12.5   15.0   17.5   20.0   22.5 \n  Matrix: [  2.5    5.0    7.5]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 10.0   12.5   15.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [ 17.5   20.0   22.5] &lt;- Row 2 (indices: 6, 7, 8)\n  Row stride: 3 (cols + padd_out = 3 + 0)\n  Index calculation: output[i][j] = output[i * 3 + j * 1]\n  Calculation: output[i][j] = input[i][j] * 2.5\n    Example: output[0][0] = input[0][0] * 2.5 = 1.0 * 2.5 = 2.5\n\nOutput Matrix Memory Layout (3x3, 9 elements, contiguous, pad=0, step=1):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]\n  Value:     2.5    5.0    7.5   10.0   12.5   15.0   17.5   20.0   22.5 \n  Matrix: [  2.5    5.0    7.5]  &lt;- Row 0 (indices: 0, 1, 2)\n          [ 10.0   12.5   15.0]  &lt;- Row 1 (indices: 3, 4, 5)\n          [ 17.5   20.0   22.5] &lt;- Row 2 (indices: 6, 7, 8)\n\n\u2713 Test PASSED\n================================================================================\n\n\n================================================================================\nTest Case 14: tiny_mat_multc_f32 - Padded and Strided Matrix Multiply Constant\n================================================================================\nParameters: rows=2, cols=3, padd_in=2, padd_out=1, step_in=2, step_out=1\nMatrix dimensions: 2x3\nConstant C: 3.0\nNote: This should use own implementation when padding is non-zero or step &gt; 1\n\nInput Matrix Memory Layout (2x3, pad=2, step=2, 10 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]\n  Value:   1.0  0.0  2.0  0.0  3.0  4.0  0.0  5.0  0.0  6.0 \n  Matrix: [1.0  X  2.0  X  3.0]  &lt;- Row 0 (data indices: 0, 2, 4)\n          [4.0  X  5.0  X  6.0]  &lt;- Row 1 (data indices: 5, 7, 9)\n          (X = unused/padding)\n  Row stride: 5 (cols + padd_in = 3 + 2)\n  Index calculation: input[i][j] = input[i * 5 + j * 2]\n    Row 0: input[0][0]=input[0]=1.0, input[0][1]=input[2]=2.0, input[0][2]=input[4]=3.0\n    Row 1: input[1][0]=input[5]=4.0, input[1][1]=input[7]=5.0, input[1][2]=input[9]=6.0\n\nExpected Output Matrix Memory Layout (2x3, pad=1, step=1, 8 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:     3.0    6.0    9.0    0.0   12.0   15.0   18.0    0.0 \n  Matrix: [  3.0    6.0    9.0  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\n          [ 12.0   15.0   18.0  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\n          (X = padding/unused)\n  Row stride: 4 (cols + padd_out = 3 + 1)\n  Index calculation: output[i][j] = output[i * 4 + j * 1]\n  Calculation: output[i][j] = input[i][j] * 3.0\n    Row 0: output[0][0] = input[0][0] * 3.0 = 1.0 * 3.0 = 3.0 (index 0)\n           output[0][1] = input[0][1] * 3.0 = 2.0 * 3.0 = 6.0 (index 1)\n           output[0][2] = input[0][2] * 3.0 = 3.0 * 3.0 = 9.0 (index 2)\n\nOutput Matrix Memory Layout (2x3, pad=1, step=1, 8 elements):\n  Index:  [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]\n  Value:     3.0    6.0    9.0    0.0   12.0   15.0   18.0    0.0 \n  Matrix: [  3.0    6.0    9.0  X]  &lt;- Row 0 (indices: 0, 1, 2, 3)\n          [ 12.0   15.0   18.0  X]  &lt;- Row 1 (indices: 4, 5, 6, 7)\n\n\u2713 Test PASSED\n================================================================================\n\n============ [test complete] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/","title":"\u77e9\u9635\u64cd\u4f5c - TINY_MATRIX","text":"<p>TINY_MATRIX\u5e93</p> <ul> <li>\u8be5\u5e93\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u77e9\u9635\u8fd0\u7b97\u5e93\uff0c\u57fa\u4e8eC++\u5b9e\u73b0\uff0c\u63d0\u4f9b\u4e86\u57fa\u672c\u7684\u77e9\u9635\u64cd\u4f5c\u548c\u7ebf\u6027\u4ee3\u6570\u529f\u80fd\u3002</li> <li>\u8be5\u5e93\u7684\u8bbe\u8ba1\u76ee\u6807\u662f\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u77e9\u9635\u64cd\u4f5c\u63a5\u53e3\uff0c\u9002\u5408\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002</li> </ul> <p>\u4f7f\u7528\u573a\u666f</p> <p>\u76f8\u5bf9\u4e8eTINY_MAT\u5e93\u800c\u8a00\uff0cTINY_MATRIX\u5e93\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u529f\u80fd\u548c\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\uff0c\u9002\u5408\u4e8e\u9700\u8981\u8fdb\u884c\u590d\u6742\u77e9\u9635\u8fd0\u7b97\u7684\u5e94\u7528\u573a\u666f\u3002\u4f46\u662f\u8bf7\u6ce8\u610f\uff0c\u8be5\u5e93\u57fa\u4e8eC++\u7f16\u5199\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_1","title":"\u51fd\u6570\u5217\u8868","text":"<pre><code>TinyMath\n    \u251c\u2500\u2500Vector\n    \u2514\u2500\u2500Matrix\n        \u251c\u2500\u2500 tiny_mat (c)\n        \u2514\u2500\u2500 tiny_matrix (c++) &lt;---\n</code></pre> <pre><code>/**\n * @file tiny_matrix.hpp\n * @author SHUAIWEN CUI (SHUAIWEN001@e.ntu.edu.sg)\n * @brief This file is the header file for the submodule matrix (advanced matrix operations) of the tiny_math middleware.\n * @version 1.0\n * @date 2025-04-17\n * @note This file is built on top of the mat.h file from the ESP-DSP library.\n *\n */\n\n#pragma once\n\n/* DEPENDENCIES */\n// TinyMath\n#include \"tiny_math_config.h\"\n#include \"tiny_vec.h\"\n#include \"tiny_mat.h\"\n\n// Standard Libraries\n#include &lt;iostream&gt;\n#include &lt;stdint.h&gt;\n\n#if MCU_PLATFORM_SELECTED == MCU_PLATFORM_ESP32\n// ESP32 DSP C++ Matrix library\n#include \"mat.h\"\n#endif\n\n/* STATEMENTS */\nnamespace tiny\n{\n    class Mat\n    {\n    public:\n        // ============================================================================\n        // Matrix Metadata\n        // ============================================================================\n        int row;         //&lt; number of rows\n        int col;         //&lt; number of columns\n        int pad;         //&lt; number of paddings between 2 rows\n        int stride;      //&lt; stride = (number of elements in a row) + padding\n        int element;     //&lt; number of elements = rows * cols\n        int memory;      //&lt; size of the data buffer = rows * stride\n        float *data;     //&lt; pointer to the data buffer\n        float *temp;     //&lt; pointer to the temporary data buffer\n        bool ext_buff;   //&lt; flag indicates that matrix use external buffer\n        bool sub_matrix; //&lt; flag indicates that matrix is a subset of another matrix\n\n        // ============================================================================\n        // Rectangular ROI Structure\n        // ============================================================================\n        /**\n         * @name Region of Interest (ROI) Structure\n         * @brief This is the structure for ROI\n         */\n        struct ROI\n        {\n            int pos_x;  ///&lt; starting column index\n            int pos_y;  ///&lt; starting row index\n            int width;  ///&lt; width of ROI (columns)\n            int height; ///&lt; height of ROI (rows)\n\n            ROI(int pos_x = 0, int pos_y = 0, int width = 0, int height = 0);\n            void resize_roi(int pos_x, int pos_y, int width, int height);\n            int area_roi(void) const;\n        };\n\n        // ============================================================================\n        // Printing Functions\n        // ============================================================================\n        void print_info() const;\n        void print_matrix(bool show_padding);\n\n        // ============================================================================\n        // Constructors &amp; Destructor\n        // ============================================================================\n        void alloc_mem(); // Allocate internal memory\n        Mat();\n        Mat(int rows, int cols);\n        Mat(int rows, int cols, int stride);\n        Mat(float *data, int rows, int cols);\n        Mat(float *data, int rows, int cols, int stride);\n        Mat(const Mat &amp;src);\n        ~Mat();\n\n        // ============================================================================\n        // Element Access\n        // ============================================================================\n        inline float &amp;operator()(int row, int col) { return data[row * stride + col]; }\n        inline const float &amp;operator()(int row, int col) const { return data[row * stride + col]; }\n\n        // ============================================================================\n        // Data Manipulation\n        // ============================================================================\n        tiny_error_t copy_paste(const Mat &amp;src, int row_pos, int col_pos);\n        tiny_error_t copy_head(const Mat &amp;src);\n        Mat view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const;\n        Mat view_roi(const Mat::ROI &amp;roi) const;\n        Mat copy_roi(int start_row, int start_col, int roi_rows, int roi_cols);\n        Mat copy_roi(const Mat::ROI &amp;roi);\n        Mat block(int start_row, int start_col, int block_rows, int block_cols);\n        void swap_rows(int row1, int row2);\n        void swap_cols(int col1, int col2);\n        void clear(void);\n\n        // ============================================================================\n        // Arithmetic Operators\n        // ============================================================================\n        Mat &amp;operator=(const Mat &amp;src);    // Copy assignment\n        Mat &amp;operator+=(const Mat &amp;A);     // Add matrix\n        Mat &amp;operator+=(float C);          // Add constant\n        Mat &amp;operator-=(const Mat &amp;A);     // Subtract matrix\n        Mat &amp;operator-=(float C);          // Subtract constant \n        Mat &amp;operator*=(const Mat &amp;A);     // Multiply matrix\n        Mat &amp;operator*=(float C);          // Multiply constant\n        Mat &amp;operator/=(const Mat &amp;B);     // Divide matrix\n        Mat &amp;operator/=(float C);          // Divide constant\n        Mat operator^(int C);              // Exponentiation\n\n        // ============================================================================\n        // Linear Algebra - Basic Operations\n        // ============================================================================\n        Mat transpose();                   // Transpose matrix\n        float determinant();               // Compute determinant (auto-selects method based on size)\n        float determinant_laplace();        // Compute determinant using Laplace expansion (O(n!), for small matrices)\n        float determinant_lu();            // Compute determinant using LU decomposition (O(n\u00b3), efficient for large matrices)\n        float determinant_gaussian();      // Compute determinant using Gaussian elimination (O(n\u00b3), efficient for large matrices)\n        Mat adjoint();                     // Compute adjoint matrix\n        Mat inverse_adjoint();            // Compute inverse using adjoint method\n        void normalize();                  // Normalize matrix\n        float norm() const;                // Compute matrix norm\n        float dotprod(const Mat &amp;A, const Mat &amp;B);  // Dot product\n\n        // ============================================================================\n        // Linear Algebra - Matrix Utilities\n        // ============================================================================\n        static Mat eye(int size);          // Create identity matrix\n        static Mat ones(int rows, int cols);  // Create matrix filled with ones\n        static Mat ones(int size);         // Create square matrix filled with ones\n        static Mat augment(const Mat &amp;A, const Mat &amp;B);  // Horizontal concatenation [A | B]\n        static Mat vstack(const Mat &amp;A, const Mat &amp;B);   // Vertical concatenation [A; B]\n\n        /**\n         * @brief Gram-Schmidt orthogonalization process\n         * @note Orthogonalizes a set of vectors using the Gram-Schmidt process\n         * @param vectors Input matrix where each column is a vector to be orthogonalized\n         * @param orthogonal_vectors Output matrix for orthogonalized vectors (each column is orthogonal)\n         * @param coefficients Output matrix for projection coefficients (R matrix in QR decomposition)\n         * @param tolerance Minimum norm threshold for linear independence check\n         * @return true if successful, false if input is invalid\n         */\n        static bool gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                               Mat &amp;coefficients, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Linear Algebra - Matrix Operations\n        // ============================================================================\n        Mat minor(int row, int col);       // Minor matrix (submatrix after removing row and col)\n        Mat cofactor(int row, int col);    // Cofactor matrix\n        Mat gaussian_eliminate() const;    // Gaussian elimination\n        Mat row_reduce_from_gaussian();   // Row reduction from Gaussian form\n        Mat inverse_gje();                 // Inverse using Gaussian-Jordan elimination\n\n        // ============================================================================\n        // Linear Algebra - Linear System Solving\n        // ============================================================================\n        Mat solve(const Mat &amp;A, const Mat &amp;b) const;  // Solve Ax = b using Gaussian elimination\n        Mat band_solve(Mat A, Mat b, int k);          // Solve banded system\n        Mat roots(Mat A, Mat y);                      // Alternative solve method\n\n        // ============================================================================\n        // Matrix Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct LUDecomposition;\n        struct CholeskyDecomposition;\n        struct QRDecomposition;\n        struct SVDDecomposition;\n\n        // Matrix property checks\n        bool is_symmetric(float tolerance = 1e-6f) const;\n        bool is_positive_definite(float tolerance = 1e-6f) const;\n\n        // Decomposition methods\n        LUDecomposition lu_decompose(bool use_pivoting = true) const;\n        CholeskyDecomposition cholesky_decompose() const;\n        QRDecomposition qr_decompose() const;\n        SVDDecomposition svd_decompose(int max_iter = 100, float tolerance = 1e-6f) const;\n\n        // Solve using decomposition (more efficient for multiple RHS)\n        static Mat solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b);\n        static Mat solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b);\n        static Mat solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b);  // Least squares solution\n\n        // Pseudo-inverse using SVD (for rank-deficient or non-square matrices)\n        static Mat pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance = 1e-6f);\n\n        // ============================================================================\n        // Eigenvalue &amp; Eigenvector Decomposition\n        // ============================================================================\n        // Forward declarations (structures defined after class)\n        struct EigenPair;\n        struct EigenDecomposition;\n\n        // Single eigenvalue methods (fast, for real-time applications)\n        EigenPair power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n        EigenPair inverse_power_iteration(int max_iter = 1000, float tolerance = 1e-6f) const;\n\n        // Complete eigendecomposition methods\n        EigenDecomposition eigendecompose_jacobi(float tolerance = 1e-6f, int max_iter = 100) const;\n        EigenDecomposition eigendecompose_qr(int max_iter = 100, float tolerance = 1e-6f) const;\n        EigenDecomposition eigendecompose(float tolerance = 1e-6f) const;  // Auto-select method\n\n    protected:\n\n    private:\n\n    };\n\n    // ============================================================================\n    // Matrix Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold LU decomposition results\n     * @note A = L * U, where L is lower triangular and U is upper triangular\n     */\n    struct Mat::LUDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix (with unit diagonal)\n        Mat U;                 ///&lt; Upper triangular matrix\n        Mat P;                 ///&lt; Permutation matrix (if pivoting used)\n        bool pivoted;          ///&lt; Whether pivoting was used\n        tiny_error_t status;   ///&lt; Computation status\n\n        LUDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold Cholesky decomposition results\n     * @note A = L * L^T, where L is lower triangular (for symmetric positive definite matrices)\n     */\n    struct Mat::CholeskyDecomposition\n    {\n        Mat L;                 ///&lt; Lower triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        CholeskyDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold QR decomposition results\n     * @note A = Q * R, where Q is orthogonal and R is upper triangular\n     */\n    struct Mat::QRDecomposition\n    {\n        Mat Q;                 ///&lt; Orthogonal matrix (Q^T * Q = I)\n        Mat R;                 ///&lt; Upper triangular matrix\n        tiny_error_t status;   ///&lt; Computation status\n\n        QRDecomposition();\n    };\n\n    /**\n     * @brief Structure to hold SVD decomposition results\n     * @note A = U * S * V^T, where U and V are orthogonal, S is diagonal (singular values)\n     */\n    struct Mat::SVDDecomposition\n    {\n        Mat U;                 ///&lt; Left singular vectors (orthogonal matrix)\n        Mat S;                 ///&lt; Singular values (diagonal matrix or vector)\n        Mat V;                 ///&lt; Right singular vectors (orthogonal matrix, V^T)\n        int rank;              ///&lt; Numerical rank of the matrix\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        SVDDecomposition();\n    };\n\n    // ============================================================================\n    // Eigenvalue &amp; Eigenvector Decomposition Structures\n    // ============================================================================\n    /**\n     * @brief Structure to hold a single eigenvalue-eigenvector pair\n     * @note Used primarily for power iteration method\n     */\n    struct Mat::EigenPair\n    {\n        float eigenvalue;      ///&lt; Eigenvalue (real part)\n        Mat eigenvector;       ///&lt; Corresponding eigenvector (column vector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenPair();\n    };\n\n    /**\n     * @brief Structure to hold complete eigenvalue decomposition results\n     * @note Contains all eigenvalues and eigenvectors\n     */\n    struct Mat::EigenDecomposition\n    {\n        Mat eigenvalues;       ///&lt; Eigenvalues (diagonal matrix or vector)\n        Mat eigenvectors;      ///&lt; Eigenvector matrix (each column is an eigenvector)\n        int iterations;        ///&lt; Number of iterations performed\n        tiny_error_t status;   ///&lt; Computation status\n\n        EigenDecomposition();\n    };\n\n    // ============================================================================\n    // Stream Operators\n    // ============================================================================\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m);\n    std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi);\n    std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m);\n\n    // ============================================================================\n    // Global Arithmetic Operators\n    // ============================================================================\n    Mat operator+(const Mat &amp;A, const Mat &amp;B);\n    Mat operator+(const Mat &amp;A, float C);\n    Mat operator-(const Mat &amp;A, const Mat &amp;B);\n    Mat operator-(const Mat &amp;A, float C);\n    Mat operator*(const Mat &amp;A, const Mat &amp;B);\n    Mat operator*(const Mat &amp;A, float C);\n    Mat operator*(float C, const Mat &amp;A);\n    Mat operator/(const Mat &amp;A, float C);\n    Mat operator/(const Mat &amp;A, const Mat &amp;B);\n    bool operator==(const Mat &amp;A, const Mat &amp;B);\n\n}\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_2","title":"\u77e9\u9635\u5143\u6570\u636e","text":"<p>\u77e9\u9635\u7ed3\u6784</p> <p>Mat\u7c7b\u4f7f\u7528\u884c\u4e3b\u5e8f\u5b58\u50a8\u5e03\u5c40\uff0c\u652f\u6301\u586b\u5145\u548c\u6b65\u5e45\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5e76\u4e0eDSP\u5e93\u517c\u5bb9\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_3","title":"\u6838\u5fc3\u7ef4\u5ea6","text":"<ul> <li> <p><code>int row</code> : \u77e9\u9635\u7684\u884c\u6570\u3002</p> </li> <li> <p><code>int col</code> : \u77e9\u9635\u7684\u5217\u6570\u3002</p> </li> <li> <p><code>int element</code> : \u5143\u7d20\u603b\u6570 = \u884c\u6570 \u00d7 \u5217\u6570\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_4","title":"\u5185\u5b58\u5e03\u5c40","text":"<ul> <li> <p><code>int stride</code> : \u6b65\u5e45 = (\u6bcf\u884c\u5143\u7d20\u6570) + \u586b\u5145\u6570\u3002\u6b65\u5e45\u51b3\u5b9a\u4e86\u5728\u5185\u5b58\u4e2d\u79fb\u52a8\u5230\u4e0b\u4e00\u884c\u9700\u8981\u8df3\u8fc7\u7684\u5143\u7d20\u6570\u3002</p> </li> <li> <p><code>int pad</code> : \u4e24\u884c\u4e4b\u95f4\u7684\u586b\u5145\u5143\u7d20\u6570\u3002\u586b\u5145\u7528\u4e8e\u5185\u5b58\u5bf9\u9f50\u548cDSP\u4f18\u5316\u3002</p> </li> <li> <p><code>int memory</code> : \u6570\u636e\u7f13\u51b2\u533a\u5927\u5c0f = \u884c\u6570 \u00d7 \u6b65\u5e45\uff08\u4ee5float\u5143\u7d20\u4e3a\u5355\u4f4d\uff09\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_5","title":"\u6570\u636e\u6307\u9488","text":"<ul> <li> <p><code>float *data</code> : \u6307\u5411\u5305\u542b\u77e9\u9635\u5143\u7d20\u7684\u6570\u636e\u7f13\u51b2\u533a\u7684\u6307\u9488\u3002\u5143\u7d20\u6309\u884c\u4e3b\u5e8f\u5b58\u50a8\uff1a\u4f4d\u7f6e(i, j)\u7684\u5143\u7d20\u4f4d\u4e8e <code>data[i * stride + j]</code>\u3002</p> </li> <li> <p><code>float *temp</code> : \u6307\u5411\u4e34\u65f6\u6570\u636e\u7f13\u51b2\u533a\u7684\u6307\u9488\uff08\u5982\u679c\u5df2\u5206\u914d\uff09\u3002\u67d0\u4e9b\u64cd\u4f5c\u5185\u90e8\u4f7f\u7528\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_6","title":"\u5185\u5b58\u7ba1\u7406\u6807\u5fd7","text":"<ul> <li> <p><code>bool ext_buff</code> : \u6807\u5fd7\u77e9\u9635\u662f\u5426\u4f7f\u7528\u5916\u90e8\u7f13\u51b2\u533a\u3002\u5f53\u4e3a<code>true</code>\u65f6\uff0c\u6790\u6784\u51fd\u6570\u4e0d\u4f1a\u91ca\u653e\u5185\u5b58\uff08\u8c03\u7528\u8005\u8d1f\u8d23\u7ba1\u7406\uff09\u3002</p> </li> <li> <p><code>bool sub_matrix</code> : \u6807\u5fd7\u77e9\u9635\u662f\u5426\u4e3a\u53e6\u4e00\u4e2a\u77e9\u9635\u7684\u5b50\u96c6/\u89c6\u56fe\u3002\u5f53\u4e3a<code>true</code>\u65f6\uff0c\u77e9\u9635\u4e0e\u7236\u77e9\u9635\u5171\u4eab\u6570\u636e\u3002</p> </li> </ul> <p>\u5185\u5b58\u5e03\u5c40\u793a\u4f8b</p> <p>\u5bf9\u4e8e3\u00d74\u77e9\u9635\uff0c\u6b65\u5e45=4\uff08\u65e0\u586b\u5145\uff09\uff1a <pre><code>[a b c d]   \u884c 0: data[0*4+0] \u5230 data[0*4+3]\n[e f g h]   \u884c 1: data[1*4+0] \u5230 data[1*4+3]\n[i j k l]   \u884c 2: data[2*4+0] \u5230 data[2*4+3]\n</code></pre></p> <p>\u5bf9\u4e8e3\u00d74\u77e9\u9635\uff0c\u6b65\u5e45=6\uff08\u586b\u5145=2\uff09\uff1a <pre><code>[a b c d _ _]   \u884c 0: data[0*6+0] \u5230 data[0*6+3]\uff0c\u586b\u5145\u5728 data[0*6+4,5]\n[e f g h _ _]   \u884c 1: data[1*6+0] \u5230 data[1*6+3]\uff0c\u586b\u5145\u5728 data[1*6+4,5]\n[i j k l _ _]   \u884c 2: data[2*6+0] \u5230 data[2*6+3]\uff0c\u586b\u5145\u5728 data[2*6+4,5]\n</code></pre></p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi","title":"ROI \u7ed3\u6784","text":"<p>\u611f\u5174\u8da3\u533a\u57df</p> <p>ROI\uff08\u611f\u5174\u8da3\u533a\u57df\uff09\u7ed3\u6784\u8868\u793a\u77e9\u9635\u7684\u77e9\u5f62\u5b50\u533a\u57df\u3002\u5b83\u4e0e<code>view_roi()</code>\u548c<code>copy_roi()</code>\u51fd\u6570\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u9ad8\u6548\u5730\u63d0\u53d6\u6216\u5f15\u7528\u5b50\u77e9\u9635\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi_1","title":"ROI \u5143\u6570\u636e","text":"<ul> <li> <p><code>int pos_x</code> : \u8d77\u59cb\u5217\u7d22\u5f15\uff08\u5de6\u4e0a\u89d2\u7684x\u5750\u6807\uff09\u3002</p> </li> <li> <p><code>int pos_y</code> : \u8d77\u59cb\u884c\u7d22\u5f15\uff08\u5de6\u4e0a\u89d2\u7684y\u5750\u6807\uff09\u3002</p> </li> <li> <p><code>int width</code> : ROI\u7684\u5bbd\u5ea6\uff08\u5217\u6570\uff09\u3002</p> </li> <li> <p><code>int height</code> : ROI\u7684\u9ad8\u5ea6\uff08\u884c\u6570\uff09\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi_2","title":"ROI \u6784\u9020\u51fd\u6570","text":"<pre><code>Mat::ROI::ROI(int pos_x = 0, int pos_y = 0, int width = 0, int height = 0);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6784\u9020\u4e00\u4e2a ROI \u5bf9\u8c61\uff0c\u9ed8\u8ba4\u503c\u4e3a (0, 0, 0, 0)\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int pos_x</code>: \u8d77\u59cb\u5217\u7d22\u5f15</p> </li> <li> <p><code>int pos_y</code>: \u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int width</code>: ROI \u7684\u5bbd\u5ea6\uff08\u5217\u6570\uff09</p> </li> <li> <p><code>int height</code>: ROI \u7684\u9ad8\u5ea6\uff08\u884c\u6570\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi_3","title":"ROI \u91cd\u7f6e\u51fd\u6570","text":"<pre><code>void Mat::ROI::resize_roi(int pos_x, int pos_y, int width, int height);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u91cd\u7f6e ROI \u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int pos_x</code>: \u8d77\u59cb\u5217\u7d22\u5f15</p> </li> <li> <p><code>int pos_y</code>: \u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int width</code>: ROI \u7684\u5bbd\u5ea6\uff08\u5217\u6570\uff09</p> </li> <li> <p><code>int height</code>: ROI \u7684\u9ad8\u5ea6\uff08\u884c\u6570\uff09</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>void</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi_4","title":"ROI \u9762\u79ef\u51fd\u6570","text":"<pre><code>int Mat::ROI::area_roi(void) const;\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u8ba1\u7b97 ROI \u7684\u9762\u79ef\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>\u6574\u6570\u7c7b\u578b ROI \u7684\u9762\u79ef</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_7","title":"\u6253\u5370\u51fd\u6570","text":"<p>\u8c03\u8bd5\u5de5\u5177</p> <p>\u8fd9\u4e9b\u51fd\u6570\u5bf9\u4e8e\u8c03\u8bd5\u548c\u7406\u89e3\u77e9\u9635\u72b6\u6001\u81f3\u5173\u91cd\u8981\u3002\u4f7f\u7528\u5b83\u4eec\u6765\u9a8c\u8bc1\u77e9\u9635\u7ef4\u5ea6\u3001\u5185\u5b58\u5e03\u5c40\u548c\u6570\u636e\u503c\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_8","title":"\u6253\u5370\u77e9\u9635\u4fe1\u606f","text":"<pre><code>void Mat::print_info() const;\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6253\u5370\u5168\u9762\u7684\u77e9\u9635\u4fe1\u606f\uff0c\u5305\u62ec\uff1a</p> <ul> <li> <p>\u7ef4\u5ea6\uff1a\u884c\u6570\u3001\u5217\u6570\u3001\u5143\u7d20\u6570</p> </li> <li> <p>\u5185\u5b58\u5e03\u5c40\uff1a\u586b\u5145\u6570\u3001\u6b65\u5e45\u3001\u5185\u5b58\u5927\u5c0f</p> </li> <li> <p>\u6307\u9488\uff1a\u6570\u636e\u7f13\u51b2\u533a\u5730\u5740\u3001\u4e34\u65f6\u7f13\u51b2\u533a\u5730\u5740</p> </li> <li> <p>\u6807\u5fd7\uff1a\u5916\u90e8\u7f13\u51b2\u533a\u4f7f\u7528\u60c5\u51b5\u3001\u5b50\u77e9\u9635\u72b6\u6001</p> </li> <li> <p>\u8b66\u544a\uff1a\u7ef4\u5ea6\u4e0d\u5339\u914d\u3001\u65e0\u6548\u72b6\u6001</p> </li> </ul> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>void</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u8c03\u8bd5: \u5bf9\u4e8e\u9a8c\u8bc1\u77e9\u9635\u72b6\u6001\u548c\u68c0\u6d4b\u5185\u5b58\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002</p> </li> <li> <p>\u5185\u5b58\u5206\u6790: \u663e\u793a\u5b9e\u9645\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u4e0e\u903b\u8f91\u5927\u5c0f\u7684\u5bf9\u6bd4\uff0c\u5e2e\u52a9\u8bc6\u522b\u5185\u5b58\u6548\u7387\u95ee\u9898\u3002</p> </li> <li> <p>\u5b50\u77e9\u9635\u68c0\u6d4b: \u6e05\u695a\u5730\u6307\u793a\u77e9\u9635\u662f\u5426\u4e3a\u89c6\u56fe\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5185\u5b58\u7ba1\u7406\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_9","title":"\u6253\u5370\u77e9\u9635\u5143\u7d20","text":"<pre><code>void Mat::print_matrix(bool show_padding);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u4ee5\u683c\u5f0f\u5316\u8868\u683c\u5f62\u5f0f\u6253\u5370\u77e9\u9635\u5143\u7d20\u3002\u53ef\u9009\u62e9\u663e\u793a\u7531\u89c6\u89c9\u5206\u9694\u7b26\u5206\u9694\u7684\u586b\u5145\u5143\u7d20\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>bool show_padding</code> : \u5982\u679c\u4e3a<code>true</code>\uff0c\u663e\u793a\u586b\u5145\u503c\u5e76\u7528\u5206\u9694\u7b26<code>|</code>\u5206\u9694\u3002\u5982\u679c\u4e3a<code>false</code>\uff0c\u4ec5\u663e\u793a\u5b9e\u9645\u77e9\u9635\u5143\u7d20\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>void</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u683c\u5f0f\u5316: \u5143\u7d20\u4ee5\u56fa\u5b9a\u5bbd\u5ea6\uff0812\u4e2a\u5b57\u7b26\uff09\u683c\u5f0f\u5316\u4ee5\u4fdd\u6301\u5bf9\u9f50\u3002</p> </li> <li> <p>\u586b\u5145\u53ef\u89c6\u5316: <code>show_padding</code>\u9009\u9879\u6709\u52a9\u4e8e\u7406\u89e3\u5185\u5b58\u5e03\u5c40\u548c\u9a8c\u8bc1\u586b\u5145\u503c\u3002</p> </li> <li> <p>\u5927\u77e9\u9635: \u5bf9\u4e8e\u975e\u5e38\u5927\u7684\u77e9\u9635\uff0c\u8003\u8651\u4f7f\u7528<code>view_roi()</code>\u6253\u5370\u7279\u5b9a\u533a\u57df\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_10","title":"\u6784\u9020\u4e0e\u6790\u6784\u51fd\u6570","text":"<p>\u5185\u5b58\u7ba1\u7406</p> <p>\u6784\u9020\u51fd\u6570\u81ea\u52a8\u5904\u7406\u5185\u5b58\u5206\u914d\u3002\u6790\u6784\u51fd\u6570\u4ec5\u5728\u5185\u5b58\u662f\u5185\u90e8\u5206\u914d\u7684\u60c5\u51b5\u4e0b\u5b89\u5168\u91ca\u653e\u5185\u5b58\uff08\u4e0d\u5305\u62ec\u5916\u90e8\u7f13\u51b2\u533a\u6216\u89c6\u56fe\uff09\u3002\u6784\u9020\u540e\u59cb\u7ec8\u68c0\u67e5<code>data</code>\u6307\u9488\u4ee5\u786e\u4fdd\u5206\u914d\u6210\u529f\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_11","title":"\u5185\u5b58\u5206\u914d","text":"<pre><code>void Mat::alloc_mem();\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6839\u636e\u8ba1\u7b97\u7684\u5185\u5b58\u9700\u6c42\u4e3a\u77e9\u9635\u5206\u914d\u5185\u5b58\u7684\u5185\u90e8\u51fd\u6570\u3002\u8bbe\u7f6e<code>ext_buff = false</code>\u5e76\u5206\u914d<code>row * stride</code>\u4e2afloat\u5143\u7d20\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>void</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u81ea\u52a8\u8c03\u7528: \u7531\u6784\u9020\u51fd\u6570\u81ea\u52a8\u8c03\u7528\u3002\u5f88\u5c11\u9700\u8981\u624b\u52a8\u8c03\u7528\u3002</p> </li> <li> <p>\u5185\u5b58\u8ba1\u7b97: \u5206\u914d<code>row * stride</code>\u4e2a\u5143\u7d20\uff0c\u53ef\u80fd\u5305\u62ec\u586b\u5145\u3002</p> </li> <li> <p>\u9519\u8bef\u5904\u7406: \u5982\u679c\u5206\u914d\u5931\u8d25\uff0c<code>data</code>\u4fdd\u6301\u4e3a<code>nullptr</code>\u3002\u6784\u9020\u540e\u59cb\u7ec8\u68c0\u67e5<code>data</code>\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_12","title":"\u9ed8\u8ba4\u6784\u9020\u51fd\u6570","text":"<pre><code>Mat::Mat();\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u9ed8\u8ba4\u6784\u9020\u51fd\u6570\u521b\u5efa\u4e00\u4e2a1\u00d71\u7684\u96f6\u77e9\u9635\u3002\u8fd9\u5bf9\u4e8e\u521d\u59cb\u5316\u548c\u4f5c\u4e3a\u9519\u8bef\u60c5\u51b5\u7684\u8fd4\u56de\u503c\u5f88\u6709\u7528\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5728\u67d0\u4e9b\u4e0a\u4e0b\u6587\u4e2d\u521b\u5efa\u77e9\u9635\u8fd0\u7b97\u7684\u6052\u7b49\u5143\u7d20\uff0c\u5c3d\u7ba1\u901a\u5e38\u60a8\u4f1a\u5e0c\u671b\u6307\u5b9a\u7ef4\u5ea6\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e00\u4e2a\u5143\u7d20\u4e3a0\u76841\u00d71\u77e9\u9635\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-matint-rows-int-cols","title":"\u6784\u9020\u51fd\u6570 - Mat(int rows, int cols)","text":"<pre><code>Mat::Mat(int rows, int cols);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6784\u9020\u51fd\u6570\u521b\u5efa\u5177\u6709\u6307\u5b9a\u7ef4\u5ea6\u7684\u77e9\u9635\u3002\u6240\u6709\u5143\u7d20\u521d\u59cb\u5316\u4e3a\u96f6\u3002\u8fd9\u662f\u6700\u5e38\u7528\u7684\u6784\u9020\u51fd\u6570\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int rows</code> : \u884c\u6570\uff08\u5fc5\u987b &gt; 0\uff09\u3002</p> </li> <li> <p><code>int cols</code> : \u5217\u6570\uff08\u5fc5\u987b &gt; 0\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e00\u4e2arows\u00d7cols\u7684\u77e9\u9635\uff0c\u6240\u6709\u5143\u7d20\u521d\u59cb\u5316\u4e3a0\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u96f6\u521d\u59cb\u5316: \u6240\u6709\u5143\u7d20\u4f7f\u7528<code>memset</code>\u8bbe\u7f6e\u4e3a\u96f6\uff0c\u786e\u4fdd\u5e72\u51c0\u7684\u72b6\u6001\u3002</p> </li> <li> <p>\u5185\u5b58\u5e03\u5c40: \u521b\u5efa\u8fde\u7eed\u7684\u5185\u5b58\u5e03\u5c40\uff0c\u65e0\u586b\u5145\uff08stride = cols\uff09\u3002</p> </li> <li> <p>\u9519\u8bef\u5904\u7406: \u5982\u679c\u5185\u5b58\u5206\u914d\u5931\u8d25\uff0c<code>data</code>\u5c06\u4e3a<code>nullptr</code>\u3002\u59cb\u7ec8\u9a8c\u8bc1\u5206\u914d\u6210\u529f\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-matint-rows-int-cols-int-stride","title":"\u6784\u9020\u51fd\u6570 - Mat(int rows, int cols, int stride)","text":"<pre><code>Mat::Mat(int rows, int cols, int stride);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6784\u9020\u51fd\u6570\u521b\u5efa\u5177\u6709\u6307\u5b9a\u7ef4\u5ea6\u3001\u5217\u6570\u548c\u6b65\u5e45\u7684\u77e9\u9635\u3002\u5f53\u9700\u8981\u586b\u5145\u4ee5\u8fdb\u884c\u5185\u5b58\u5bf9\u9f50\u6216DSP\u4f18\u5316\u65f6\u5f88\u6709\u7528\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int rows</code> : \u884c\u6570\u3002</p> </li> <li> <p><code>int cols</code> : \u5217\u6570\u3002</p> </li> <li> <p><code>int stride</code> : \u6b65\u5e45\uff08\u5fc5\u987b \u2265 cols\uff09\u3002\u586b\u5145 = stride - cols\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e00\u4e2a\u5177\u6709\u6b65\u5e45\u7684rows\u00d7cols\u77e9\u9635\uff0c\u6240\u6709\u5143\u7d20\u521d\u59cb\u5316\u4e3a0\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>DSP\u4f18\u5316: \u67d0\u4e9bDSP\u5e93\u9700\u8981\u5bf9\u9f50\u7684\u5185\u5b58\u3002\u4f7f\u7528\u6b65\u5e45\u786e\u4fdd\u9002\u5f53\u7684\u5bf9\u9f50\u3002</p> </li> <li> <p>\u5185\u5b58\u6548\u7387: \u586b\u5145\u5141\u8bb8\u5728\u5bf9\u9f50\u8fb9\u754c\u4e0a\u8fdb\u884c\u9ad8\u6548\u7684\u5411\u91cf\u5316\u64cd\u4f5c\u3002</p> </li> <li> <p>\u517c\u5bb9\u6027: \u5b9e\u73b0\u4e0e\u4f7f\u7528\u6b65\u5e45\u5185\u5b58\u5e03\u5c40\u7684\u5916\u90e8\u5e93\u7684\u517c\u5bb9\u6027\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-matfloat-data-int-rows-int-cols","title":"\u6784\u9020\u51fd\u6570 - Mat(float *data, int rows, int cols)","text":"<pre><code>Mat::Mat(float *data, int rows, int cols);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6784\u9020\u51fd\u6570\u5728\u5916\u90e8\u6570\u636e\u7f13\u51b2\u533a\u4e0a\u521b\u5efa\u77e9\u9635\u89c6\u56fe\u3002\u77e9\u9635\u4e0d\u62e5\u6709\u5185\u5b58\uff1b\u8c03\u7528\u8005\u8d1f\u8d23\u7ba1\u7406\u5b83\u3002\u5bf9\u4e8e\u4e0e\u73b0\u6709\u6570\u636e\u6570\u7ec4\u63a5\u53e3\u5f88\u6709\u7528\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>float *data</code> : \u6307\u5411\u5916\u90e8\u6570\u636e\u7f13\u51b2\u533a\u7684\u6307\u9488\uff08\u5728\u77e9\u9635\u751f\u547d\u5468\u671f\u5185\u5fc5\u987b\u4fdd\u6301\u6709\u6548\uff09\u3002</p> </li> <li> <p><code>int rows</code> : \u884c\u6570\u3002</p> </li> <li> <p><code>int cols</code> : \u5217\u6570\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e00\u4e2a<code>ext_buff = true</code>\u7684\u77e9\u9635\u89c6\u56fe\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u96f6\u62f7\u8d1d: \u4e0d\u53d1\u751f\u5185\u5b58\u62f7\u8d1d\uff1b\u77e9\u9635\u76f4\u63a5\u5f15\u7528\u5916\u90e8\u6570\u636e\u3002</p> </li> <li> <p>\u751f\u547d\u5468\u671f\u7ba1\u7406: \u5916\u90e8\u7f13\u51b2\u533a\u5728\u77e9\u9635\u5b58\u5728\u671f\u95f4\u5fc5\u987b\u4fdd\u6301\u6709\u6548\u3002\u6790\u6784\u51fd\u6570\u4e0d\u4f1a\u91ca\u653e\u6b64\u5185\u5b58\u3002</p> </li> <li> <p>\u6570\u636e\u5e03\u5c40: \u5047\u8bbe\u884c\u4e3b\u5e8f\u5e03\u5c40\uff0c\u65e0\u586b\u5145\uff08stride = cols\uff09\u3002</p> </li> <li> <p>\u4f7f\u7528\u573a\u666f: </p> </li> <li> <p>\u5305\u88c5C\u6570\u7ec4</p> </li> <li> <p>\u4e0e\u5176\u4ed6\u5e93\u63a5\u53e3</p> </li> <li> <p>\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u62f7\u8d1d</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-matfloat-data-int-rows-int-cols-int-stride","title":"\u6784\u9020\u51fd\u6570 - Mat(float *data, int rows, int cols, int stride)","text":"<pre><code>Mat::Mat(float *data, int rows, int cols, int stride);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6784\u9020\u51fd\u6570\u5728\u5177\u6709\u6307\u5b9a\u6b65\u5e45\u7684\u5916\u90e8\u6570\u636e\u7f13\u51b2\u533a\u4e0a\u521b\u5efa\u77e9\u9635\u89c6\u56fe\u3002\u652f\u6301\u6b65\u5e45\u5185\u5b58\u5e03\u5c40\u4ee5\u5b9e\u73b0DSP\u517c\u5bb9\u6027\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>float *data</code> : \u6307\u5411\u5916\u90e8\u6570\u636e\u7f13\u51b2\u533a\u7684\u6307\u9488\uff08\u5728\u77e9\u9635\u751f\u547d\u5468\u671f\u5185\u5fc5\u987b\u4fdd\u6301\u6709\u6548\uff09\u3002</p> </li> <li> <p><code>int rows</code> : \u884c\u6570\u3002</p> </li> <li> <p><code>int cols</code> : \u5217\u6570\u3002</p> </li> <li> <p><code>int stride</code> : \u6b65\u5e45\uff08\u5fc5\u987b \u2265 cols\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e00\u4e2a<code>ext_buff = true</code>\u5e76\u5177\u6709\u6307\u5b9a\u6b65\u5e45\u7684\u77e9\u9635\u89c6\u56fe\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6b65\u5e45\u5e03\u5c40: \u5bf9\u4e8e\u4f7f\u7528\u6b65\u5e45\u5185\u5b58\u5e03\u5c40\u7684DSP\u5e93\u81f3\u5173\u91cd\u8981\u3002</p> </li> <li> <p>\u5185\u5b58\u5b89\u5168: \u4e0e\u4e0a\u4e00\u4e2a\u6784\u9020\u51fd\u6570\u76f8\u540c\u7684\u751f\u547d\u5468\u671f\u8981\u6c42 - \u5916\u90e8\u7f13\u51b2\u533a\u5fc5\u987b\u4fdd\u6301\u6709\u6548\u3002</p> </li> <li> <p>\u586b\u5145\u652f\u6301: \u53ef\u4ee5\u5904\u7406\u884c\u95f4\u6709\u586b\u5145\u7684\u7f13\u51b2\u533a\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-matconst-mat-src","title":"\u62f7\u8d1d\u6784\u9020\u51fd\u6570 - Mat(const Mat &amp;src)","text":"<pre><code>Mat::Mat(const Mat &amp;src);\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u62f7\u8d1d\u6784\u9020\u51fd\u6570\u4ece\u6e90\u77e9\u9635\u521b\u5efa\u65b0\u77e9\u9635\u3002\u4f7f\u7528\u667a\u80fd\u62f7\u8d1d\u7b56\u7565\uff1a\u5e38\u89c4\u77e9\u9635\u8fdb\u884c\u6df1\u62f7\u8d1d\uff0c\u5b50\u77e9\u9635\u89c6\u56fe\u8fdb\u884c\u6d45\u62f7\u8d1d\u3002</p> <p>\u62f7\u8d1d\u7b56\u7565:</p> <ul> <li> <p>\u5e38\u89c4\u77e9\u9635: \u6df1\u62f7\u8d1d - \u5206\u914d\u65b0\u5185\u5b58\u5e76\u62f7\u8d1d\u6240\u6709\u6570\u636e</p> </li> <li> <p>\u5b50\u77e9\u9635\u89c6\u56fe: \u6d45\u62f7\u8d1d - \u4e0e\u6e90\u5171\u4eab\u6570\u636e\uff08\u521b\u5efa\u53e6\u4e00\u4e2a\u89c6\u56fe\uff09</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;src</code> : \u6e90\u77e9\u9635\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u6839\u636e\u6e90\u7c7b\u578b\u5177\u6709\u62f7\u8d1d\u6216\u5171\u4eab\u6570\u636e\u7684\u65b0\u77e9\u9635\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u81ea\u52a8\u9009\u62e9: \u6839\u636e\u6e90\u77e9\u9635\u7c7b\u578b\u81ea\u52a8\u9009\u62e9\u6df1\u62f7\u8d1d\u6216\u6d45\u62f7\u8d1d\u3002</p> </li> <li> <p>\u5185\u5b58\u6548\u7387: \u5b50\u77e9\u9635\u89c6\u56fe\u8fdb\u884c\u6d45\u62f7\u8d1d\u4ee5\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\u5206\u914d\u3002</p> </li> <li> <p>\u72ec\u7acb\u6027: \u6df1\u62f7\u8d1d\u662f\u72ec\u7acb\u7684\uff1b\u4fee\u6539\u4e0d\u4f1a\u5f71\u54cd\u6e90\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_13","title":"\u6790\u6784\u51fd\u6570","text":"<pre><code>Mat::~Mat();\n</code></pre> <p>\u63cf\u8ff0: </p> <p>\u6790\u6784\u51fd\u6570\u5b89\u5168\u5730\u91ca\u653e\u5206\u914d\u7684\u5185\u5b58\u3002\u4ec5\u91ca\u653e\u5185\u90e8\u5206\u914d\u7684\u5185\u5b58\uff08<code>ext_buff = false</code>\uff09\u3002\u5916\u90e8\u7f13\u51b2\u533a\u548c\u89c6\u56fe\u4e0d\u4f1a\u88ab\u91ca\u653e\u3002</p> <p>\u5185\u5b58\u7ba1\u7406:</p> <ul> <li> <p>\u5982\u679c<code>ext_buff = false</code>\uff0c\u91ca\u653e<code>data</code>\u7f13\u51b2\u533a</p> </li> <li> <p>\u5982\u679c\u5df2\u5206\u914d\uff0c\u91ca\u653e<code>temp</code>\u7f13\u51b2\u533a</p> </li> <li> <p>\u5bf9\u5916\u90e8\u7f13\u51b2\u533a\u6216\u89c6\u56fe\u4e0d\u6267\u884c\u4efb\u4f55\u64cd\u4f5c</p> </li> </ul> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>void</p> <p>\u6784\u9020\u51fd\u6570\u548c\u6790\u6784\u51fd\u6570\u89c4\u5219</p> <ul> <li>\u6784\u9020\u51fd\u6570\u51fd\u6570\u5fc5\u987b\u4e0e\u7c7b\u540d\u76f8\u540c\u4e14\u65e0\u8fd4\u56de\u7c7b\u578b</li> <li>C++\u5141\u8bb8\u901a\u8fc7\u66f4\u6539\u53c2\u6570\u6570\u91cf/\u987a\u5e8f\u8fdb\u884c\u51fd\u6570\u91cd\u8f7d</li> <li>\u5f53\u5bf9\u8c61\u8d85\u51fa\u4f5c\u7528\u57df\u65f6\uff0c\u6790\u6784\u51fd\u6570\u4f1a\u81ea\u52a8\u8c03\u7528</li> <li>\u6784\u9020\u540e\u59cb\u7ec8\u68c0\u67e5<code>data != nullptr</code>\u4ee5\u9a8c\u8bc1\u5206\u914d\u6210\u529f</li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_14","title":"\u5143\u7d20\u8bbf\u95ee","text":"<p>\u77e9\u9635\u7d22\u5f15</p> <p>Mat\u7c7b\u4f7f\u7528\u8fd0\u7b97\u7b26\u91cd\u8f7d\u63d0\u4f9b\u76f4\u89c2\u7684\u77e9\u9635\u5143\u7d20\u8bbf\u95ee\u3002<code>operator()</code>\u5141\u8bb8\u4f7f\u7528<code>A(i, j)</code>\u8fd9\u6837\u7684\u81ea\u7136\u8bed\u6cd5\uff0c\u800c\u4e0d\u662f<code>A.data[i * stride + j]</code>\u3002\u5b9e\u73b0\u81ea\u52a8\u5904\u7406\u6b65\u5e45\u548c\u586b\u5145\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_15","title":"\u8bbf\u95ee\u77e9\u9635\u5143\u7d20\uff08\u975e\u5e38\u91cf\uff09","text":"<pre><code>inline float &amp;operator()(int row, int col);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ee5\u8bfb\u5199\u65b9\u5f0f\u8bbf\u95ee\u77e9\u9635\u5143\u7d20\u3002\u8fd4\u56de\u5143\u7d20\u7684\u5f15\u7528\uff0c\u5141\u8bb8\u8bfb\u53d6\u548c\u4fee\u6539\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u4f4d\u7f6e(row, col)\u7684\u5143\u7d20\u8bbf\u95ee\u4e3a<code>data[row * stride + col]</code>\uff0c\u5176\u4e2d\u6b65\u5e45\u8003\u8651\u4e86\u586b\u5145\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>int row</code> : \u884c\u7d22\u5f15\uff08\u57fa\u4e8e0\uff0c\u5fc5\u987b\u5728\u8303\u56f4[0, row-1]\u5185\uff09\u3002</p> </li> <li> <p><code>int col</code> : \u5217\u7d22\u5f15\uff08\u57fa\u4e8e0\uff0c\u5fc5\u987b\u5728\u8303\u56f4[0, col-1]\u5185\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>float&amp;</code> - \u77e9\u9635\u5143\u7d20\u7684\u5f15\u7528\uff0c\u5141\u8bb8\u4fee\u6539\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u8fb9\u754c\u68c0\u67e5: \u4e3a\u4e86\u6027\u80fd\uff0c\u4e0d\u8fdb\u884c\u81ea\u52a8\u8fb9\u754c\u68c0\u67e5\u3002\u786e\u4fdd\u7d22\u5f15\u6709\u6548\u3002</p> </li> <li> <p>\u6b65\u5e45\u5904\u7406: \u81ea\u52a8\u8003\u8651\u6b65\u5e45\uff0c\u56e0\u6b64\u53ef\u4ee5\u6b63\u786e\u5904\u7406\u5e26\u586b\u5145\u7684\u77e9\u9635\u3002</p> </li> <li> <p>\u6027\u80fd: \u5185\u8054\u51fd\u6570\uff0c\u5f00\u9500\u6700\u5c0f\uff0c\u9002\u7528\u4e8e\u7d27\u5bc6\u5faa\u73af\u3002</p> </li> <li> <p>\u793a\u4f8b: <code>A(2, 3) = 5.0f;</code> \u5c06\u7b2c2\u884c\u7b2c3\u5217\u7684\u5143\u7d20\u8bbe\u7f6e\u4e3a5.0\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_16","title":"\u8bbf\u95ee\u77e9\u9635\u5143\u7d20\uff08\u5e38\u91cf\uff09","text":"<pre><code>inline const float &amp;operator()(int row, int col) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ee5\u53ea\u8bfb\u65b9\u5f0f\u8bbf\u95ee\u77e9\u9635\u5143\u7d20\u3002\u8fd4\u56de\u5e38\u91cf\u5f15\u7528\uff0c\u9632\u6b62\u4fee\u6539\u3002\u5f53\u77e9\u9635\u4e3aconst\u65f6\u4f7f\u7528\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li> <p><code>int row</code> : \u884c\u7d22\u5f15\uff08\u57fa\u4e8e0\uff09\u3002</p> </li> <li> <p><code>int col</code> : \u5217\u7d22\u5f15\uff08\u57fa\u4e8e0\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>const float&amp;</code> - \u77e9\u9635\u5143\u7d20\u7684\u5e38\u91cf\u5f15\u7528\uff08\u53ea\u8bfb\uff09\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5e38\u91cf\u6b63\u786e\u6027: \u5b9e\u73b0\u6b63\u786e\u7684const-correct\u4ee3\u7801\u3002\u5728const\u6210\u5458\u51fd\u6570\u4e2d\u4f7f\u7528\u6b64\u7248\u672c\u3002</p> </li> <li> <p>\u5b89\u5168\u6027: \u9632\u6b62\u610f\u5916\u4fee\u6539const\u77e9\u9635\u3002</p> </li> </ul> <p>\u8fd0\u7b97\u7b26\u91cd\u8f7d</p> <p>\u8fd9\u4e9b\u51fd\u6570\u91cd\u8f7d\u4e86<code>()</code>\u8fd0\u7b97\u7b26\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u7684\u77e9\u9635\u7d22\u5f15\u8bed\u6cd5\uff1a <pre><code>Mat A(3, 4);\nA(1, 2) = 3.14f;        // \u5199\u8bbf\u95ee\nfloat val = A(1, 2);    // \u8bfb\u8bbf\u95ee\nconst Mat&amp; B = A;\nfloat val2 = B(1, 2);   // \u53ea\u8bfb\u8bbf\u95ee\uff08\u4f7f\u7528const\u7248\u672c\uff09\n</code></pre></p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_17","title":"\u6570\u636e\u64cd\u4f5c","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_18","title":"\u590d\u5236\u5176\u4ed6\u77e9\u9635\u5230\u5f53\u524d\u77e9\u9635","text":"<pre><code>tiny_error_t copy_paste(const Mat &amp;src, int row_pos, int col_pos);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u6e90\u77e9\u9635\u7684\u5143\u7d20\u590d\u5236\u5230\u5f53\u524d\u77e9\u9635\u7684\u6307\u5b9a\u4f4d\u7f6e\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;src</code>: \u6e90\u77e9\u9635\u5bf9\u8c61</p> </li> <li> <p><code>int row_pos</code>: \u76ee\u6807\u77e9\u9635\u7684\u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int col_pos</code>: \u76ee\u6807\u77e9\u9635\u7684\u8d77\u59cb\u5217\u7d22\u5f15</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u9519\u8bef\u4ee3\u7801</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_19","title":"\u590d\u5236\u77e9\u9635\u5934\u90e8","text":"<pre><code>tiny_error_t copy_head(const Mat &amp;src);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u6e90\u77e9\u9635\u7684\u5934\u90e8\u4fe1\u606f\u590d\u5236\u5230\u5f53\u524d\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;src</code>: \u6e90\u77e9\u9635\u5bf9\u8c61</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u9519\u8bef\u4ee3\u7801</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_20","title":"\u83b7\u53d6\u5b50\u77e9\u9635\u89c6\u56fe","text":"<pre><code>Mat view_roi(int start_row, int start_col, int roi_rows, int roi_cols) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u83b7\u53d6\u5f53\u524d\u77e9\u9635\u7684\u5b50\u77e9\u9635\u89c6\u56fe\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int start_row</code>: \u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int start_col</code>: \u8d77\u59cb\u5217\u7d22\u5f15</p> </li> <li> <p><code>int roi_rows</code>: \u5b50\u77e9\u9635\u7684\u884c\u6570</p> </li> <li> <p><code>int roi_cols</code>: \u5b50\u77e9\u9635\u7684\u5217\u6570</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5b50\u77e9\u9635\u5bf9\u8c61</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-roi","title":"\u83b7\u53d6\u5b50\u77e9\u9635\u89c6\u56fe - \u4f7f\u7528 ROI \u7ed3\u6784","text":"<pre><code>Mat view_roi(const Mat::ROI &amp;roi) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u83b7\u53d6\u5f53\u524d\u77e9\u9635\u7684\u5b50\u77e9\u9635\u89c6\u56fe\uff0c\u4f7f\u7528 ROI \u7ed3\u6784\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat::ROI &amp;roi</code>: ROI \u7ed3\u6784\u5bf9\u8c61</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5b50\u77e9\u9635\u5bf9\u8c61</p> <p>\u8b66\u544a</p> <p>\u4e0e ESP-DSP \u4e0d\u540c\uff0cview_roi \u4e0d\u5141\u8bb8\u8bbe\u7f6e\u6b65\u957f\uff0c\u56e0\u4e3a\u5b83\u4f1a\u6839\u636e\u5217\u6570\u548c\u586b\u5145\u6570\u81ea\u52a8\u8ba1\u7b97\u6b65\u957f\u3002\u8be5\u51fd\u6570\u8fd8\u4f1a\u62d2\u7edd\u975e\u6cd5\u8bf7\u6c42\uff0c\u5373\u8d85\u51fa\u8303\u56f4\u7684\u8bf7\u6c42\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_21","title":"\u83b7\u53d6\u5b50\u77e9\u9635\u526f\u672c","text":"<pre><code>Mat copy_roi(int start_row, int start_col, int roi_rows, int roi_cols);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u83b7\u53d6\u5f53\u524d\u77e9\u9635\u7684\u5b50\u77e9\u9635\u526f\u672c\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int start_row</code>: \u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int start_col</code>: \u8d77\u59cb\u5217\u7d22\u5f15</p> </li> <li> <p><code>int roi_rows</code>: \u5b50\u77e9\u9635\u7684\u884c\u6570</p> </li> <li> <p><code>int roi_cols</code>: \u5b50\u77e9\u9635\u7684\u5217\u6570</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5b50\u77e9\u9635\u5bf9\u8c61</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-roi_1","title":"\u83b7\u53d6\u5b50\u77e9\u9635\u526f\u672c - \u4f7f\u7528 ROI \u7ed3\u6784","text":"<pre><code>Mat copy_roi(const Mat::ROI &amp;roi);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u83b7\u53d6\u5f53\u524d\u77e9\u9635\u7684\u5b50\u77e9\u9635\u526f\u672c\uff0c\u4f7f\u7528 ROI \u7ed3\u6784\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat::ROI &amp;roi</code>: ROI \u7ed3\u6784\u5bf9\u8c61</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5b50\u77e9\u9635\u5bf9\u8c61</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_22","title":"\u83b7\u53d6\u77e9\u9635\u5757","text":"<pre><code>Mat block(int start_row, int start_col, int block_rows, int block_cols);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u83b7\u53d6\u5f53\u524d\u77e9\u9635\u7684\u5757\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int start_row</code>: \u8d77\u59cb\u884c\u7d22\u5f15</p> </li> <li> <p><code>int start_col</code>: \u8d77\u59cb\u5217\u7d22\u5f15</p> </li> <li> <p><code>int block_rows</code>: \u5757\u7684\u884c\u6570</p> </li> <li> <p><code>int block_cols</code>: \u5757\u7684\u5217\u6570</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5757\u5bf9\u8c61</p> <p>view_roi | copy_roi | block \u4e4b\u95f4\u7684\u533a\u522b</p> <ul> <li> <p><code>view_roi</code> : \u4ece\u8be5\u77e9\u9635\u6d45\u62f7\u8d1d\u5b50\u77e9\u9635 (ROI)\u3002</p> </li> <li> <p><code>copy_roi</code> : \u4ece\u8be5\u77e9\u9635\u6df1\u62f7\u8d1d\u5b50\u77e9\u9635 (ROI)\u3002\u521a\u6027\u62f7\u8d1d\uff0c\u901f\u5ea6\u66f4\u5feb\u3002</p> </li> <li> <p><code>block</code> : \u4ece\u8be5\u77e9\u9635\u6df1\u62f7\u8d1d\u5757\u3002\u67d4\u6027\u62f7\u8d1d\uff0c\u901f\u5ea6\u66f4\u6162\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_23","title":"\u4ea4\u6362\u884c","text":"<pre><code>void Mat::swap_rows(int row1, int row2);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ea4\u6362\u5f53\u524d\u77e9\u9635\u7684\u4e24\u884c\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int row1</code>: \u7b2c\u4e00\u884c\u7d22\u5f15</p> </li> <li> <p><code>int row2</code>: \u7b2c\u4e8c\u884c\u7d22\u5f15</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>void</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_24","title":"\u4ea4\u6362\u5217","text":"<pre><code>void Mat::swap_cols(int col1, int col2);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ea4\u6362\u5f53\u524d\u77e9\u9635\u7684\u4e24\u5217\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int col1</code>: \u7b2c\u4e00\u5217\u7d22\u5f15</p> </li> <li> <p><code>int col2</code>: \u7b2c\u4e8c\u5217\u7d22\u5f15</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>void</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_25","title":"\u6e05\u9664\u77e9\u9635","text":"<pre><code>void Mat::clear(void);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u901a\u8fc7\u5c06\u6240\u6709\u5143\u7d20\u8bbe\u7f6e\u4e3a\u96f6\u6765\u6e05\u9664\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>void</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_26","title":"\u7b97\u672f\u8fd0\u7b97\u7b26","text":"<p>\u5c31\u5730\u64cd\u4f5c</p> <p>\u672c\u8282\u5b9a\u4e49\u4e86\u4f5c\u7528\u4e8e\u5f53\u524d\u77e9\u9635\u672c\u8eab\u7684\u7b97\u672f\u8fd0\u7b97\u7b26\uff08\u5c31\u5730\u64cd\u4f5c\uff09\u3002\u8fd9\u4e9b\u8fd0\u7b97\u7b26\u4fee\u6539\u77e9\u9635\u5e76\u8fd4\u56de\u5176\u5f15\u7528\uff0c\u652f\u6301\u94fe\u5f0f\u64cd\u4f5c\u5982<code>A += B += C</code>\u3002\u8fd9\u4e9b\u8fd0\u7b97\u7b26\u7ecf\u8fc7\u4f18\u5316\u4ee5\u5904\u7406\u586b\u5145\uff0c\u5e76\u5728\u53ef\u7528\u65f6\u4f7f\u7528DSP\u52a0\u901f\u51fd\u6570\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_27","title":"\u62f7\u8d1d\u8d4b\u503c","text":"<pre><code>Mat &amp;operator=(const Mat &amp;src);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u77e9\u9635\u7684\u62f7\u8d1d\u8d4b\u503c\u8fd0\u7b97\u7b26\u3002\u5c06\u6e90\u77e9\u9635\u7684\u5143\u7d20\u590d\u5236\u5230\u5f53\u524d\u77e9\u9635\u3002\u5fc5\u8981\u65f6\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u5185\u5b58\u6765\u5904\u7406\u7ef4\u5ea6\u53d8\u5316\u3002\u4e3a\u9632\u6b62\u610f\u5916\u6570\u636e\u635f\u574f\uff0c\u7981\u6b62\u5bf9\u5b50\u77e9\u9635\u89c6\u56fe\u8fdb\u884c\u8d4b\u503c\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u521b\u5efa\u6e90\u77e9\u9635\u7684\u72ec\u7acb\u526f\u672c\u3002\u4e0e\u62f7\u8d1d\u6784\u9020\u51fd\u6570\u4e0d\u540c\uff0c\u8fd9\u7528\u4e8e\u73b0\u6709\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;src</code> : \u6e90\u77e9\u9635\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5bf9\u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528\uff08\u652f\u6301\u94fe\u5f0f\u64cd\u4f5c\uff09\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5185\u5b58\u7ba1\u7406: \u5982\u679c\u7ef4\u5ea6\u4e0d\u540c\uff0c\u81ea\u52a8\u91cd\u65b0\u5206\u914d\u5185\u5b58\u3002\u5982\u679c\u5185\u5b58\u662f\u5185\u90e8\u5206\u914d\u7684\uff0c\u91ca\u653e\u65e7\u5185\u5b58\u3002</p> </li> <li> <p>\u5b50\u77e9\u9635\u4fdd\u62a4: \u7981\u6b62\u5bf9\u5b50\u77e9\u9635\u89c6\u56fe\u8fdb\u884c\u8d4b\u503c\uff0c\u4ee5\u9632\u6b62\u610f\u5916\u6570\u636e\u635f\u574f\u3002</p> </li> <li> <p>\u81ea\u8d4b\u503c: \u5b89\u5168\u5904\u7406\u81ea\u8d4b\u503c (A = A)\u3002</p> </li> <li> <p>\u6027\u80fd: \u5bf9\u4e8en\u00d7n\u77e9\u9635\u4e3aO(n\u00b2)\u3002\u5bf9\u4e8e\u5927\u77e9\u9635\uff0c\u8003\u8651\u89c6\u56fe\u662f\u5426\u8db3\u591f\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_28","title":"\u52a0\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat &amp;operator+=(const Mat &amp;A);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u52a0\u6cd5\u8fd0\u7b97\u7b26\uff0c\u5c06\u6e90\u77e9\u9635\u7684\u5143\u7d20\u52a0\u5230\u5f53\u524d\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;A</code>: \u6e90\u77e9\u9635\u5bf9\u8c61</li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-","title":"\u52a0\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat &amp;operator+=(float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u5e38\u91cf\u6309\u5143\u7d20\u52a0\u5230\u5f53\u524d\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float C</code>: \u8981\u52a0\u7684\u5e38\u91cf</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_29","title":"\u51cf\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat &amp;operator-=(const Mat &amp;A);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ece\u5f53\u524d\u77e9\u9635\u4e2d\u6309\u5143\u7d20\u51cf\u53bb\u6e90\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;A</code>: \u6e90\u77e9\u9635\u5bf9\u8c61</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_1","title":"\u51cf\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat &amp;operator-=(float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4ece\u5f53\u524d\u77e9\u9635\u4e2d\u6309\u5143\u7d20\u51cf\u53bb\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float C</code>: \u8981\u51cf\u7684\u5e38\u91cf</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_30","title":"\u77e9\u9635\u4e58\u6cd5","text":"<pre><code>Mat &amp;operator*=(const Mat &amp;A);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u77e9\u9635\u4e58\u6cd5\uff1athis = this * A\u3002\u6267\u884c\u6807\u51c6\u77e9\u9635\u4e58\u6cd5\uff08\u975e\u9010\u5143\u7d20\uff09\u3002\u5f53\u524d\u77e9\u9635\u7684\u5217\u6570\u5fc5\u987b\u7b49\u4e8eA\u7684\u884c\u6570\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u77e9\u9635\u4e58\u6cd5 C = A * B\uff0c\u5176\u4e2d C\u1d62\u2c7c = \u03a3\u2096 A\u1d62\u2096 * B\u2096\u2c7c\u3002\u8fd9\u662f\u6807\u51c6\u77e9\u9635\u4e58\u79ef\uff0c\u4e0d\u662f\u9010\u5143\u7d20\u4e58\u6cd5\u3002</p> <p>\u7ef4\u5ea6\u8981\u6c42:  - \u5f53\u524d\u77e9\u9635: m \u00d7 n</p> <ul> <li> <p>\u77e9\u9635 A: n \u00d7 p</p> </li> <li> <p>\u7ed3\u679c: m \u00d7 p</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;A</code> : \u8981\u4e58\u7684\u77e9\u9635\uff08\u5fc5\u987b\u6709n\u884c\uff0c\u5176\u4e2dn = \u5f53\u524d\u77e9\u9635\u7684\u5217\u6570\uff09\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5bf9\u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5185\u5b58\u6548\u7387: \u521b\u5efa\u4e34\u65f6\u526f\u672c\u4ee5\u907f\u514d\u5728\u8ba1\u7b97\u671f\u95f4\u8986\u76d6\u6570\u636e\uff0c\u7136\u540e\u66f4\u65b0\u5f53\u524d\u77e9\u9635\u3002</p> </li> <li> <p>\u586b\u5145\u652f\u6301: \u5728\u53ef\u7528\u65f6\u4f7f\u7528\u4e13\u7528DSP\u51fd\u6570\u5904\u7406\u5e26\u586b\u5145\u7684\u77e9\u9635\u3002</p> </li> <li> <p>\u6027\u80fd: \u5bf9\u4e8em\u00d7n * n\u00d7p\u4e58\u6cd5\u4e3aO(mnp)\u3002\u5728ESP32\u5e73\u53f0\u4e0a\u4f7f\u7528\u4f18\u5316\u7684DSP\u51fd\u6570\u3002</p> </li> <li> <p>\u5e38\u89c1\u9519\u8bef: \u8fd9\u662f\u77e9\u9635\u4e58\u6cd5\uff0c\u4e0d\u662f\u9010\u5143\u7d20\u7684\u3002\u5bf9\u4e8e\u9010\u5143\u7d20\uff0c\u4f7f\u7528\u5e26\u6709<code>operator()()</code>\u7684\u5faa\u73af\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_2","title":"\u4e58\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat &amp;operator*=(float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u4e58\u4ee5\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float C</code>: \u5e38\u91cf\u4e58\u6570</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_31","title":"\u9664\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat &amp;operator/=(const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u9664\u6cd5\uff1athis = this / B</p> <p>\u53c2\u6570:</p> <ul> <li><code>const Mat &amp;B</code>: \u9664\u6570\u77e9\u9635</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_3","title":"\u9664\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat &amp;operator/=(float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u5f53\u524d\u77e9\u9635\u6309\u5143\u7d20\u9664\u4ee5\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float C</code>: \u5e38\u91cf\u9664\u6570</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat&amp; - \u5f53\u524d\u77e9\u9635\u7684\u5f15\u7528</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_32","title":"\u5e42\u8fd0\u7b97\u7b26","text":"<pre><code>Mat operator^(int C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u6574\u6570\u5e42\u8fd0\u7b97\u3002\u8fd4\u56de\u4e00\u4e2a\u65b0\u77e9\u9635\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u90fd\u63d0\u5347\u5230\u7ed9\u5b9a\u5e42\u6b21\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>int C</code>: \u6307\u6570\uff08\u6574\u6570\uff09</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u5e42\u8fd0\u7b97\u540e\u7684\u65b0\u77e9\u9635</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_33","title":"\u7ebf\u6027\u4ee3\u6570","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_34","title":"\u8f6c\u7f6e\u77e9\u9635","text":"<pre><code>Mat Mat::transpose();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u77e9\u9635\u7684\u8f6c\u7f6e\uff0c\u8fd4\u56de\u65b0\u77e9\u9635\u3002\u77e9\u9635A\u7684\u8f6c\u7f6eA^T \u901a\u8fc7\u4ea4\u6362\u884c\u548c\u5217\u5f97\u5230\uff1a(A^T)\u1d62\u2c7c = A\u2c7c\u1d62\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <ul> <li> <p>\u5bf9\u4e8e\u4efb\u4f55\u77e9\u9635A\uff0c(A^T) ^T = A</p> </li> <li> <p>(A + B)^T = A^T + B^T</p> </li> <li> <p>(AB)^T = B^T * A^T</p> </li> <li> <p>\u5bf9\u4e8e\u65b9\u9635\uff0cdet(A) = det(A^T)</p> </li> </ul> <p>\u53c2\u6570:</p> <p>\u65e0\u3002</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u8f6c\u7f6e\u540e\u7684\u77e9\u9635 (col \u00d7 row)\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5185\u5b58\u5e03\u5c40: \u521b\u5efa\u65b0\u77e9\u9635\uff0c\u56e0\u6b64\u5185\u5b58\u4f7f\u7528\u91cf\u6682\u65f6\u7ffb\u500d\u3002\u5bf9\u4e8e\u5927\u77e9\u9635\uff0c\u8003\u8651\u5185\u5b58\u9650\u5236\u3002</p> </li> <li> <p>\u5bf9\u79f0\u77e9\u9635: \u5982\u679cA = A^T\uff0c\u77e9\u9635\u662f\u5bf9\u79f0\u7684\u3002\u4f7f\u7528<code>is_symmetric()</code>\u68c0\u67e5\u3002</p> </li> <li> <p>\u5e94\u7528: </p> </li> <li> <p>\u5185\u79ef: u^T * v</p> </li> <li> <p>\u4e8c\u6b21\u578b: x^T * A * x</p> </li> <li> <p>\u77e9\u9635\u65b9\u7a0b: A^T * A\uff08\u6b63\u89c4\u65b9\u7a0b\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_35","title":"\u4f59\u5b50\u5f0f\u77e9\u9635","text":"<pre><code>Mat Mat::minor(int row, int col);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u901a\u8fc7\u79fb\u9664\u6307\u5b9a\u7684\u884c\u548c\u5217\u6765\u8ba1\u7b97\u4f59\u5b50\u5f0f\u77e9\u9635\u3002\u4f59\u5b50\u5f0f\u662f\u79fb\u9664\u4e00\u884c\u4e00\u5217\u540e\u5f97\u5230\u7684\u5b50\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int row</code>: \u8981\u79fb\u9664\u7684\u884c\u7d22\u5f15</p> </li> <li> <p><code>int col</code>: \u8981\u79fb\u9664\u7684\u5217\u7d22\u5f15</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - (n-1)x(n-1) \u7684\u4f59\u5b50\u5f0f\u77e9\u9635</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_36","title":"\u4ee3\u6570\u4f59\u5b50\u5f0f\u77e9\u9635","text":"<pre><code>Mat Mat::cofactor(int row, int col);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u4ee3\u6570\u4f59\u5b50\u5f0f\u77e9\u9635\uff08\u4e0e\u4f59\u5b50\u5f0f\u77e9\u9635\u76f8\u540c\uff09\u3002\u4ee3\u6570\u4f59\u5b50\u5f0f\u77e9\u9635\u4e0e\u4f59\u5b50\u5f0f\u77e9\u9635\u76f8\u540c\u3002\u7b26\u53f7 (-1)^(i+j) \u5728\u8ba1\u7b97\u4ee3\u6570\u4f59\u5b50\u5f0f\u503c\u65f6\u5e94\u7528\uff0c\u800c\u4e0d\u662f\u5e94\u7528\u5230\u77e9\u9635\u5143\u7d20\u672c\u8eab\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int row</code>: \u8981\u79fb\u9664\u7684\u884c\u7d22\u5f15</p> </li> <li> <p><code>int col</code>: \u8981\u79fb\u9664\u7684\u5217\u7d22\u5f15</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - (n-1)x(n-1) \u7684\u4ee3\u6570\u4f59\u5b50\u5f0f\u77e9\u9635\uff08\u4e0e\u4f59\u5b50\u5f0f\u77e9\u9635\u76f8\u540c\uff09</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_37","title":"\u884c\u5217\u5f0f\uff08\u81ea\u52a8\u9009\u62e9\u65b9\u6cd5\uff09","text":"<pre><code>float Mat::determinant();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u65b9\u9635\u7684\u884c\u5217\u5f0f\uff0c\u6839\u636e\u77e9\u9635\u5927\u5c0f\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5c0f\u77e9\u9635\uff08n \u2264 4\uff09\uff0c\u4f7f\u7528\u62c9\u666e\u62c9\u65af\u5c55\u5f00\u6cd5\uff1b\u5bf9\u4e8e\u5927\u77e9\u9635\uff08n &gt; 4\uff09\uff0c\u4f7f\u7528LU\u5206\u89e3\u6cd5\u4ee5\u63d0\u9ad8\u6548\u7387\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <p>\u884c\u5217\u5f0f\u662f\u65b9\u9635\u7684\u4e00\u4e2a\u91cd\u8981\u6570\u503c\u7279\u5f81\uff0c\u5177\u6709\u4ee5\u4e0b\u6027\u8d28\uff1a</p> <ul> <li> <p>det(AB) = det(A) * det(B)</p> </li> <li> <p>det(A^T) = det(A)</p> </li> <li> <p>det(A^(-1)) = 1 / det(A)</p> </li> <li> <p>\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\uff0cdet(A) = 0</p> </li> </ul> <p>\u65b9\u6cd5\u9009\u62e9:</p> <ul> <li> <p>\u5c0f\u77e9\u9635 (n \u2264 4): \u4f7f\u7528 <code>determinant_laplace()</code> - \u62c9\u666e\u62c9\u65af\u5c55\u5f00\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6 O(n!)\uff0c\u5bf9\u5c0f\u77e9\u9635\u66f4\u51c6\u786e</p> </li> <li> <p>\u5927\u77e9\u9635 (n &gt; 4): \u4f7f\u7528 <code>determinant_lu()</code> - LU\u5206\u89e3\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6 O(n\u00b3)\uff0c\u6548\u7387\u66f4\u9ad8</p> </li> </ul> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>float - \u884c\u5217\u5f0f\u7684\u503c</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u81ea\u52a8\u9009\u62e9: \u5bf9\u4e8e\u5927\u591a\u6570\u5e94\u7528\uff0c\u76f4\u63a5\u4f7f\u7528 <code>determinant()</code> \u5373\u53ef\uff0c\u51fd\u6570\u4f1a\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u65b9\u6cd5</p> </li> <li> <p>\u6027\u80fd\u4f18\u5316: \u5982\u679c\u9700\u8981\u591a\u6b21\u8ba1\u7b97\u76f8\u540c\u5927\u5c0f\u7684\u77e9\u9635\u884c\u5217\u5f0f\uff0c\u53ef\u4ee5\u8003\u8651\u76f4\u63a5\u8c03\u7528 <code>determinant_lu()</code> \u6216 <code>determinant_gaussian()</code></p> </li> <li> <p>\u7cbe\u5ea6\u8981\u6c42: \u5bf9\u4e8e\u5c0f\u77e9\u9635\uff0c<code>determinant_laplace()</code> \u53ef\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6570\u503c\u7cbe\u5ea6</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_4","title":"\u884c\u5217\u5f0f - \u62c9\u666e\u62c9\u65af\u5c55\u5f00\u6cd5","text":"<pre><code>float Mat::determinant_laplace();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u62c9\u666e\u62c9\u65af\u5c55\u5f00\uff08\u4f59\u5b50\u5f0f\u5c55\u5f00\uff09\u8ba1\u7b97\u65b9\u9635\u7684\u884c\u5217\u5f0f\u3002\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n!)\uff0c\u4ec5\u9002\u7528\u4e8e\u5c0f\u77e9\u9635\uff08n \u2264 4\uff09\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <p>\u62c9\u666e\u62c9\u65af\u5c55\u5f00\u662f\u884c\u5217\u5f0f\u7684\u9012\u5f52\u5b9a\u4e49\u65b9\u6cd5\uff1a</p> <ul> <li> <p>\u5bf9\u4e8e 1\u00d71 \u77e9\u9635: det([a]) = a</p> </li> <li> <p>\u5bf9\u4e8e 2\u00d72 \u77e9\u9635: det([[a,b],[c,d]]) = ad - bc</p> </li> <li> <p>\u5bf9\u4e8e n\u00d7n \u77e9\u9635: det(A) = \u03a3\u2c7c\u208c\u2081\u207f (-1)\u2071\u207a\u02b2 a\u1d62\u2c7c * det(M\u1d62\u2c7c)\uff0c\u5176\u4e2d M\u1d62\u2c7c \u662f\u4f59\u5b50\u5f0f\u77e9\u9635</p> </li> </ul> <p>\u672c\u5b9e\u73b0\u4f7f\u7528\u7b2c\u4e00\u884c\u5c55\u5f00\uff0c\u9012\u5f52\u8ba1\u7b97\u4f59\u5b50\u5f0f\u7684\u884c\u5217\u5f0f\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>float - \u884c\u5217\u5f0f\u7684\u503c</p> <p>\u6027\u80fd\u8b66\u544a</p> <p>\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n!)\uff0c\u4ec5\u9002\u7528\u4e8e\u5c0f\u77e9\u9635\uff08n \u2264 4\uff09\u3002\u5bf9\u4e8e\u5927\u77e9\u9635\uff0c\u8bf7\u4f7f\u7528 <code>determinant_lu()</code> \u6216 <code>determinant_gaussian()</code>\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-lu","title":"\u884c\u5217\u5f0f - LU\u5206\u89e3\u6cd5","text":"<pre><code>float Mat::determinant_lu();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528LU\u5206\u89e3\u8ba1\u7b97\u65b9\u9635\u7684\u884c\u5217\u5f0f\u3002\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n\u00b3)\uff0c\u9002\u7528\u4e8e\u5927\u77e9\u9635\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <p>LU\u5206\u89e3\u5c06\u77e9\u9635\u5206\u89e3\u4e3a A = P * L * U\uff0c\u5176\u4e2d\uff1a</p> <ul> <li> <p>P \u662f\u7f6e\u6362\u77e9\u9635\uff08\u5982\u679c\u4f7f\u7528\u4e3b\u5143\uff09</p> </li> <li> <p>L \u662f\u5355\u4f4d\u5bf9\u89d2\u7ebf\u7684\u4e0b\u4e09\u89d2\u77e9\u9635</p> </li> <li> <p>U \u662f\u4e0a\u4e09\u89d2\u77e9\u9635</p> </li> </ul> <p>\u884c\u5217\u5f0f\u8ba1\u7b97\u516c\u5f0f\uff1adet(A) = det(P) * det(L) * det(U)</p> <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p>det(P) = (-1)^(\u7f6e\u6362\u7684\u7b26\u53f7)\uff0c\u7531\u884c\u4ea4\u6362\u6b21\u6570\u51b3\u5b9a</p> </li> <li> <p>det(L) = 1\uff08\u56e0\u4e3aL\u662f\u5355\u4f4d\u5bf9\u89d2\u7ebf\u7684\u4e0b\u4e09\u89d2\u77e9\u9635\uff09</p> </li> <li> <p>det(U) = \u220f\u1d62 U\u1d62\u1d62\uff08U\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u4e58\u79ef\uff09</p> </li> </ul> <p>\u7b97\u6cd5\u6b65\u9aa4:</p> <ol> <li>\u6267\u884cLU\u5206\u89e3\uff08\u5e26\u4e3b\u5143\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\uff09</li> <li>\u8ba1\u7b97\u7f6e\u6362\u77e9\u9635\u7684\u884c\u5217\u5f0f det(P)</li> <li>\u8ba1\u7b97\u4e0a\u4e09\u89d2\u77e9\u9635U\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e58\u79ef det(U)</li> <li>\u8fd4\u56de det(P) * det(U)</li> </ol> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>float - \u884c\u5217\u5f0f\u7684\u503c\u3002\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\u6216\u63a5\u8fd1\u5947\u5f02\u7684\uff0c\u8fd4\u56de 0.0</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6548\u7387: \u5bf9\u4e8e n &gt; 4 \u7684\u77e9\u9635\uff0c\u6bd4\u62c9\u666e\u62c9\u65af\u5c55\u5f00\u6cd5\u5feb\u5f97\u591a</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u4f7f\u7528\u4e3b\u5143\uff08pivoting\uff09\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027</p> </li> <li> <p>\u5947\u5f02\u77e9\u9635: \u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\uff0cLU\u5206\u89e3\u4f1a\u5931\u8d25\uff0c\u51fd\u6570\u8fd4\u56de 0.0</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_5","title":"\u884c\u5217\u5f0f - \u9ad8\u65af\u6d88\u5143\u6cd5","text":"<pre><code>float Mat::determinant_gaussian();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u8ba1\u7b97\u65b9\u9635\u7684\u884c\u5217\u5f0f\u3002\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n\u00b3)\uff0c\u9002\u7528\u4e8e\u5927\u77e9\u9635\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <p>\u9ad8\u65af\u6d88\u5143\u6cd5\u5c06\u77e9\u9635\u8f6c\u6362\u4e3a\u4e0a\u4e09\u89d2\u5f62\u5f0f\uff0c\u7136\u540e\u8ba1\u7b97\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u4e58\u79ef\u3002\u884c\u5217\u5f0f\u7684\u503c\u7b49\u4e8e\u4e0a\u4e09\u89d2\u77e9\u9635\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u4e58\u79ef\uff0c\u5e76\u6839\u636e\u884c\u4ea4\u6362\u6b21\u6570\u8c03\u6574\u7b26\u53f7\u3002</p> <p>\u7b97\u6cd5\u6b65\u9aa4:</p> <ol> <li>\u4f7f\u7528\u90e8\u5206\u4e3b\u5143\u6cd5\u8fdb\u884c\u9ad8\u65af\u6d88\u5143\uff0c\u5c06\u77e9\u9635\u8f6c\u6362\u4e3a\u4e0a\u4e09\u89d2\u5f62\u5f0f</li> <li>\u8ddf\u8e2a\u884c\u4ea4\u6362\u6b21\u6570</li> <li>\u8ba1\u7b97\u4e0a\u4e09\u89d2\u77e9\u9635\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u4e58\u79ef</li> <li>\u6839\u636e\u884c\u4ea4\u6362\u6b21\u6570\u8c03\u6574\u7b26\u53f7\uff1a\u6bcf\u6b21\u884c\u4ea4\u6362\u4f7f\u884c\u5217\u5f0f\u4e58\u4ee5 -1</li> </ol> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>float - \u884c\u5217\u5f0f\u7684\u503c\u3002\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\uff0c\u8fd4\u56de 0.0</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6548\u7387: \u5bf9\u4e8e\u5927\u77e9\u9635\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n\u00b3)\uff0c\u4e0eLU\u5206\u89e3\u6cd5\u76f8\u5f53</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u4f7f\u7528\u90e8\u5206\u4e3b\u5143\u6cd5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027</p> </li> <li> <p>\u5b9e\u73b0\u7b80\u5355: \u76f8\u6bd4LU\u5206\u89e3\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\uff0c\u4f46\u529f\u80fd\u8f83\u5c11\uff08\u4e0d\u80fd\u7528\u4e8e\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_38","title":"\u4f34\u968f\u77e9\u9635","text":"<pre><code>Mat Mat::adjoint();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u65b9\u9635\u7684\u4f34\u968f\uff08\u6216\u4f34\u968f\u8f6c\u7f6e\uff09\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4f34\u968f\u77e9\u9635\u5bf9\u8c61</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_39","title":"\u5f52\u4e00\u5316","text":"<pre><code>void Mat::normalize();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528L2\u8303\u6570\uff08Frobenius\u8303\u6570\uff09\u5f52\u4e00\u5316\u77e9\u9635\u3002\u5f52\u4e00\u5316\u540e\uff0c||Matrix|| = 1\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>void</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_40","title":"\u8303\u6570","text":"<pre><code>float Mat::norm() const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u77e9\u9635\u7684Frobenius\u8303\u6570\uff08L2\u8303\u6570\uff09\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>float - \u8ba1\u7b97\u5f97\u5230\u7684\u77e9\u9635\u8303\u6570</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_6","title":"\u77e9\u9635\u6c42\u9006 -- \u57fa\u4e8e\u4f34\u968f\u77e9\u9635","text":"<pre><code>Mat Mat::inverse_adjoint();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u4f34\u968f\u77e9\u9635\u6cd5\u8ba1\u7b97\u65b9\u9635\u7684\u9006\u77e9\u9635\u3002\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\uff0c\u8fd4\u56de\u96f6\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u9006\u77e9\u9635\u5bf9\u8c61\u3002\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\uff0c\u8fd4\u56de\u96f6\u77e9\u9635</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_41","title":"\u5355\u4f4d\u77e9\u9635","text":"<pre><code>static Mat Mat::eye(int size);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u751f\u6210\u6307\u5b9a\u5927\u5c0f\u7684\u5355\u4f4d\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>int size</code>: \u65b9\u9635\u7684\u7ef4\u5ea6</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u5355\u4f4d\u77e9\u9635 (size x size)</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_42","title":"\u589e\u5e7f\u77e9\u9635\uff08\u6c34\u5e73\u8fde\u63a5\uff09","text":"<pre><code>static Mat Mat::augment(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u901a\u8fc7\u6c34\u5e73\u8fde\u63a5\u4e24\u4e2a\u77e9\u9635\u521b\u5efa\u589e\u5e7f\u77e9\u9635 [A | B]\u3002A\u548cB\u7684\u884c\u6570\u5fc5\u987b\u5339\u914d\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u5de6\u4fa7\u77e9\u9635</p> </li> <li> <p><code>const Mat &amp;B</code>: \u53f3\u4fa7\u77e9\u9635</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u589e\u5e7f\u77e9\u9635 [A B]</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_43","title":"\u5782\u76f4\u5806\u53e0","text":"<pre><code>static Mat Mat::vstack(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5782\u76f4\u5806\u53e0\u4e24\u4e2a\u77e9\u9635 [A; B]\u3002A\u548cB\u7684\u5217\u6570\u5fc5\u987b\u5339\u914d\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u9876\u90e8\u77e9\u9635</p> </li> <li> <p><code>const Mat &amp;B</code>: \u5e95\u90e8\u77e9\u9635</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u5782\u76f4\u5806\u53e0\u7684\u77e9\u9635 [A; B]</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#gram-schmidt","title":"Gram-Schmidt\u6b63\u4ea4\u5316","text":"<pre><code>static bool Mat::gram_schmidt_orthogonalize(const Mat &amp;vectors, Mat &amp;orthogonal_vectors, \n                                            Mat &amp;coefficients, float tolerance = 1e-6f);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528Gram-Schmidt\u8fc7\u7a0b\u5bf9\u4e00\u7ec4\u5411\u91cf\u8fdb\u884c\u6b63\u4ea4\u5316\u3002\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6b63\u4ea4\u5316\u51fd\u6570\uff0c\u53ef\u91cd\u590d\u7528\u4e8eQR\u5206\u89e3\u548c\u5176\u4ed6\u9700\u8981\u6b63\u4ea4\u57fa\u7684\u5e94\u7528\u3002\u4f7f\u7528\u6539\u8fdb\u7684Gram-Schmidt\u7b97\u6cd5\u548c\u91cd\u65b0\u6b63\u4ea4\u5316\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u7ed9\u5b9a\u4e00\u7ec4\u5411\u91cf {v\u2081, v\u2082, ..., v\u2099}\uff0cGram-Schmidt\u8fc7\u7a0b\u4ea7\u751f\u6b63\u4ea4\u96c6\u5408 {q\u2081, q\u2082, ..., q\u2099}\uff0c\u5176\u4e2d\uff1a</p> <ul> <li> <p>q\u2081 = v\u2081 / ||v\u2081||</p> </li> <li> <p>q\u2c7c = (v\u2c7c - \u03a3\u1d62\u208c\u2081\u02b2\u207b\u00b9\u27e8v\u2c7c, q\u1d62\u27e9q\u1d62) / ||v\u2c7c - \u03a3\u1d62\u208c\u2081\u02b2\u207b\u00b9\u27e8v\u2c7c, q\u1d62\u27e9q\u1d62||</p> </li> </ul> <p>\u6539\u8fdb\u7248\u672c\u7acb\u5373\u51cf\u53bb\u6295\u5f71\uff0c\u8fd9\u63d0\u9ad8\u4e86\u6570\u503c\u7a33\u5b9a\u6027\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;vectors</code> : \u8f93\u5165\u77e9\u9635\uff0c\u5176\u4e2d\u6bcf\u5217\u662f\u8981\u6b63\u4ea4\u5316\u7684\u5411\u91cf (m \u00d7 n)\u3002</p> </li> <li> <p><code>Mat &amp;orthogonal_vectors</code> : \u8f93\u51fa\u77e9\u9635\uff0c\u7528\u4e8e\u6b63\u4ea4\u5316\u5411\u91cf (m \u00d7 n)\uff0c\u6bcf\u5217\u90fd\u662f\u6b63\u4ea4\u4e14\u5f52\u4e00\u5316\u7684\u3002</p> </li> <li> <p><code>Mat &amp;coefficients</code> : \u8f93\u51fa\u77e9\u9635\uff0c\u7528\u4e8e\u6295\u5f71\u7cfb\u6570 (n \u00d7 n\uff0c\u4e0a\u4e09\u89d2)\uff0c\u7c7b\u4f3c\u4e8eQR\u5206\u89e3\u4e2d\u7684R\u77e9\u9635\u3002</p> </li> <li> <p><code>float tolerance</code> : \u7ebf\u6027\u72ec\u7acb\u6027\u68c0\u67e5\u7684\u6700\u5c0f\u8303\u6570\u9608\u503c\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>bool</code> - \u6210\u529f\u8fd4\u56de<code>true</code>\uff0c\u8f93\u5165\u65e0\u6548\u8fd4\u56de<code>false</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u5b9e\u73b0\u4f7f\u7528\u6539\u8fdb\u7684Gram-Schmidt\u548c\u91cd\u65b0\u6b63\u4ea4\u5316\uff0c\u8fd9\u663e\u8457\u63d0\u9ad8\u4e86\u8fd1\u7ebf\u6027\u76f8\u5173\u5411\u91cf\u7684\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>QR\u5206\u89e3: \u6b64\u51fd\u6570\u7531<code>qr_decompose()</code>\u5185\u90e8\u4f7f\u7528\u3002\u5bf9\u4e8eQR\u5206\u89e3\uff0c\u7cfb\u6570\u77e9\u9635\u5bf9\u5e94\u4e8eR\u77e9\u9635\u3002</p> </li> <li> <p>\u57fa\u6784\u9020: \u7528\u4e8e\u4ece\u4e00\u7ec4\u5411\u91cf\u6784\u9020\u6b63\u4ea4\u57fa\uff0c\u8fd9\u5728\u8bb8\u591a\u7ebf\u6027\u4ee3\u6570\u5e94\u7528\u4e2d\u90fd\u662f\u57fa\u7840\u3002</p> </li> <li> <p>\u6027\u80fd: \u5bf9\u4e8e\u5927\u77e9\u9635\uff0c\u8003\u8651\u8ba1\u7b97\u6210\u672c\u3002\u5bf9\u4e8em\u7ef4\u5411\u91cf\u548cn\u4e2a\u5411\u91cf\uff0c\u590d\u6742\u5ea6\u4e3aO(mn\u00b2)\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#1","title":"\u51681\u77e9\u9635\uff08\u77e9\u5f62\uff09","text":"<pre><code>static Mat Mat::ones(int rows, int cols);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u521b\u5efa\u6307\u5b9a\u5927\u5c0f\u7684\u51681\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int rows</code>: \u884c\u6570</p> </li> <li> <p><code>int cols</code>: \u5217\u6570</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u77e9\u9635 [rows x cols]\uff0c\u6240\u6709\u5143\u7d20 = 1</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#1_1","title":"\u51681\u77e9\u9635\uff08\u65b9\u9635\uff09","text":"<pre><code>static Mat Mat::ones(int size);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u521b\u5efa\u6307\u5b9a\u5927\u5c0f\u7684\u65b9\u9635\uff0c\u6240\u6709\u5143\u7d20\u4e3a1\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>int size</code>: \u65b9\u9635\u7684\u5927\u5c0f\uff08\u884c\u6570 = \u5217\u6570\uff09</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u65b9\u9635 [size x size]\uff0c\u6240\u6709\u5143\u7d20 = 1</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_44","title":"\u9ad8\u65af\u6d88\u5143\u6cd5","text":"<pre><code>Mat Mat::gaussian_eliminate() const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6267\u884c\u9ad8\u65af\u6d88\u5143\u6cd5\uff0c\u5c06\u77e9\u9635\u8f6c\u6362\u4e3a\u884c\u9636\u68af\u5f62\u5f0f\uff08REF\uff09\u3002\u8fd9\u662f\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\u548c\u8ba1\u7b97\u77e9\u9635\u79e9\u7684\u7b2c\u4e00\u6b65\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u9ad8\u65af\u6d88\u5143\u901a\u8fc7\u521d\u7b49\u884c\u53d8\u6362\u5c06\u77e9\u9635\u8f6c\u6362\u4e3a\u884c\u9636\u68af\u5f62\u5f0f\uff1a</p> <ol> <li> <p>\u884c\u4ea4\u6362: \u4ea4\u6362\u4e24\u884c</p> </li> <li> <p>\u884c\u7f29\u653e: \u5c06\u4e00\u884c\u4e58\u4ee5\u975e\u96f6\u6807\u91cf</p> </li> <li> <p>\u884c\u52a0\u6cd5: \u5c06\u4e00\u884c\u7684\u500d\u6570\u52a0\u5230\u53e6\u4e00\u884c</p> </li> </ol> <p>\u884c\u9636\u68af\u5f62\u5f0f\uff08REF\uff09\u7684\u6027\u8d28:</p> <ul> <li> <p>\u6240\u6709\u96f6\u884c\u5728\u5e95\u90e8</p> </li> <li> <p>\u6bcf\u4e2a\u975e\u96f6\u884c\u7684\u524d\u5bfc\u7cfb\u6570\uff08\u4e3b\u5143\uff09\u4f4d\u4e8e\u4e0a\u4e00\u884c\u4e3b\u5143\u7684\u53f3\u4fa7</p> </li> <li> <p>\u4e3b\u5143\u4e0b\u65b9\u7684\u6240\u6709\u6761\u76ee\u4e3a\u96f6</p> </li> </ul> <p>\u53c2\u6570:</p> <p>\u65e0\u3002</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4e0a\u4e09\u89d2\u77e9\u9635\uff08REF\u5f62\u5f0f\uff09\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3: \u6c42\u89e3Ax = b\u7684\u7b2c\u4e00\u6b65\u3002REF\u4e4b\u540e\uff0c\u4f7f\u7528\u56de\u4ee3\u3002</p> </li> <li> <p>\u79e9\u8ba1\u7b97: \u79e9\u7b49\u4e8eREF\u4e2d\u975e\u96f6\u884c\u7684\u6570\u91cf\u3002</p> </li> <li> <p>\u884c\u5217\u5f0f: \u53ef\u4ee5\u4eceREF\u8ba1\u7b97\u884c\u5217\u5f0f\uff08\u5bf9\u89d2\u5143\u7d20\u7684\u4e58\u79ef\uff0c\u6839\u636e\u884c\u4ea4\u6362\u8fdb\u884c\u8c03\u6574\uff09\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u5b9e\u73b0\u4f7f\u7528\u90e8\u5206\u4e3b\u5143\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u6027\u80fd: \u5bf9\u4e8en\u00d7n\u77e9\u9635\u4e3aO(n\u00b3)\u3002\u5bf9\u4e8e\u591a\u4e2a\u7cfb\u7edf\uff0c\u4f18\u5148\u4f7f\u7528LU\u5206\u89e3\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_45","title":"\u4ece\u9ad8\u65af\u6d88\u5143\u5230\u884c\u6700\u7b80\u5f62\u5f0f","text":"<pre><code>Mat Mat::row_reduce_from_gaussian();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u77e9\u9635\uff08\u5047\u8bbe\u5df2\u4e3a\u884c\u9636\u68af\u5f62\u5f0f\uff09\u8f6c\u6362\u4e3a\u7b80\u5316\u884c\u9636\u68af\u5f62\u5f0f\uff08RREF\uff09\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - RREF\u5f62\u5f0f\u7684\u77e9\u9635</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_7","title":"\u9ad8\u65af-\u7ea6\u65e6\u6d88\u5143\u6cd5\u6c42\u9006","text":"<pre><code>Mat Mat::inverse_gje();\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u9ad8\u65af-\u7ea6\u65e6\u6d88\u5143\u6cd5\u8ba1\u7b97\u65b9\u9635\u7684\u9006\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <p>void</p> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u5982\u679c\u77e9\u9635\u53ef\u9006\u5219\u8fd4\u56de\u9006\u77e9\u9635\uff0c\u5426\u5219\u8fd4\u56de\u7a7a\u77e9\u9635</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_46","title":"\u70b9\u79ef","text":"<pre><code>float Mat::dotprod(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\uff08Nx1\uff09\u7684\u70b9\u79ef\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u5411\u91cf A (Nx1)</p> </li> <li> <p><code>const Mat &amp;B</code>: \u8f93\u5165\u5411\u91cf B (Nx1)</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>float - \u8ba1\u7b97\u5f97\u5230\u7684\u70b9\u79ef\u503c</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_47","title":"\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4","text":"<pre><code>Mat Mat::solve(const Mat &amp;A, const Mat &amp;b) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u548c\u56de\u4ee3\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edfAx = b\u3002\u8fd9\u662f\u9002\u7528\u4e8e\u826f\u6761\u4ef6\u7cfb\u7edf\u7684\u76f4\u63a5\u65b9\u6cd5\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a</p> <ol> <li> <p>\u524d\u5411\u6d88\u5143: \u5c06\u589e\u5e7f\u77e9\u9635[A|b]\u8f6c\u6362\u4e3a\u4e0a\u4e09\u89d2\u5f62\u5f0f</p> </li> <li> <p>\u56de\u4ee3: \u4ece\u4e0b\u5230\u4e0a\u6c42\u89e3Ux = y</p> </li> </ol> <p>\u7b97\u6cd5:</p> <ul> <li> <p>\u521b\u5efa\u589e\u5e7f\u77e9\u9635 [A | b]</p> </li> <li> <p>\u5e94\u7528\u9ad8\u65af\u6d88\u5143\u5f97\u5230 [U | y]\uff0c\u5176\u4e2dU\u662f\u4e0a\u4e09\u89d2\u77e9\u9635</p> </li> <li> <p>\u4f7f\u7528\u56de\u4ee3\u6c42\u89e3Ux = y\uff1ax\u1d62 = (y\u1d62 - \u03a3\u2c7c\u208c\u1d62\u208a\u2081\u207f U\u1d62\u2c7cx\u2c7c) / U\u1d62\u1d62</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code> : \u7cfb\u6570\u77e9\u9635 (N\u00d7N)\uff0c\u5fc5\u987b\u662f\u65b9\u9635\u4e14\u975e\u5947\u5f02\u3002</p> </li> <li> <p><code>const Mat &amp;b</code> : \u53f3\u7aef\u9879\u5411\u91cf (N\u00d71)\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u89e3\u5411\u91cf (N\u00d71)\uff0c\u5305\u542b\u65b9\u7a0bAx = b\u7684\u6839\u3002\u5982\u679c\u7cfb\u7edf\u662f\u5947\u5f02\u7684\u6216\u4e0d\u517c\u5bb9\u7684\uff0c\u8fd4\u56de\u7a7a\u77e9\u9635\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5355\u4e2a\u7cfb\u7edf: \u5bf9\u4e8e\u6c42\u89e3\u4e00\u4e2a\u7cfb\u7edf\u5f88\u9ad8\u6548\u3002\u5bf9\u4e8e\u5177\u6709\u76f8\u540cA\u7684\u591a\u4e2a\u7cfb\u7edf\uff0c\u4f7f\u7528LU\u5206\u89e3 + <code>solve_lu()</code>\u3002</p> </li> <li> <p>\u6761\u4ef6\u6570: \u5bf9\u4e8e\u75c5\u6001\u77e9\u9635\uff0c\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u5982\u679c\u7ed3\u679c\u4e0d\u51c6\u786e\uff0c\u68c0\u67e5\u6761\u4ef6\u6570\u3002</p> </li> <li> <p>\u5947\u5f02\u7cfb\u7edf: \u5982\u679cA\u662f\u5947\u5f02\u7684\uff08det(A) = 0\uff09\uff0c\u8fd4\u56de\u7a7a\u77e9\u9635\u3002\u5bf9\u4e8e\u79e9\u4e8f\u7cfb\u7edf\uff0c\u4f7f\u7528SVD + \u4f2a\u9006\u3002</p> </li> <li> <p>\u6027\u80fd: \u6d88\u5143\u4e3aO(n\u00b3)\uff0c\u56de\u4ee3\u4e3aO(n\u00b2)\u3002\u603b\u8ba1O(n\u00b3)\u3002</p> </li> <li> <p>\u66ff\u4ee3\u65b9\u6cd5:</p> </li> <li> <p>\u5bf9\u4e8eSPD\u77e9\u9635\uff1a\u4f7f\u7528Cholesky\u5206\u89e3 + <code>solve_cholesky()</code>\uff08\u66f4\u5feb\uff09</p> </li> <li> <p>\u5bf9\u4e8e\u591a\u4e2a\u53f3\u7aef\u9879\uff1a\u4f7f\u7528LU\u5206\u89e3 + <code>solve_lu()</code>\uff08\u66f4\u9ad8\u6548\uff09</p> </li> <li> <p>\u5bf9\u4e8e\u8d85\u5b9a\u7cfb\u7edf\uff1a\u4f7f\u7528QR\u5206\u89e3 + <code>solve_qr()</code>\uff08\u6700\u5c0f\u4e8c\u4e58\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_48","title":"\u5e26\u72b6\u77e9\u9635\u6c42\u89e3","text":"<pre><code>Mat Mat::band_solve(Mat A, Mat b, int k);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u4f18\u5316\u7684\u9ad8\u65af\u6d88\u5143\u6cd5\u6c42\u89e3\u5e26\u72b6\u77e9\u9635\u65b9\u7a0b\u7ec4 Ax = b\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>Mat A</code>: \u7cfb\u6570\u77e9\u9635 (NxN) - \u5e26\u72b6\u77e9\u9635</p> </li> <li> <p><code>Mat b</code>: \u7ed3\u679c\u5411\u91cf (Nx1)</p> </li> <li> <p><code>int k</code>: \u77e9\u9635\u7684\u5e26\u5bbd\uff08\u975e\u96f6\u5e26\u7684\u5bbd\u5ea6\uff09</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u89e3\u5411\u91cf (Nx1)\uff0c\u5305\u542b\u65b9\u7a0b Ax = b \u7684\u6839</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_49","title":"\u7ebf\u6027\u7cfb\u7edf\u6c42\u6839","text":"<pre><code>Mat Mat::roots(Mat A, Mat y);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u4e0d\u540c\u65b9\u6cd5\u6c42\u89e3\u77e9\u9635\u3002\u8fd9\u662f 'solve' \u51fd\u6570\u7684\u53e6\u4e00\u79cd\u5b9e\u73b0\uff0c\u539f\u7406\u4e0a\u6ca1\u6709\u533a\u522b\u3002\u6b64\u65b9\u6cd5\u4f7f\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf A * x = y\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>Mat A</code>: \u77e9\u9635 [N]x[N]\uff0c\u5305\u542b\u8f93\u5165\u7cfb\u6570</p> </li> <li> <p><code>Mat y</code>: \u5411\u91cf [N]x[1]\uff0c\u5305\u542b\u7ed3\u679c\u503c</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u77e9\u9635 [N]x[1]\uff0c\u5305\u542b\u6839</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_50","title":"\u77e9\u9635\u5c5e\u6027\u4e0e\u5206\u89e3","text":"<p>\u77e9\u9635\u5206\u89e3\u6982\u8ff0</p> <p>\u77e9\u9635\u5206\u89e3\u662f\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u4e2d\u7684\u57fa\u672c\u5de5\u5177\u3002\u5b83\u4eec\u5c06\u77e9\u9635\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u7ec4\u4ef6\uff0c\u63ed\u793a\u5176\u7ed3\u6784\u5e76\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002\u4e0d\u540c\u7684\u5206\u89e3\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u77e9\u9635\u548c\u5e94\u7528\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_51","title":"\u77e9\u9635\u5c5e\u6027\u68c0\u67e5","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_52","title":"\u68c0\u67e5\u5bf9\u79f0\u6027","text":"<pre><code>bool Mat::is_symmetric(float tolerance = 1e-6f) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u68c0\u67e5\u77e9\u9635\u5728\u7ed9\u5b9a\u5bb9\u5dee\u5185\u662f\u5426\u5bf9\u79f0\u3002\u77e9\u9635A\u662f\u5bf9\u79f0\u7684\uff0c\u5982\u679cA = A^T\uff0c\u5373\u5bf9\u4e8e\u6240\u6709i, j\uff0cA(i,j) = A(j,i)\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635\uff0c\u6240\u6709\u7279\u5f81\u503c\u90fd\u662f\u5b9e\u6570\uff0c\u5e76\u4e14\u53ef\u4ee5\u9009\u62e9\u7279\u5f81\u5411\u91cf\u4f7f\u5176\u6b63\u4ea4\u3002\u5bf9\u79f0\u77e9\u9635\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u90fd\u662f\u57fa\u7840\u7684\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u52a8\u529b\u5b66\u548c\u4f18\u5316\u4e2d\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float tolerance</code> : \u5141\u8bb8\u7684\u6700\u5927\u5dee\u503c |A(i,j) - A(j,i)|\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>bool</code> - \u5982\u679c\u8fd1\u4f3c\u5bf9\u79f0\u8fd4\u56de<code>true</code>\uff0c\u5426\u5219\u8fd4\u56de<code>false</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u7279\u5f81\u5206\u89e3: \u5bf9\u79f0\u77e9\u9635\u53ef\u4ee5\u4f7f\u7528\u66f4\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u7279\u5f81\u5206\u89e3\u65b9\u6cd5\uff08\u4f8b\u5982Jacobi\u65b9\u6cd5\uff09\u3002</p> </li> <li> <p>Cholesky\u5206\u89e3: \u53ea\u6709\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u53ef\u4ee5\u4f7f\u7528Cholesky\u5206\u89e3\u8fdb\u884c\u5206\u89e3\u3002</p> </li> <li> <p>\u7ed3\u6784\u52a8\u529b\u5b66: \u7ed3\u6784\u5206\u6790\u4e2d\u7684\u521a\u5ea6\u548c\u8d28\u91cf\u77e9\u9635\u901a\u5e38\u662f\u5bf9\u79f0\u7684\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_53","title":"\u68c0\u67e5\u6b63\u5b9a\u6027","text":"<pre><code>bool Mat::is_positive_definite(float tolerance = 1e-6f) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528Sylvester\u51c6\u5219\u68c0\u67e5\u77e9\u9635\u662f\u5426\u6b63\u5b9a\u3002\u5bf9\u79f0\u77e9\u9635A\u662f\u6b63\u5b9a\u7684\uff0c\u5982\u679c\u5bf9\u4e8e\u6240\u6709\u975e\u96f6\u5411\u91cfx\uff0cx^T A x &gt; 0\uff0c\u6216\u8005\u7b49\u4ef7\u5730\uff0c\u6240\u6709\u7279\u5f81\u503c\u90fd\u662f\u6b63\u7684\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>Sylvester\u51c6\u5219\u6307\u51fa\uff0c\u5bf9\u79f0\u77e9\u9635\u662f\u6b63\u5b9a\u7684\u5f53\u4e14\u4ec5\u5f53\u6240\u6709\u524d\u5bfc\u4e3b\u5b50\u5f0f\u90fd\u662f\u6b63\u7684\u3002\u4e3a\u4e86\u6548\u7387\uff0c\u51fd\u6570\u68c0\u67e5\u524d\u51e0\u4e2a\u524d\u5bfc\u4e3b\u5b50\u5f0f\u548c\u5bf9\u89d2\u5143\u7d20\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>float tolerance</code> : \u6570\u503c\u68c0\u67e5\u7684\u5bb9\u5dee\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>bool</code> - \u5982\u679c\u77e9\u9635\u662f\u6b63\u5b9a\u7684\u8fd4\u56de<code>true</code>\uff0c\u5426\u5219\u8fd4\u56de<code>false</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>Cholesky\u5206\u89e3: \u6b63\u5b9a\u77e9\u9635\u53ef\u4ee5\u4f7f\u7528Cholesky\u5206\u89e3\u8fdb\u884c\u5206\u89e3\uff0c\u8fd9\u6bd4LU\u5206\u89e3\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u3002</p> </li> <li> <p>\u4f18\u5316: \u6b63\u5b9aHessian\u77e9\u9635\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u793a\u5c40\u90e8\u6700\u5c0f\u503c\u3002</p> </li> <li> <p>\u7a33\u5b9a\u6027\u5206\u6790: \u5728\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u67d0\u4e9b\u77e9\u9635\u7684\u6b63\u5b9a\u6027\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_54","title":"\u77e9\u9635\u5206\u89e3\u7ed3\u6784","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#lu","title":"LU\u5206\u89e3\u7ed3\u6784","text":"<pre><code>struct Mat::LUDecomposition\n{\n    Mat L;                 // \u4e0b\u4e09\u89d2\u77e9\u9635\uff08\u5355\u4f4d\u5bf9\u89d2\u7ebf\uff09\n    Mat U;                 // \u4e0a\u4e09\u89d2\u77e9\u9635\n    Mat P;                 // \u7f6e\u6362\u77e9\u9635\uff08\u5982\u679c\u4f7f\u7528\u4e3b\u5143\uff09\n    bool pivoted;          // \u662f\u5426\u4f7f\u7528\u4e3b\u5143\n    tiny_error_t status;   // \u8ba1\u7b97\u72b6\u6001\n\n    LUDecomposition();\n};\n</code></pre> <p>\u63cf\u8ff0:</p> <p>LU\u5206\u89e3\u7ed3\u679c\u7684\u5bb9\u5668\u3002\u5206\u89e3A = P * L * U\uff08\u5e26\u4e3b\u5143\uff09\u6216A = L * U\uff08\u4e0d\u5e26\u4e3b\u5143\uff09\uff0c\u5176\u4e2dL\u662f\u5355\u4f4d\u5bf9\u89d2\u7ebf\u7684\u4e0b\u4e09\u89d2\u77e9\u9635\uff0cU\u662f\u4e0a\u4e09\u89d2\u77e9\u9635\uff0cP\u662f\u7f6e\u6362\u77e9\u9635\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>LU\u5206\u89e3\u5c06\u77e9\u9635\u5206\u89e3\u4e3a\u4e0b\u4e09\u89d2\u548c\u4e0a\u4e09\u89d2\u77e9\u9635\uff0c\u5b9e\u73b0\u7ebf\u6027\u7cfb\u7edf\u7684\u9ad8\u6548\u6c42\u89e3\u3002\u4f7f\u7528\u4e3b\u5143\u65f6\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u8fd1\u5947\u5f02\u77e9\u9635\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#cholesky","title":"Cholesky\u5206\u89e3\u7ed3\u6784","text":"<pre><code>struct Mat::CholeskyDecomposition\n{\n    Mat L;                 // \u4e0b\u4e09\u89d2\u77e9\u9635\n    tiny_error_t status;   // \u8ba1\u7b97\u72b6\u6001\n\n    CholeskyDecomposition();\n};\n</code></pre> <p>\u63cf\u8ff0:</p> <p>Cholesky\u5206\u89e3\u7ed3\u679c\u7684\u5bb9\u5668\u3002\u5bf9\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\uff0cA = L * L^T\uff0c\u5176\u4e2dL\u662f\u4e0b\u4e09\u89d2\u77e9\u9635\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>Cholesky\u5206\u89e3\u662f\u7528\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u4e13\u7528LU\u5206\u89e3\u3002\u5b83\u53ea\u9700\u8981LU\u5206\u89e3\u7684\u4e00\u534a\u5b58\u50a8\u548c\u8ba1\u7b97\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#qr","title":"QR\u5206\u89e3\u7ed3\u6784","text":"<pre><code>struct Mat::QRDecomposition\n{\n    Mat Q;                 // \u6b63\u4ea4\u77e9\u9635 (Q^T * Q = I)\n    Mat R;                 // \u4e0a\u4e09\u89d2\u77e9\u9635\n    tiny_error_t status;   // \u8ba1\u7b97\u72b6\u6001\n\n    QRDecomposition();\n};\n</code></pre> <p>\u63cf\u8ff0:</p> <p>QR\u5206\u89e3\u7ed3\u679c\u7684\u5bb9\u5668\u3002A = Q * R\uff0c\u5176\u4e2dQ\u662f\u6b63\u4ea4\u7684\uff08Q^T * Q = I\uff09\uff0cR\u662f\u4e0a\u4e09\u89d2\u77e9\u9635\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>QR\u5206\u89e3\u5c06\u77e9\u9635\u8868\u793a\u4e3a\u6b63\u4ea4\u77e9\u9635\u548c\u4e0a\u4e09\u89d2\u77e9\u9635\u7684\u4e58\u79ef\u3002\u5b83\u5728\u6570\u503c\u4e0a\u7a33\u5b9a\uff0c\u662f\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u7684\u57fa\u7840\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#svd","title":"SVD\u5206\u89e3\u7ed3\u6784","text":"<pre><code>struct Mat::SVDDecomposition\n{\n    Mat U;                 // \u5de6\u5947\u5f02\u5411\u91cf\uff08\u6b63\u4ea4\u77e9\u9635\uff09\n    Mat S;                 // \u5947\u5f02\u503c\uff08\u5bf9\u89d2\u77e9\u9635\u6216\u5411\u91cf\uff09\n    Mat V;                 // \u53f3\u5947\u5f02\u5411\u91cf\uff08\u6b63\u4ea4\u77e9\u9635\uff0cV^T\uff09\n    int rank;              // \u77e9\u9635\u7684\u6570\u503c\u79e9\n    int iterations;        // \u6267\u884c\u7684\u8fed\u4ee3\u6b21\u6570\n    tiny_error_t status;   // \u8ba1\u7b97\u72b6\u6001\n\n    SVDDecomposition();\n};\n</code></pre> <p>\u63cf\u8ff0:</p> <p>SVD\u5206\u89e3\u7ed3\u679c\u7684\u5bb9\u5668\u3002A = U * S * V^T\uff0c\u5176\u4e2dU\u548cV\u662f\u6b63\u4ea4\u77e9\u9635\uff0cS\u5728\u5bf9\u89d2\u7ebf\u4e0a\u5305\u542b\u5947\u5f02\u503c\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>SVD\u662f\u6700\u901a\u7528\u7684\u77e9\u9635\u5206\u89e3\u3002\u5947\u5f02\u503c\u63ed\u793a\u77e9\u9635\u7684\u79e9\u3001\u6761\u4ef6\u6570\uff0c\u5e76\u80fd\u591f\u8ba1\u7b97\u79e9\u4e8f\u77e9\u9635\u7684\u4f2a\u9006\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_55","title":"\u77e9\u9635\u5206\u89e3\u65b9\u6cd5","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#lu_1","title":"LU\u5206\u89e3","text":"<pre><code>Mat::LUDecomposition Mat::lu_decompose(bool use_pivoting = true) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97LU\u5206\u89e3\uff1aA = P * L * U\uff08\u5e26\u4e3b\u5143\uff09\u6216A = L * U\uff08\u4e0d\u5e26\u4e3b\u5143\uff09\u3002\u5bf9\u4e8e\u6c42\u89e3\u5177\u6709\u76f8\u540c\u7cfb\u6570\u77e9\u9635\u7684\u591a\u4e2a\u7cfb\u7edf\u5f88\u9ad8\u6548\u3002</p> <p>\u6570\u5b66\u539f\u7406: </p> <ul> <li> <p>\u4e0d\u5e26\u4e3b\u5143: A = L * U\uff0c\u5176\u4e2dL\u5177\u6709\u5355\u4f4d\u5bf9\u89d2\u7ebf</p> </li> <li> <p>\u5e26\u4e3b\u5143: P * A = L * U\uff0c\u5176\u4e2dP\u662f\u7f6e\u6362\u77e9\u9635</p> </li> </ul> <p>\u5206\u89e3\u901a\u8fc7\u6c42\u89e3Ly = Pb\uff08\u524d\u5411\u66ff\u6362\uff09\u7136\u540eUx = y\uff08\u540e\u5411\u66ff\u6362\uff09\u6765\u6c42\u89e3Ax = b\u3002</p> <p>\u53c2\u6570:</p> <ul> <li><code>bool use_pivoting</code> : \u662f\u5426\u4f7f\u7528\u90e8\u5206\u4e3b\u5143\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\uff08\u9ed8\u8ba4\uff1atrue\uff09\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>LUDecomposition</code>\uff0c\u5305\u542bL\u3001U\u3001P\u77e9\u9635\u548c\u72b6\u6001\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u591a\u4e2a\u53f3\u7aef\u9879: \u4e00\u65e6\u5206\u89e3\uff0c\u4f7f\u7528<code>solve_lu()</code>\u9ad8\u6548\u6c42\u89e3\u5177\u6709\u4e0d\u540c\u53f3\u7aef\u9879\u7684\u591a\u4e2a\u7cfb\u7edf\u3002</p> </li> <li> <p>\u884c\u5217\u5f0f: det(A) = det(P) * det(L) * det(U) = det(P) * det(U)\uff08\u56e0\u4e3adet(L) = 1\uff09\u3002</p> </li> <li> <p>\u9006\u77e9\u9635: \u53ef\u4ee5\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5355\u4f4d\u5411\u91cfe\u1d62\u6c42\u89e3LUx = e\u1d62\u6765\u8ba1\u7b97A^(-1)\u3002</p> </li> <li> <p>\u6027\u80fd: \u5206\u89e3\u4e3aO(n\u00b3)\uff0c\u5206\u89e3\u540e\u6bcf\u6b21\u6c42\u89e3\u4e3aO(n\u00b2)\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#cholesky_1","title":"Cholesky\u5206\u89e3","text":"<pre><code>Mat::CholeskyDecomposition Mat::cholesky_decompose() const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684Cholesky\u5206\u89e3\uff1aA = L * L^T\u3002\u5bf9\u4e8eSPD\u77e9\u9635\u6bd4LU\u66f4\u5feb\uff0c\u5e38\u7528\u4e8e\u7ed3\u6784\u52a8\u529b\u5b66\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5bf9\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635A\uff0c\u5b58\u5728\u552f\u4e00\u7684\u5177\u6709\u6b63\u5bf9\u89d2\u5143\u7d20\u7684\u4e0b\u4e09\u89d2\u77e9\u9635L\uff0c\u4f7f\u5f97A = L * L^T\u3002\u8fd9\u672c\u8d28\u4e0a\u662f\u5229\u7528\u5bf9\u79f0\u6027\u7684\u4e13\u7528LU\u5206\u89e3\u3002</p> <p>\u53c2\u6570:</p> <p>\u65e0\uff08\u77e9\u9635\u5fc5\u987b\u662f\u5bf9\u79f0\u6b63\u5b9a\u7684\uff09\u3002</p> <p>\u8fd4\u56de\u503c:</p> <p><code>CholeskyDecomposition</code>\uff0c\u5305\u542bL\u77e9\u9635\u548c\u72b6\u6001\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6548\u7387: \u9700\u8981\u5927\u7ea6LU\u5206\u89e3\u7684\u4e00\u534a\u8ba1\u7b97\u548c\u5b58\u50a8\u3002</p> </li> <li> <p>\u7a33\u5b9a\u6027: \u5bf9\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u6bd4LU\u66f4\u7a33\u5b9a\u3002</p> </li> <li> <p>\u5e94\u7528: </p> </li> <li> <p>\u7ed3\u6784\u52a8\u529b\u5b66\uff1a\u8d28\u91cf\u548c\u521a\u5ea6\u77e9\u9635\u901a\u5e38\u662fSPD</p> </li> <li> <p>\u4f18\u5316\uff1aNewton\u65b9\u6cd5\u4e2d\u7684Hessian\u77e9\u9635</p> </li> <li> <p>\u7edf\u8ba1\uff1a\u534f\u65b9\u5dee\u77e9\u9635</p> </li> <li> <p>\u9519\u8bef\u5904\u7406: \u5982\u679c\u77e9\u9635\u4e0d\u5bf9\u79f0\u6216\u4e0d\u662f\u6b63\u5b9a\u7684\uff0c\u8fd4\u56de\u9519\u8bef\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#qr_1","title":"QR\u5206\u89e3","text":"<pre><code>Mat::QRDecomposition Mat::qr_decompose() const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97QR\u5206\u89e3\uff1aA = Q * R\uff0c\u5176\u4e2dQ\u662f\u6b63\u4ea4\u7684\uff0cR\u662f\u4e0a\u4e09\u89d2\u7684\u3002\u6570\u503c\u7a33\u5b9a\uff0c\u7528\u4e8e\u6700\u5c0f\u4e8c\u4e58\u548c\u6b63\u4ea4\u5316\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>QR\u5206\u89e3\u5c06\u4efb\u4f55\u77e9\u9635\u8868\u793a\u4e3a\u6b63\u4ea4\u77e9\u9635Q\uff08Q^T * Q = I\uff09\u548c\u4e0a\u4e09\u89d2\u77e9\u9635R\u7684\u4e58\u79ef\u3002\u4f7f\u7528\u6539\u8fdb\u7684Gram-Schmidt\u8fc7\u7a0b\u548c\u91cd\u65b0\u6b63\u4ea4\u5316\u8ba1\u7b97\u5206\u89e3\u3002</p> <p>\u53c2\u6570:</p> <p>\u65e0\u3002</p> <p>\u8fd4\u56de\u503c:</p> <p><code>QRDecomposition</code>\uff0c\u5305\u542bQ\u548cR\u77e9\u9635\u548c\u72b6\u6001\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6700\u5c0f\u4e8c\u4e58: \u5bf9\u4e8e\u8d85\u5b9a\u7cfb\u7edfAx \u2248 b\uff0c\u6700\u5c0f\u5316||Ax - b||\u2082\u7684\u89e3\u662fx = R^(-1) * Q^T * b\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: QR\u5206\u89e3\u5bf9\u4e8e\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u6bd4\u6b63\u89c4\u65b9\u7a0b\u66f4\u7a33\u5b9a\u3002</p> </li> <li> <p>\u7279\u5f81\u5206\u89e3: QR\u7b97\u6cd5\u8fed\u4ee3\u4f7f\u7528QR\u5206\u89e3\u6765\u67e5\u627e\u7279\u5f81\u503c\u3002</p> </li> <li> <p>\u79e9\u63ed\u793a: A\u7684\u79e9\u7b49\u4e8eR\u7684\u975e\u96f6\u5bf9\u89d2\u5143\u7d20\u6570\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#svd_1","title":"SVD\u5206\u89e3","text":"<pre><code>Mat::SVDDecomposition Mat::svd_decompose(int max_iter = 100, float tolerance = 1e-6f) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u8ba1\u7b97\u5947\u5f02\u503c\u5206\u89e3\uff1aA = U * S * V^T\u3002\u6700\u901a\u7528\u7684\u5206\u89e3\uff0c\u7528\u4e8e\u79e9\u4f30\u8ba1\u3001\u4f2a\u9006\u3001\u964d\u7ef4\u3002\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u5206\u89e3\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>SVD\u5c06\u4efb\u4f55m \u00d7 n\u77e9\u9635A\u5206\u89e3\u4e3a\uff1a - U: m \u00d7 m\u6b63\u4ea4\u77e9\u9635\uff08\u5de6\u5947\u5f02\u5411\u91cf\uff09</p> <ul> <li> <p>S: m \u00d7 n\u5bf9\u89d2\u77e9\u9635\uff08\u5947\u5f02\u503c\u03c3\u2081 \u2265 \u03c3\u2082 \u2265 ... \u2265 \u03c3\u1d63 \u2265 0\uff09</p> </li> <li> <p>V: n \u00d7 n\u6b63\u4ea4\u77e9\u9635\uff08\u53f3\u5947\u5f02\u5411\u91cf\uff09</p> </li> </ul> <p>\u5947\u5f02\u503c\u63ed\u793a\u77e9\u9635\u7684\u57fa\u672c\u5c5e\u6027\uff1a\u79e9\u3001\u6761\u4ef6\u6570\u548c\u6570\u503c\u884c\u4e3a\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int max_iter</code> : \u6700\u5927\u8fed\u4ee3\u6b21\u6570\uff08\u9ed8\u8ba4\uff1a100\uff09\u3002</p> </li> <li> <p><code>float tolerance</code> : \u6536\u655b\u5bb9\u5dee\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>SVDDecomposition</code>\uff0c\u5305\u542bU\u3001S\u3001V\u77e9\u9635\u3001\u79e9\u548c\u72b6\u6001\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u79e9\u4f30\u8ba1: \u6570\u503c\u79e9\u662f\u9ad8\u4e8e\u5bb9\u5dee\u9608\u503c\u7684\u5947\u5f02\u503c\u6570\u91cf\u3002</p> </li> <li> <p>\u4f2a\u9006: A\u207a = V * S\u207a * U^T\uff0c\u5176\u4e2dS\u207a\u5bf9\u4e8e\u975e\u96f6\u03c3\u1d62\u67091/\u03c3\u1d62\u3002</p> </li> <li> <p>\u964d\u7ef4: \u622a\u65adSVD\uff08\u4ec5\u4fdd\u7559\u6700\u5927\u5947\u5f02\u503c\uff09\u63d0\u4f9b\u4f4e\u79e9\u8fd1\u4f3c\u3002</p> </li> <li> <p>\u6761\u4ef6\u6570: \u03ba(A) = \u03c3\u2081 / \u03c3\u1d63\uff0c\u5176\u4e2d\u03c3\u1d63\u662f\u6700\u5c0f\u7684\u975e\u96f6\u5947\u5f02\u503c\u3002</p> </li> <li> <p>\u5e94\u7528: </p> </li> <li> <p>\u79e9\u4e8f\u7cfb\u7edf\u7684\u6700\u5c0f\u4e8c\u4e58</p> </li> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09</p> </li> <li> <p>\u56fe\u50cf\u538b\u7f29</p> </li> <li> <p>\u964d\u566a</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_56","title":"\u4f7f\u7528\u5206\u89e3\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#lu_2","title":"\u4f7f\u7528LU\u5206\u89e3\u6c42\u89e3","text":"<pre><code>static Mat Mat::solve_lu(const LUDecomposition &amp;lu, const Mat &amp;b);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u9884\u8ba1\u7b97\u7684LU\u5206\u89e3\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edfAx = b\u3002\u5f53\u6c42\u89e3\u5177\u6709\u76f8\u540c\u7cfb\u6570\u77e9\u9635\u7684\u591a\u4e2a\u7cfb\u7edf\u65f6\uff0c\u6bd4<code>solve()</code>\u66f4\u9ad8\u6548\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u7ed9\u5b9aA = P * L * U\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6c42\u89e3Ax = b\uff1a 1. \u6c42\u89e3Ly = Pb\uff08\u524d\u5411\u66ff\u6362\uff09 2. \u6c42\u89e3Ux = y\uff08\u540e\u5411\u66ff\u6362\uff09</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const LUDecomposition &amp;lu</code> : \u9884\u8ba1\u7b97\u7684LU\u5206\u89e3\u3002</p> </li> <li> <p><code>const Mat &amp;b</code> : \u53f3\u7aef\u9879\u5411\u91cf (N\u00d71)\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u89e3\u5411\u91cf (N\u00d71)\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u591a\u4e2a\u53f3\u7aef\u9879: \u8ba1\u7b97\u4e00\u6b21LU\u5206\u89e3\u540e\uff0c\u9ad8\u6548\u6c42\u89e3\u591a\u4e2a\u7cfb\u7edf\u3002</p> </li> <li> <p>\u6027\u80fd: \u6bcf\u6b21\u6c42\u89e3O(n\u00b2) vs \u5b8c\u6574\u6c42\u89e3O(n\u00b3)\uff0c\u5bf9\u4e8e\u591a\u4e2a\u53f3\u7aef\u9879\u8282\u7701\u663e\u8457\u3002</p> </li> <li> <p>\u5185\u5b58: \u91cd\u7528\u5206\u89e3\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#cholesky_2","title":"\u4f7f\u7528Cholesky\u5206\u89e3\u6c42\u89e3","text":"<pre><code>static Mat Mat::solve_cholesky(const CholeskyDecomposition &amp;chol, const Mat &amp;b);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u9884\u8ba1\u7b97\u7684Cholesky\u5206\u89e3\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edfAx = b\u3002\u5bf9\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u6bd4LU\u66f4\u9ad8\u6548\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u7ed9\u5b9aA = L * L^T\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6c42\u89e3Ax = b\uff1a 1. \u6c42\u89e3Ly = b\uff08\u524d\u5411\u66ff\u6362\uff09 2. \u6c42\u89e3L^T x = y\uff08\u540e\u5411\u66ff\u6362\uff09</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const CholeskyDecomposition &amp;chol</code> : \u9884\u8ba1\u7b97\u7684Cholesky\u5206\u89e3\u3002</p> </li> <li> <p><code>const Mat &amp;b</code> : \u53f3\u7aef\u9879\u5411\u91cf (N\u00d71)\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u89e3\u5411\u91cf (N\u00d71)\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u6548\u7387: \u5bf9\u4e8eSPD\u77e9\u9635\uff0c\u5728\u5206\u89e3\u548c\u6c42\u89e3\u65b9\u9762\u90fd\u6bd4LU\u66f4\u5feb\u3002</p> </li> <li> <p>\u7a33\u5b9a\u6027: \u5bf9\u4e8eSPD\u77e9\u9635\u5728\u6570\u503c\u4e0a\u66f4\u7a33\u5b9a\u3002</p> </li> <li> <p>\u5e94\u7528: \u7ed3\u6784\u52a8\u529b\u5b66\u3001\u4f18\u5316\u3001\u7edf\u8ba1\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#qr_2","title":"\u4f7f\u7528QR\u5206\u89e3\u6c42\u89e3\uff08\u6700\u5c0f\u4e8c\u4e58\uff09","text":"<pre><code>static Mat Mat::solve_qr(const QRDecomposition &amp;qr, const Mat &amp;b);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528QR\u5206\u89e3\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\u3002\u4e3a\u8d85\u5b9a\u7cfb\u7edf\uff08\u65b9\u7a0b\u6570\u591a\u4e8e\u672a\u77e5\u6570\uff09\u63d0\u4f9b\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5bf9\u4e8eAx \u2248 b\uff08\u8d85\u5b9a\uff09\uff0c\u6700\u5c0f\u4e8c\u4e58\u89e3\u6700\u5c0f\u5316||Ax - b||\u2082\u3002\u4f7f\u7528A = Q * R\uff1a - x = R^(-1) * Q^T * b</p> <p>\u8fd9\u907f\u514d\u4e86\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u6b63\u89c4\u65b9\u7a0bA^T * A * x = A^T * b\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const QRDecomposition &amp;qr</code> : \u9884\u8ba1\u7b97\u7684QR\u5206\u89e3\u3002</p> </li> <li> <p><code>const Mat &amp;b</code> : \u53f3\u7aef\u9879\u5411\u91cf (M\u00d71\uff0c\u5176\u4e2dM \u2265 N)\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u6700\u5c0f\u4e8c\u4e58\u89e3\u5411\u91cf (N\u00d71)\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u8d85\u5b9a\u7cfb\u7edf: \u5904\u7406\u65b9\u7a0b\u6570\u591a\u4e8e\u672a\u77e5\u6570\u7684\u60c5\u51b5\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u6bd4\u76f4\u63a5\u6c42\u89e3\u6b63\u89c4\u65b9\u7a0b\u66f4\u7a33\u5b9a\u3002</p> </li> <li> <p>\u5e94\u7528: </p> </li> <li> <p>\u66f2\u7ebf\u62df\u5408</p> </li> <li> <p>\u6570\u636e\u56de\u5f52</p> </li> <li> <p>\u4fe1\u53f7\u5904\u7406</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_57","title":"\u4f2a\u9006","text":"<pre><code>static Mat Mat::pseudo_inverse(const SVDDecomposition &amp;svd, float tolerance = 1e-6f);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528SVD\u5206\u89e3\u8ba1\u7b97Moore-Penrose\u4f2a\u9006A\u207a\u3002\u9002\u7528\u4e8e\u79e9\u4e8f\u6216\u975e\u65b9\u77e9\u9635\uff0c\u5176\u4e2d\u5e38\u89c4\u9006\u4e0d\u5b58\u5728\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5bf9\u4e8eA = U * S * V^T\uff0c\u4f2a\u9006\u4e3aA\u207a = V * S\u207a * U^T\uff0c\u5176\u4e2dS\u207a\u5bf9\u4e8e\u5947\u5f02\u503c\u03c3\u1d62 &gt; tolerance\u67091/\u03c3\u1d62\uff0c\u5426\u5219\u4e3a0\u3002</p> <p>\u4f2a\u9006\u7684\u6027\u8d28:</p> <ul> <li> <p>A * A\u207a * A = A</p> </li> <li> <p>A\u207a * A * A\u207a = A\u207a</p> </li> <li> <p>(A * A\u207a)^T = A * A\u207a</p> </li> <li> <p>(A\u207a * A)^T = A\u207a * A</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const SVDDecomposition &amp;svd</code> : \u9884\u8ba1\u7b97\u7684SVD\u5206\u89e3\u3002</p> </li> <li> <p><code>float tolerance</code> : \u5947\u5f02\u503c\u9608\u503c\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002\u4f4e\u4e8e\u6b64\u503c\u7684\u5947\u5f02\u503c\u88ab\u89c6\u4e3a\u96f6\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u4f2a\u9006\u77e9\u9635\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u79e9\u4e8f\u7cfb\u7edf: \u4e3aA\u4e0d\u662f\u6ee1\u79e9\u7684\u7cfb\u7edf\u63d0\u4f9b\u89e3\u3002</p> </li> <li> <p>\u6700\u5c0f\u8303\u6570\u89e3: \u5bf9\u4e8e\u6b20\u5b9a\u7cfb\u7edf\uff0c\u7ed9\u51fa\u5177\u6709\u6700\u5c0f||x||\u2082\u7684\u89e3\u3002</p> </li> <li> <p>\u6700\u5c0f\u4e8c\u4e58: \u5bf9\u4e8e\u8d85\u5b9a\u7cfb\u7edf\uff0c\u7ed9\u51fa\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002</p> </li> <li> <p>\u5e94\u7528:</p> </li> <li> <p>\u63a7\u5236\u7cfb\u7edf</p> </li> <li> <p>\u4fe1\u53f7\u5904\u7406</p> </li> <li> <p>\u673a\u5668\u5b66\u4e60\uff08\u6b63\u5219\u5316\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_8","title":"\u7ebf\u6027\u4ee3\u6570 - \u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#mateigenpair","title":"\u7ed3\u6784\u4f53\uff1a<code>Mat::EigenPair</code>","text":"<pre><code>Mat::EigenPair::EigenPair();\n// fields:\n// float eigenvalue;      // \u7279\u5f81\u503c\uff08power_iteration \u4e3a\u6700\u5927\u6a21\uff0cinverse_power_iteration \u4e3a\u6700\u5c0f\u6a21\uff09\n// Mat eigenvector;       // \u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\uff08n x 1\uff09\n// int iterations;        // \u8fed\u4ee3\u6b21\u6570\uff08\u82e5\u4e3a\u8fed\u4ee3\u6cd5\u8fd4\u56de\uff09\n// tiny_error_t status;   // \u8ba1\u7b97\u72b6\u6001\uff08TINY_OK / \u9519\u8bef\u7801\uff09\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u7528\u4e8e\u4fdd\u5b58\u5355\u4e00\u7279\u5f81\u503c/\u7279\u5f81\u5411\u91cf\u5bf9\u53ca\u5176\u8ba1\u7b97\u4fe1\u606f\u3002\u5e38\u7531 <code>power_iteration</code> \u6216 <code>inverse_power_iteration</code> \u8fd4\u56de\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#mateigendecomposition","title":"\u7ed3\u6784\u4f53\uff1a<code>Mat::EigenDecomposition</code>","text":"<pre><code>Mat::EigenDecomposition::EigenDecomposition();\n// fields:\n// Mat eigenvalues;    // n x 1 \u77e9\u9635\uff0c\u5b58\u653e\u7279\u5f81\u503c\n// Mat eigenvectors;   // n x n \u77e9\u9635\uff0c\u901a\u5e38\u6bcf\u4e00\u5217\u4e3a\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\n// int iterations;     // \u8fed\u4ee3\u6b21\u6570\uff08\u82e5\u4e3a\u8fed\u4ee3\u6cd5\u8fd4\u56de\uff09\n// tiny_error_t status; // \u8ba1\u7b97\u72b6\u6001\uff08TINY_OK / \u9519\u8bef\u7801\uff09\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u7528\u4e8e\u4fdd\u5b58\u5b8c\u6574\u7684\u7279\u5f81\u503c\u5206\u89e3\u7ed3\u679c\uff0c\u5305\u62ec\u5168\u90e8\u7279\u5f81\u503c\u548c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\u77e9\u9635\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_58","title":"\u5e42\u8fed\u4ee3\uff08\u6c42\u4e3b\u7279\u5f81\u503c/\u5411\u91cf\uff09","text":"<pre><code>Mat::EigenPair Mat::power_iteration(int max_iter, float tolerance) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u5e42\u8fed\u4ee3\u6cd5\u8ba1\u7b97\u77e9\u9635\u7684\u4e3b\u7279\u5f81\u503c\uff08\u7edd\u5bf9\u503c\u6700\u5927\uff09\u53ca\u5bf9\u5e94\u7279\u5f81\u5411\u91cf\u3002\u5feb\u901f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6SHM\u5e94\u7528\uff0c\u53ef\u5feb\u901f\u8bc6\u522b\u4e3b\u9891\u7387\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u5e42\u8fed\u4ee3\u901a\u8fc7\u8fed\u4ee3\u5730\u5c06\u77e9\u9635\u5e94\u7528\u4e8e\u5411\u91cf\u6765\u627e\u5230\u7edd\u5bf9\u503c\u6700\u5927\u7684\u7279\u5f81\u503c\uff1a</p> <ol> <li> <p>\u4ece\u968f\u673a\u5411\u91cfv\u2080\u5f00\u59cb</p> </li> <li> <p>\u8fed\u4ee3\uff1av\u2096\u208a\u2081 = A * v\u2096 / ||A * v\u2096||</p> </li> <li> <p>\u7279\u5f81\u503c\u4f30\u8ba1\uff1a\u03bb\u2096 = (v\u2096^T * A * v\u2096) / (v\u2096^T * v\u2096) (Rayleigh\u5546)</p> </li> </ol> <p>\u6536\u655b\u6027:</p> <p>\u65b9\u6cd5\u6536\u655b\u5230\u4e3b\u7279\u5f81\u503c\uff0c\u5982\u679c\uff1a</p> <ul> <li> <p>\u4e3b\u7279\u5f81\u503c\u662f\u552f\u4e00\u7684 (|\u03bb\u2081| &gt; |\u03bb\u2082| \u2265 ... \u2265 |\u03bb\u2099|)</p> </li> <li> <p>\u521d\u59cb\u5411\u91cf\u5728\u4e3b\u7279\u5f81\u5411\u91cf\u65b9\u5411\u4e0a\u6709\u975e\u96f6\u5206\u91cf</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int max_iter</code> : \u6700\u5927\u8fed\u4ee3\u6b21\u6570\uff08\u5178\u578b\u9ed8\u8ba4\u503c\uff1a1000\uff09\u3002</p> </li> <li> <p><code>float tolerance</code> : \u6536\u655b\u5bb9\u5dee\uff08\u4f8b\u5982 1e-6\uff09\u3002\u6536\u655b\u6027\u901a\u8fc7 |\u03bb\u2096 - \u03bb\u2096\u208b\u2081| &lt; tolerance * |\u03bb\u2096| \u68c0\u67e5\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>EigenPair</code>\uff0c\u5305\u542b <code>eigenvalue</code>\u3001<code>eigenvector</code>\u3001<code>iterations</code> \u548c <code>status</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5b9e\u65f6\u5e94\u7528: \u5bf9\u4e8e\u5206\u79bb\u826f\u597d\u7684\u7279\u5f81\u503c\u5feb\u901f\u6536\u655b\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u3002</p> </li> <li> <p>\u521d\u59cb\u5316: \u5b9e\u73b0\u4f7f\u7528\u667a\u80fd\u521d\u59cb\u5316\u7b56\u7565\uff08\u5217\u7edd\u5bf9\u503c\u4e4b\u548c\uff09\u4ee5\u907f\u514d\u6536\u655b\u5230\u8f83\u5c0f\u7684\u7279\u5f81\u503c\u3002</p> </li> <li> <p>\u6536\u655b\u901f\u5ea6: \u6536\u655b\u662f\u7ebf\u6027\u7684\uff0c\u901f\u5ea6\u4e3a |\u03bb\u2082|/|\u03bb\u2081|\u3002\u5f53\u7279\u5f81\u503c\u63a5\u8fd1\u65f6\u8f83\u6162\u3002</p> </li> <li> <p>\u5c40\u9650\u6027: </p> </li> <li> <p>\u53ea\u627e\u5230\u4e00\u4e2a\u7279\u5f81\u503c-\u7279\u5f81\u5411\u91cf\u5bf9</p> </li> <li> <p>\u9700\u8981 |\u03bb\u2081| &gt; |\u03bb\u2082|\uff08\u4e3b\u7279\u5f81\u503c\u5fc5\u987b\u552f\u4e00\uff09</p> </li> <li> <p>\u5982\u679c\u7279\u5f81\u503c\u63a5\u8fd1\uff0c\u53ef\u80fd\u6536\u655b\u7f13\u6162</p> </li> <li> <p>\u5e94\u7528:</p> </li> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\uff08\u7b2c\u4e00\u4e3b\u6210\u5206\uff09</p> </li> <li> <p>PageRank\u7b97\u6cd5</p> </li> <li> <p>\u7ed3\u6784\u52a8\u529b\u5b66\uff08\u57fa\u9891\uff09</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_59","title":"\u53cd\u5e42\u8fed\u4ee3\uff08\u6c42\u6700\u5c0f\u7279\u5f81\u503c/\u5411\u91cf\uff09","text":"<pre><code>Mat::EigenPair Mat::inverse_power_iteration(int max_iter, float tolerance) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528\u53cd\u5e42\u8fed\u4ee3\u6cd5\u8ba1\u7b97\u77e9\u9635\u7684\u6700\u5c0f\uff08\u6700\u5c0f\u6a21\uff09\u7279\u5f81\u503c\u53ca\u5176\u5bf9\u5e94\u7279\u5f81\u5411\u91cf\u3002\u5bf9\u4e8e\u7cfb\u7edf\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u2014\u2014\u5728\u7ed3\u6784\u52a8\u529b\u5b66\u4e2d\u627e\u5230\u57fa\u9891/\u6700\u4f4e\u6a21\u6001\u3002\u8be5\u65b9\u6cd5\u5bf9\u4e8eSHM\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2d\u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u4e8e\u7cfb\u7edf\u7684\u57fa\u672c\u9891\u7387\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>\u53cd\u5e42\u8fed\u4ee3\u5c06\u5e42\u8fed\u4ee3\u5e94\u7528\u4e8eA^(-1)\uff0c \u5176\u7279\u5f81\u503c\u4e3a1/\u03bb\u1d62\u3002\u7531\u4e8e1/\u03bb\u2099\u662fA^(-1) \u7684\u6700\u5927\u7279\u5f81\u503c\uff0c\u8be5\u65b9\u6cd5\u6536\u655b\u5230A\u7684\u6700\u5c0f\u7279\u5f81\u503c\uff1a</p> <ol> <li> <p>\u4ece\u5411\u91cfv\u2080\u5f00\u59cb</p> </li> <li> <p>\u8fed\u4ee3\uff1a\u6c42\u89e3A * y\u2096 = v\u2096\uff0c\u7136\u540ev\u2096\u208a\u2081 = y\u2096 / ||y\u2096||</p> </li> <li> <p>\u7279\u5f81\u503c\u4f30\u8ba1\uff1a\u03bb\u2096 = (v\u2096^T * A * v\u2096) / (v\u2096^T * v\u2096) (Rayleigh\u5546)</p> </li> </ol> <p>\u6536\u655b\u6027:</p> <p>\u6536\u655b\u5230\u6700\u5c0f\u7279\u5f81\u503c\uff0c\u5982\u679c\uff1a</p> <ul> <li> <p>\u6700\u5c0f\u7279\u5f81\u503c\u662f\u552f\u4e00\u7684 (|\u03bb\u2099| &lt; |\u03bb\u2099\u208b\u2081| \u2264 ... \u2264 |\u03bb\u2081|)</p> </li> <li> <p>\u77e9\u9635A\u662f\u53ef\u9006\u7684\uff08\u975e\u5947\u5f02\uff09</p> </li> <li> <p>\u521d\u59cb\u5411\u91cf\u5728\u6700\u5c0f\u7279\u5f81\u5411\u91cf\u65b9\u5411\u4e0a\u6709\u5206\u91cf</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int max_iter</code> : \u6700\u5927\u8fed\u4ee3\u6b21\u6570\uff08\u9ed8\u8ba4\uff1a1000\uff09\u3002</p> </li> <li> <p><code>float tolerance</code> : \u6536\u655b\u5bb9\u5dee\uff08\u9ed8\u8ba4\uff1a1e-6\uff09\u3002\u4f7f\u7528\u76f8\u5bf9\u5bb9\u5dee\uff1a|\u03bb\u2096 - \u03bb\u2096\u208b\u2081| &lt; tolerance * max(|\u03bb\u2096|, 1.0)\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>EigenPair</code>\uff0c\u5305\u542b\u6700\u5c0f\u7279\u5f81\u503c\u3001\u7279\u5f81\u5411\u91cf\u3001\u8fed\u4ee3\u6b21\u6570\u548c\u72b6\u6001\u3002</p> <p>\u7b97\u6cd5\u6b65\u9aa4:</p> <ol> <li> <p>\u521d\u59cb\u5316\u5f52\u4e00\u5316\u7279\u5f81\u5411\u91cfv\uff08\u4f7f\u7528\u4ea4\u66ff\u7b26\u53f7\u4ee5\u907f\u514d\u4e0e\u4e3b\u7279\u5f81\u5411\u91cf\u5bf9\u9f50\uff09</p> </li> <li> <p>\u8fed\u4ee3\uff1a\u4f7f\u7528<code>solve()</code>\u6c42\u89e3A * y = v\uff08\u7b49\u4ef7\u4e8ey = A^(-1) * v\uff09</p> </li> <li> <p>\u5f52\u4e00\u5316y\u5f97\u5230\u65b0\u7684v</p> </li> <li> <p>\u4f7f\u7528Rayleigh\u5546\u8ba1\u7b97\u7279\u5f81\u503c\u4f30\u8ba1\uff1a\u03bb = (v^T * A * v) / (v^T * v)</p> </li> <li> <p>\u4f7f\u7528\u76f8\u5bf9\u5bb9\u5dee\u68c0\u67e5\u6536\u655b\u6027</p> </li> </ol> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u7cfb\u7edf\u8bc6\u522b: \u5bf9\u4e8e\u5728\u7ed3\u6784\u52a8\u529b\u5b66\u4e2d\u67e5\u627e\u57fa\u9891\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2d\u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u4e8e\u6700\u4f4e\u56fa\u6709\u9891\u7387\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u5b9e\u73b0\u5305\u62ec\u5bf9\u5947\u5f02\u77e9\u9635\u7684\u68c0\u67e5\uff0c\u5e76\u4f18\u96c5\u5730\u5904\u7406\u8fd1\u5947\u5f02\u60c5\u51b5\u3002</p> </li> <li> <p>\u521d\u59cb\u5316\u7b56\u7565: \u4f7f\u7528\u4ea4\u66ff\u7b26\u53f7\u6a21\u5f0f\u4ee5\u907f\u514d\u6536\u655b\u5230\u8f83\u5927\u7684\u7279\u5f81\u503c\uff0c\u786e\u4fdd\u6536\u655b\u5230\u6700\u5c0f\u7279\u5f81\u503c\u3002</p> </li> <li> <p>\u6027\u80fd: \u6bcf\u6b21\u8fed\u4ee3\u9700\u8981\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\uff08\u5bf9\u4e8e\u7a20\u5bc6\u77e9\u9635\u4e3aO(n\u00b3)\uff09\uff0c\u4f46\u901a\u5e38\u6bd4\u5e42\u8fed\u4ee3\u6536\u655b\u66f4\u5feb\u3002</p> </li> <li> <p>\u4e0e\u5e42\u8fed\u4ee3\u4e92\u8865: </p> </li> <li> <p>\u5e42\u8fed\u4ee3\uff1a\u627e\u5230\u03bb_max\uff08\u6700\u9ad8\u9891\u7387\uff09</p> </li> <li> <p>\u53cd\u5e42\u8fed\u4ee3\uff1a\u627e\u5230\u03bb_min\uff08\u57fa\u9891\uff09</p> </li> <li> <p>\u4e24\u8005\u4e00\u8d77\u63d0\u4f9b\u7cfb\u7edf\u7684\u9891\u7387\u8303\u56f4</p> </li> <li> <p>\u5e94\u7528:</p> </li> <li> <p>\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08\u57fa\u9891\u68c0\u6d4b\uff09</p> </li> <li> <p>\u6a21\u6001\u5206\u6790\uff08\u6700\u4f4e\u6a21\u6001\u5f62\u72b6\uff09</p> </li> <li> <p>\u7cfb\u7edf\u8bc6\u522b</p> </li> <li> <p>\u7a33\u5b9a\u6027\u5206\u6790\uff08\u6700\u5c0f\u7279\u5f81\u503c\u8868\u793a\u7a33\u5b9a\u6027\u88d5\u5ea6\uff09</p> </li> </ul> <p>\u6ce8\u610f:</p> <ul> <li> <p>\u8981\u6c42\u77e9\u9635\u4e3a\u65b9\u9635\u4e14\u6570\u636e\u6307\u9488\u975e\u7a7a\uff1b\u5426\u5219\u8fd4\u56de\u9519\u8bef\u72b6\u6001\u3002</p> </li> <li> <p>\u77e9\u9635\u5fc5\u987b\u662f\u53ef\u9006\u7684\uff08\u975e\u5947\u5f02\uff09\u624d\u80fd\u4f7f\u6b64\u65b9\u6cd5\u5de5\u4f5c\u3002\u5982\u679c\u77e9\u9635\u662f\u5947\u5f02\u7684\u6216\u63a5\u8fd1\u5947\u5f02\u7684\uff0c\u8be5\u65b9\u6cd5\u5c06\u4f18\u96c5\u5730\u5931\u8d25\u3002</p> </li> <li> <p>\u53cd\u5e42\u8fed\u4ee3\u53ea\u8fd4\u56de\u6700\u5c0f\u7279\u5f81\u503c/\u5411\u91cf\u5bf9\u3002\u82e5\u9700\u5168\u90e8\u7279\u5f81\u503c/\u5411\u91cf\u8bf7\u4f7f\u7528\u4e0b\u9762\u7684\u5206\u89e3\u51fd\u6570\u3002</p> </li> <li> <p>\u8be5\u65b9\u6cd5\u4e0e\u5e42\u8fed\u4ee3\u4e92\u8865\uff1a\u5e42\u8fed\u4ee3\u627e\u5230\u6700\u5927\u7279\u5f81\u503c\uff0c\u800c\u53cd\u5e42\u8fed\u4ee3\u627e\u5230\u6700\u5c0f\u7279\u5f81\u503c\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#jacobi","title":"Jacobi \u7279\u5f81\u5206\u89e3\uff08\u5bf9\u79f0\u77e9\u9635\uff09","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose_jacobi(float tolerance, int max_iter) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528Jacobi\u65b9\u6cd5\u8ba1\u7b97\u5b8c\u6574\u7684\u7279\u5f81\u503c\u5206\u89e3\u3002\u63a8\u8350\u7528\u4e8e\u5bf9\u79f0\u77e9\u9635\uff08\u826f\u597d\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u52a8\u529b\u5b66\u5e94\u7528\uff09\u3002\u7a33\u5065\u4e14\u51c6\u786e\uff0c\u662f\u7ed3\u6784\u52a8\u529b\u5b66\u77e9\u9635\u7684\u7406\u60f3\u9009\u62e9\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>Jacobi\u65b9\u6cd5\u901a\u8fc7\u4e00\u7cfb\u5217\u6b63\u4ea4\u76f8\u4f3c\u53d8\u6362\uff08Givens\u65cb\u8f6c\uff09\u5bf9\u89d2\u5316\u5bf9\u79f0\u77e9\u9635\uff1a 1. \u627e\u5230\u6700\u5927\u7684\u975e\u5bf9\u89d2\u5143\u7d20a\u209aq 2. \u8ba1\u7b97\u65cb\u8f6c\u89d2\u03b8\u4ee5\u5c06\u8be5\u5143\u7d20\u7f6e\u96f6 3. \u5e94\u7528\u65cb\u8f6c\uff1aA' = J^T * A * J\uff0c\u5176\u4e2dJ\u662f\u65cb\u8f6c\u77e9\u9635 4. \u91cd\u590d\u76f4\u5230\u6240\u6709\u975e\u5bf9\u89d2\u5143\u7d20\u4f4e\u4e8e\u5bb9\u5dee</p> <p>\u6536\u655b\u6027:</p> <p>\u5f53\u6700\u5927\u975e\u5bf9\u89d2\u5143\u7d20\u4f4e\u4e8e\u5bb9\u5dee\u65f6\u65b9\u6cd5\u6536\u655b\u3002\u6bcf\u6b21\u65cb\u8f6c\u5c06\u4e00\u4e2a\u975e\u5bf9\u89d2\u5143\u7d20\u7f6e\u96f6\uff0c\u8fc7\u7a0b\u6301\u7eed\u76f4\u5230\u77e9\u9635\u5bf9\u89d2\u5316\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>float tolerance</code> : \u6536\u655b\u9608\u503c\uff08\u4f8b\u5982 1e-6\uff09\u3002\u5141\u8bb8\u7684\u975e\u5bf9\u89d2\u5143\u7d20\u6700\u5927\u5e45\u5ea6\u3002</p> </li> <li> <p><code>int max_iter</code> : \u6700\u5927\u8fed\u4ee3\u6b21\u6570\uff08\u4f8b\u5982 100\uff09\u3002\u5bf9\u4e8en\u00d7n\u77e9\u9635\uff0c\u901a\u5e38\u9700\u8981O(n\u00b2)\u6b21\u8fed\u4ee3\u6536\u655b\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>EigenDecomposition</code>\uff0c\u5305\u542b <code>eigenvalues</code>\u3001<code>eigenvectors</code>\u3001<code>iterations</code> \u548c <code>status</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u5bf9\u79f0\u77e9\u9635: \u4e13\u4e3a\u5bf9\u79f0\u77e9\u9635\u8bbe\u8ba1\u3002\u5bf9\u4e8e\u975e\u5bf9\u79f0\u77e9\u9635\uff0c\u4f7f\u7528QR\u65b9\u6cd5\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635\u975e\u5e38\u7a33\u5b9a\uff0c\u5177\u6709\u826f\u597d\u7684\u6b63\u4ea4\u6027\u4fdd\u6301\u3002</p> </li> <li> <p>\u7cbe\u5ea6: \u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u7279\u5f81\u503c/\u7279\u5f81\u5411\u91cf\u5bf9\u7684\u5e94\u7528\u3002</p> </li> <li> <p>\u6027\u80fd: \u6bcf\u6b21\u8fed\u4ee3O(n\u00b3)\uff0c\u4f46\u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635\u901a\u5e38\u6bd4QR\u9700\u8981\u66f4\u5c11\u7684\u8fed\u4ee3\u3002</p> </li> <li> <p>\u5e94\u7528:</p> </li> <li> <p>\u7ed3\u6784\u52a8\u529b\u5b66\uff1a\u521a\u5ea6\u548c\u8d28\u91cf\u77e9\u9635\u662f\u5bf9\u79f0\u7684</p> </li> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09</p> </li> <li> <p>\u8c31\u805a\u7c7b</p> </li> <li> <p>\u4e8c\u6b21\u578b\u4f18\u5316</p> </li> </ul> <p>\u6ce8\u610f:</p> <p>\u5982\u679c\u77e9\u9635\u4e0d\u662f\u8fd1\u4f3c\u5bf9\u79f0\uff0c\u51fd\u6570\u4f1a\u53d1\u51fa\u8b66\u544a\uff0c\u4f46\u4ecd\u53ef\u80fd\u8fd0\u884c\u3002\u5bf9\u4e8e\u975e\u5bf9\u79f0\u77e9\u9635\uff0c\u63a8\u8350\u4f7f\u7528QR\u65b9\u6cd5\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#qr_3","title":"QR \u7279\u5f81\u5206\u89e3\uff08\u4e00\u822c\u77e9\u9635\uff09","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose_qr(int max_iter, float tolerance) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u4f7f\u7528QR\u7b97\u6cd5\u8ba1\u7b97\u7279\u5f81\u503c\u5206\u89e3\u3002\u9002\u7528\u4e8e\u4e00\u822c\uff08\u53ef\u80fd\u975e\u5bf9\u79f0\uff09\u77e9\u9635\u3002\u652f\u6301\u975e\u5bf9\u79f0\u77e9\u9635\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u590d\u6570\u7279\u5f81\u503c\uff08\u4ec5\u8fd4\u56de\u5b9e\u90e8\uff09\u3002</p> <p>\u6570\u5b66\u539f\u7406:</p> <p>QR\u7b97\u6cd5\u8fed\u4ee3\u5e94\u7528QR\u5206\u89e3\uff1a</p> <ol> <li> <p>\u4eceA\u2080 = A\u5f00\u59cb</p> </li> <li> <p>\u5bf9\u4e8ek = 0, 1, 2, ...\uff1a \u8ba1\u7b97QR\u5206\u89e3\uff1aA\u2096 = Q\u2096 * R\u2096\uff0c\u7136\u540e\u66f4\u65b0\uff1aA\u2096\u208a\u2081 = R\u2096 * Q\u2096</p> </li> <li> <p>A\u2096\u6536\u655b\u5230\u4e0a\u4e09\u89d2\u5f62\u5f0f\uff08Schur\u5f62\u5f0f\uff09\uff0c\u7279\u5f81\u503c\u5728\u5bf9\u89d2\u7ebf\u4e0a</p> </li> </ol> <p>\u6536\u655b\u6027:</p> <p>\u5f53A\u2096\u8fd1\u4f3c\u4e0a\u4e09\u89d2\uff08\u6b21\u5bf9\u89d2\u5143\u7d20 &lt; \u5bb9\u5dee\uff09\u65f6\u7b97\u6cd5\u6536\u655b\u3002\u7279\u5f81\u503c\u51fa\u73b0\u5728\u5bf9\u89d2\u7ebf\u4e0a\uff0c\u7279\u5f81\u5411\u91cf\u4eceQ\u77e9\u9635\u7d2f\u79ef\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>int max_iter</code> : \u6700\u5927QR\u8fed\u4ee3\u6b21\u6570\uff08\u9ed8\u8ba4\uff1a100\uff09\u3002</p> </li> <li> <p><code>float tolerance</code> : \u6536\u655b\u5bb9\u5dee\uff08\u4f8b\u5982 1e-6\uff09\u3002\u4f7f\u7528\u76f8\u5bf9\u5bb9\u5dee\uff0c\u6bd4\u8f83\u6b21\u5bf9\u89d2\u5143\u7d20\u4e0e\u5bf9\u89d2\u5143\u7d20\u3002</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>EigenDecomposition</code>\uff0c\u5305\u542b\u7279\u5f81\u503c\u3001\u7279\u5f81\u5411\u91cf\u3001\u8fed\u4ee3\u6b21\u6570\u548c\u72b6\u6001\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u4e00\u822c\u77e9\u9635: \u53ef\u4ee5\u5904\u7406\u975e\u5bf9\u79f0\u77e9\u9635\uff0c\u4e0eJacobi\u65b9\u6cd5\u4e0d\u540c\u3002</p> </li> <li> <p>\u590d\u6570\u7279\u5f81\u503c: \u975e\u5bf9\u79f0\u77e9\u9635\u53ef\u80fd\u5177\u6709\u590d\u6570\u7279\u5f81\u503c\uff1b\u5f53\u524d\u5b9e\u73b0\u4ec5\u8fd4\u56de\u5b9e\u90e8\u3002</p> </li> <li> <p>\u6570\u503c\u7a33\u5b9a\u6027: \u4f7f\u7528\u6539\u8fdb\u7684Gram-Schmidt\u548c\u91cd\u65b0\u6b63\u4ea4\u5316\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u6027\u80fd: \u6bcf\u6b21\u8fed\u4ee3O(n\u00b3)\u3002\u53ef\u80fd\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u624d\u80fd\u6536\u655b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u75c5\u6001\u77e9\u9635\u3002</p> </li> <li> <p>\u6536\u655b\u52a0\u901f: \u5b9e\u73b0\u53ef\u4ee5\u4ece\u79fb\u4f4d\uff08Wilkinson\u79fb\u4f4d\uff09\u4e2d\u53d7\u76ca\u4ee5\u52a0\u5feb\u6536\u655b\uff0c\u4f46\u5f53\u524d\u7248\u672c\u4f7f\u7528\u57fa\u672cQR\u8fed\u4ee3\u3002</p> </li> <li> <p>\u5e94\u7528:</p> </li> <li> <p>\u4e00\u822c\u77e9\u9635\u7279\u5f81\u503c\u95ee\u9898</p> </li> <li> <p>\u52a8\u529b\u7cfb\u7edf\u5206\u6790</p> </li> <li> <p>\u63a7\u5236\u7406\u8bba\uff08\u7cfb\u7edf\u6781\u70b9\uff09</p> </li> </ul> <p>\u6ce8\u610f:</p> <p>QR\u5728\u6b64\u5b9e\u73b0\u4e2d\u4f7f\u7528Gram\u2013Schmidt\u6784\u9020Q/R\uff1b\u5bf9\u4e8e\u75c5\u6001\u77e9\u9635\u53ef\u80fd\u4e0d\u592a\u7a33\u5b9a\u3002\u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635\uff0c\u7531\u4e8e\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\uff0c\u63a8\u8350\u4f7f\u7528Jacobi\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_60","title":"\u81ea\u52a8\u7279\u5f81\u5206\u89e3\uff08\u6839\u636e\u77e9\u9635\u7279\u6027\u9009\u62e9\u65b9\u6cd5\uff09","text":"<pre><code>Mat::EigenDecomposition Mat::eigendecompose(float tolerance) const;\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u7b80\u4fbf\u63a5\u53e3\uff0c\u4f1a\u5148\u8c03\u7528 <code>is_symmetric(tolerance * 10)</code> \u5224\u65ad\u77e9\u9635\u662f\u5426\u8fd1\u4f3c\u5bf9\u79f0\uff1a</p> <ul> <li> <p>\u82e5\u4e3a\u5bf9\u79f0\uff0c\u4f7f\u7528 <code>eigendecompose_jacobi</code>\uff1b</p> </li> <li> <p>\u5426\u5219\u4f7f\u7528 <code>eigendecompose_qr</code>\u3002</p> </li> </ul> <p>\u53c2\u6570:</p> <ul> <li><code>float tolerance</code>\uff1a\u7528\u4e8e\u5bf9\u79f0\u6027\u68c0\u6d4b\u4e0e\u5206\u89e3\u6536\u655b\u5224\u65ad\uff08\u5efa\u8bae <code>1e-6</code>\uff09\u3002</li> </ul> <p>\u8fd4\u56de\u503c:</p> <p><code>EigenDecomposition</code>\u3002</p> <p>\u4f7f\u7528\u5efa\u8bae:</p> <ul> <li> <p>\u82e5\u660e\u786e\u77e5\u9053\u77e9\u9635\u4e3a\u5bf9\u79f0\u77e9\u9635\uff08\u5982\u521a\u5ea6\u77e9\u9635\u3001\u8d28\u91cf\u77e9\u9635\uff09\uff0c\u76f4\u63a5\u4f7f\u7528 <code>eigendecompose_jacobi</code>\uff1b\u5bf9\u975e\u5bf9\u79f0\u6216\u4e0d\u786e\u5b9a\u60c5\u5f62\uff0c\u53ef\u4f7f\u7528 <code>eigendecompose</code>\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u8f83\u5927\u77e9\u9635\u6216\u5d4c\u5165\u5f0f\u573a\u666f\uff0cJacobi/QR \u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u8bf7\u6743\u8861\u6570\u503c\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_61","title":"\u6d41\u64cd\u4f5c\u7b26","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_62","title":"\u77e9\u9635\u8f93\u51fa\u6d41\u64cd\u4f5c\u7b26","text":"<pre><code>std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat &amp;m);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u77e9\u9635\u7684\u91cd\u8f7d\u8f93\u51fa\u6d41\u64cd\u4f5c\u7b26\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>std::ostream &amp;os</code> : \u8f93\u51fa\u6d41\u3002</p> </li> <li> <p><code>const Mat &amp;m</code> : \u8981\u8f93\u51fa\u7684\u77e9\u9635\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#roi_5","title":"ROI\u8f93\u51fa\u6d41\u64cd\u4f5c\u7b26","text":"<pre><code>std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const Mat::ROI &amp;roi);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>ROI\u7ed3\u6784\u4f53\u7684\u91cd\u8f7d\u8f93\u51fa\u6d41\u64cd\u4f5c\u7b26\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>std::ostream &amp;os</code> : \u8f93\u51fa\u6d41\u3002</p> </li> <li> <p><code>const Mat::ROI &amp;roi</code> : ROI\u7ed3\u6784\u3002</p> </li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_63","title":"\u77e9\u9635\u8f93\u5165\u6d41\u64cd\u4f5c\u7b26","text":"<pre><code>std::istream &amp;operator&gt;&gt;(std::istream &amp;is, Mat &amp;m);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u77e9\u9635\u7684\u91cd\u8f7d\u8f93\u5165\u6d41\u64cd\u4f5c\u7b26\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>std::istream &amp;is</code> : \u8f93\u5165\u6d41\u3002</p> </li> <li> <p><code>Mat &amp;m</code> : \u8981\u8f93\u5165\u7684\u77e9\u9635\u3002</p> </li> </ul> <p>Tip</p> <p>\u672c\u8282\u5b9e\u9645\u4e0a\u5728\u663e\u793a\u77e9\u9635\u65b9\u9762\u4e0e\u6253\u5370\u51fd\u6570\u6709\u4e9b\u91cd\u53e0\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_64","title":"\u5168\u5c40\u7b97\u672f\u8fd0\u7b97\u7b26","text":"<p>\u975e\u4fee\u6539\u64cd\u4f5c</p> <p>\u672c\u8282\u4e2d\u7684\u8fd0\u7b97\u7b26\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u77e9\u9635\u5bf9\u8c61\uff0c\u4f5c\u4e3a\u8fd0\u7b97\u7ed3\u679c\u3002\u539f\u59cb\u77e9\u9635\u4fdd\u6301\u4e0d\u53d8\u3002\u8fd9\u4e9b\u662f\u51fd\u6570\u5f0f\u64cd\u4f5c\uff0c\u4e0d\u4fee\u6539\u5176\u64cd\u4f5c\u6570\uff0c\u4f7f\u5176\u53ef\u4ee5\u5b89\u5168\u5730\u4e0econst\u5f15\u7528\u548c\u4e34\u65f6\u5bf9\u8c61\u4e00\u8d77\u4f7f\u7528\u3002</p> <p>\u4f55\u65f6\u4f7f\u7528</p> <ul> <li>\u4f7f\u7528\u5168\u5c40\u8fd0\u7b97\u7b26 (A + B) \u5f53\u60a8\u60f3\u4fdd\u7559\u539f\u59cb\u77e9\u9635\u65f6</li> <li>\u4f7f\u7528\u6210\u5458\u8fd0\u7b97\u7b26 (A += B) \u5f53\u60a8\u60f3\u5c31\u5730\u4fee\u6539\u77e9\u9635\u65f6\uff08\u66f4\u8282\u7701\u5185\u5b58\uff09</li> </ul>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_65","title":"\u52a0\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat operator+(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u4e24\u4e2a\u77e9\u9635\u76f8\u52a0\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u7b2c\u4e00\u4e2a\u77e9\u9635</p> </li> <li> <p><code>const Mat &amp;B</code>: \u7b2c\u4e8c\u4e2a\u77e9\u9635</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A+B</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_9","title":"\u52a0\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat operator+(const Mat &amp;A, float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u5e38\u91cf\u52a0\u5230\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> <li> <p><code>float C</code>: \u8f93\u5165\u5e38\u91cf</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A+C</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_66","title":"\u51cf\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat operator-(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u4e24\u4e2a\u77e9\u9635\u76f8\u51cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u7b2c\u4e00\u4e2a\u77e9\u9635</p> </li> <li> <p><code>const Mat &amp;B</code>: \u7b2c\u4e8c\u4e2a\u77e9\u9635</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A-B</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_10","title":"\u51cf\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat operator-(const Mat &amp;A, float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u4ece\u77e9\u9635\u4e2d\u51cf\u53bb\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> <li> <p><code>float C</code>: \u8f93\u5165\u5e38\u91cf</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A-C</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_67","title":"\u4e58\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat operator*(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u5c06\u4e24\u4e2a\u77e9\u9635\u76f8\u4e58\uff08\u77e9\u9635\u4e58\u6cd5\uff09\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u7b2c\u4e00\u4e2a\u77e9\u9635</p> </li> <li> <p><code>const Mat &amp;B</code>: \u7b2c\u4e8c\u4e2a\u77e9\u9635</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A*B</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_11","title":"\u4e58\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf","text":"<pre><code>Mat operator*(const Mat &amp;A, float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u77e9\u9635\u4e58\u4ee5\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> <li> <p><code>float C</code>: \u6d6e\u70b9\u6570\u503c</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A*C</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_12","title":"\u4e58\u6cd5\u8fd0\u7b97\u7b26 - \u5e38\u91cf\uff08\u5de6\u4fa7\uff09","text":"<pre><code>Mat operator*(float C, const Mat &amp;A);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u5e38\u91cf\u4e58\u4ee5\u77e9\u9635\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>float C</code>: \u6d6e\u70b9\u6570\u503c</p> </li> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 C*A</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_68","title":"\u9664\u6cd5\u8fd0\u7b97\u7b26","text":"<pre><code>Mat operator/(const Mat &amp;A, float C);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u77e9\u9635\u9664\u4ee5\u5e38\u91cf\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> <li> <p><code>float C</code>: \u6d6e\u70b9\u6570\u503c</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 A/C</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#-_13","title":"\u9664\u6cd5\u8fd0\u7b97\u7b26 - \u77e9\u9635","text":"<pre><code>Mat operator/(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u6309\u5143\u7d20\u5c06\u77e9\u9635 A \u9664\u4ee5\u77e9\u9635 B\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u8f93\u5165\u77e9\u9635 A</p> </li> <li> <p><code>const Mat &amp;B</code>: \u8f93\u5165\u77e9\u9635 B</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>Mat - \u7ed3\u679c\u77e9\u9635 C\uff0c\u5176\u4e2d C[i,j] = A[i,j]/B[i,j]</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-api/#_69","title":"\u7b49\u4e8e\u8fd0\u7b97\u7b26","text":"<pre><code>bool operator==(const Mat &amp;A, const Mat &amp;B);\n</code></pre> <p>\u63cf\u8ff0:</p> <p>\u7b49\u4e8e\u8fd0\u7b97\u7b26\uff0c\u68c0\u67e5\u4e24\u4e2a\u77e9\u9635\u662f\u5426\u76f8\u7b49\u3002</p> <p>\u53c2\u6570:</p> <ul> <li> <p><code>const Mat &amp;A</code>: \u7b2c\u4e00\u4e2a\u77e9\u9635\u5bf9\u8c61</p> </li> <li> <p><code>const Mat &amp;B</code>: \u7b2c\u4e8c\u4e2a\u77e9\u9635\u5bf9\u8c61</p> </li> </ul> <p>\u8fd4\u56de\u503c:</p> <p>\u5e03\u5c14\u503c\uff0c\u8868\u793a\u4e24\u4e2a\u77e9\u9635\u662f\u5426\u76f8\u7b49</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-code/","title":"\u4ee3\u7801","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-test/","title":"\u6d4b\u8bd5","text":"<p>Tip</p> <p>\u4ee5\u4e0b\u7684\u6d4b\u8bd5\u7528\u4ee3\u7801\u548c\u6848\u4f8b\u4e5f\u4f5c\u4e3a\u4f7f\u7528\u6559\u5b66\u6848\u4f8b\u3002</p>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#_2","title":"\u7ed3\u679c\u8f93\u51fa","text":""},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-i-1-3","title":"PHASE I: \u5bf9\u8c61\u57fa\u7840 (\u7ec4 1-3)","text":"<p>\u76ee\u7684\uff1a\u5b66\u4e60\u521b\u5efa\u548c\u64cd\u4f5c\u77e9\u9635\u5bf9\u8c61</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 1: Object Foundation - Constructor &amp; Destructor Tests]\n[Test 1.1] Default Constructor\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9a78\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.2] Constructor with Rows and Cols\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |\n           0            0            0            0       |\n           0            0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.3] Constructor with Rows, Cols and Stride\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9ad0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.4] Constructor with External Data\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fc9928c\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |\n           4            5            6            7       |\n           8            9           10           11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.5] Constructor with External Data and Stride\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc992e0\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 1.6] Copy Constructor\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9bd8\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 2: Object Foundation - Element Access Tests]\n[Test 2.1] Non-const Access\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            3\nelements        6\npaddings        0\nstride          3\nmemory          6\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         1.1          2.2          3.3       |\n         4.4          5.5          6.6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 2.2] Const Access\nconst_mat(0, 0): 1.1\n\n[Group 3: Object Foundation - Data Manipulation Tests (ROI Operations)]\n[Material Matrices]\nmatA:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            3\nelements        6\npaddings        0\nstride          3\nmemory          6\ndata pointer    0x3fce9a9c\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2          0.3       |\n         0.4          0.5          0.6       |\n&lt;&lt;&lt; Matrix Elements\n\nmatB:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatC:\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9a78\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.1] Copy ROI - Over Range Case\n[&gt;&gt;&gt; Error ! &lt;&lt;&lt;] Invalid column position \nmatB after copy_paste matA at (1, 2):\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4            5            6            7       |      0 \n           8            9           10           11       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nnothing changed.\n[Test 3.1] Copy ROI - Suitable Range Case\nmatB after copy_paste matA at (1, 1):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nsuccessfully copied.\n[Test 3.2] Copy Head\nmatC after copy_head matB:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.2] Copy Head - Memory Sharing Check\nmatB(0, 0) = 99.99f\nmatC:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       99.99            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.3] Get a View of ROI - Low Level Function\nget a view of ROI with overrange dimensions - rows:\n[Error] Invalid ROI request.\nget a view of ROI with overrange dimensions - cols:\n[Error] Invalid ROI request.\nget a view of ROI with suitable dimensions:\nroi3:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991dc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |    0.3            0            8 \n         0.4          0.5       |    0.6            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.4] Get a View of ROI - Using ROI Structure\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991dc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |    0.3            0            8 \n         0.4          0.5       |    0.6            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.5] Copy ROI - Low Level Function\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9bfc\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.6] Copy ROI - Using ROI Structure\ntime for copy_roi using ROI structure: 28 ms\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9c10\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.7] Block\ntime for block: 34 ms\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        0\nstride          2\nmemory          4\ndata pointer    0x3fce9c24\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.1          0.2       |\n         0.4          0.5       |\n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.8] Swap Rows\nmatB before swap rows:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       99.99            1            2            3       |      0 \n           4          0.1          0.2          0.3       |      0 \n           8          0.4          0.5          0.6       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after swap_rows(0, 2):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           8          0.4          0.5          0.6       |      0 \n           4          0.1          0.2          0.3       |      0 \n       99.99            1            2            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.9] Swap Columns\nmatB before swap columns:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           8          0.4          0.5          0.6       |      0 \n           4          0.1          0.2          0.3       |      0 \n       99.99            1            2            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after swap_cols(0, 2):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.5          0.4            8          0.6       |      0 \n         0.2          0.1            4          0.3       |      0 \n           2            1        99.99            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n[Test 3.10] Clear\nmatB before clear:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n         0.5          0.4            8          0.6       |      0 \n         0.2          0.1            4          0.3       |      0 \n           2            1        99.99            3       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nmatB after clear:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991c4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n           0            0            0            0       |      0 \n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-ii-4","title":"PHASE II: \u57fa\u672c\u8fd0\u7b97 (\u7ec4 4)","text":"<p>\u76ee\u7684\uff1a\u5b66\u4e60\u57fa\u672c\u7b97\u672f\u8fd0\u7b97</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 4.1: Basic Operations - Assignment Operator Tests]\n\n[Test 4.1.1] Assignment (Same Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.2] Assignment (Different Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.3] Assignment to Sub-Matrix (Expect Error)\n[Error] Assignment to a sub-matrix is not allowed.\nMatrix Elements &gt;&gt;&gt;\n           5            6       |      7            0            8 \n           9           10       |     11            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.1.4] Self-Assignment\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.2: Matrix Addition Tests]\n\n[Test 4.2.1] Matrix Addition (Same Dimensions)\nMatrix Elements &gt;&gt;&gt;\n           2            3            4       |\n           5            6            7       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.2] Sub-Matrix Addition\nMatrix Elements &gt;&gt;&gt;\n          10           12       |      7            0            8 \n          18           20       |     11            0           12 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.3] Full Matrix + Sub-Matrix Addition\nMatrix Elements &gt;&gt;&gt;\n          12           14       |\n          20           22       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.2.4] Addition Dimension Mismatch (Expect Error)\n[Error] Matrix addition failed: Dimension mismatch (2x2 vs 3x3)\n\n[Group 4.3: Constant Addition Tests]\n\n[Test 4.3.1] Full Matrix + Constant\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.2] Sub-Matrix + Constant\nMatrix Elements &gt;&gt;&gt;\n           8            9       |      7            0            8 \n          12           13       |     11            0           12 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.3] Add Zero\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.3.4] Add Negative Constant\nMatrix Elements &gt;&gt;&gt;\n          -5            5       |\n          15           25       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.4: Matrix Subtraction Tests]\n\n[Test 4.4.1] Matrix Subtraction\nMatrix Elements &gt;&gt;&gt;\n           4            5       |\n           6            7       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.4.2] Subtraction Dimension Mismatch (Expect Error)\n[Error] Matrix subtraction failed: Dimension mismatch (2x2 vs 3x3)\n\n[Group 4.5: Constant Subtraction Tests]\n\n[Test 4.5.1] Full Matrix - Constant\nMatrix Elements &gt;&gt;&gt;\n          -1            0            1       |\n           2            3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.5.2] Sub-Matrix - Constant\nMatrix Elements &gt;&gt;&gt;\n         3.5          4.5       |      7            0            8 \n         7.5          8.5       |     11            0   4.2039e-45 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.6: Matrix Element-wise Division Tests]\n\n[Test 4.6.1] Element-wise Division (Same Dimensions, No Zero)\nMatrix Elements &gt;&gt;&gt;\n           5            5       |\n           6            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.6.2] Dimension Mismatch (Expect Error)\n[Error] Matrix division failed: Dimension mismatch (2x2 vs 3x3)\n\n[Test 4.6.3] Division by Matrix Containing Zero (Expect Error)\n[Error] Matrix division failed: Division by zero detected.\nMatrix Elements &gt;&gt;&gt;\n           5           10       |\n          15           20       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.7: Matrix Division by Constant Tests]\n\n[Test 4.7.1] Divide Full Matrix by Positive Constant\nMatrix Elements &gt;&gt;&gt;\n           1          1.5            2       |\n         2.5            3          3.5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.7.2] Divide Matrix by Negative Constant\nMatrix Elements &gt;&gt;&gt;\n          -2           -4       |\n          -6           -8       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.7.3] Division by Zero Constant (Expect Error)\n[Error] Matrix division by zero is undefined.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 4.8: Matrix Exponentiation Tests]\n\n[Test 4.8.1] Raise Each Element to Power of 2\nMatrix Elements &gt;&gt;&gt;\n           4            9       |\n          16           25       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.2] Raise Each Element to Power of 0\nMatrix Elements &gt;&gt;&gt;\n           1            1       |\n           1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.3] Raise Each Element to Power of 1\nMatrix Elements &gt;&gt;&gt;\n           9            8       |\n           7            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.4] Raise Each Element to Power of -1 (Expect Error or Warning)\n[Error] Negative exponent not supported in operator^.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 4.8.5] Raise Matrix Containing Zero to Power of 3\nMatrix Elements &gt;&gt;&gt;\n           0            8       |\n          -1           27       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-iii-5","title":"PHASE III: \u77e9\u9635\u5c5e\u6027 (\u7ec4 5)","text":"<p>\u76ee\u7684\uff1a\u7406\u89e3\u77e9\u9635\u5c5e\u6027\u548c\u57fa\u672c\u7ebf\u6027\u4ee3\u6570</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 5.1: Matrix Properties - Matrix Transpose Tests]\n\n[Test 5.1.1] Transpose of 2x3 Matrix\nOriginal 2x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nTransposed 3x2 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            4       |\n           2            5       |\n           3            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.2] Transpose of 3x3 Square Matrix\nOriginal 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nTransposed 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            4            7       |\n           2            5            8       |\n           3            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.3] Transpose of Matrix with Padding\nOriginal 4x2 Matrix (with padding):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |      0 \n           3            4       |      0 \n           5            6       |      0 \n           7            8       |      0 \n&lt;&lt;&lt; Matrix Elements\n\nTransposed 2x4 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            3            5            7       |\n           2            4            6            8       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.1.4] Transpose of Empty Matrix\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.2: Matrix Minor and Cofactor Tests]\n\n[Test 5.2.1] Minor of 3x3 Matrix (Remove Row 1, Col 1)\nOriginal 3x3 Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nMinor Matrix (remove row 1, col 1, no sign):\nMatrix Elements &gt;&gt;&gt;\n           1            3       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.2] Cofactor of 3x3 Matrix (Remove Row 1, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      The sign (-1)^(i+j) is applied when computing cofactor value, not to matrix elements.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           1            3       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.3] Minor (Remove Row 0, Col 0)\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.4] Cofactor (Remove Row 0, Col 0)\nNote: Cofactor matrix is the same as minor matrix.\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.5] Cofactor (Remove Row 0, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      When computing cofactor value, sign (-1)^(0+1) = -1 would be applied.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           4            6       |\n           7            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.6] Minor (Remove Row 2, Col 2)\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.7] Cofactor (Remove Row 2, Col 2)\nNote: Cofactor matrix is the same as minor matrix.\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           4            5       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.8] Minor of 4x4 Matrix (Remove Row 2, Col 1)\nMatrix Elements &gt;&gt;&gt;\n           1            2            3            4       |\n           5            6            7            8       |\n           9           10           11           12       |\n          13           14           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\nMinor Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            3            4       |\n           5            7            8       |\n          13           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.9] Cofactor of 4x4 Matrix (Remove Row 2, Col 1)\nNote: Cofactor matrix is the same as minor matrix.\n      When computing cofactor value, sign (-1)^(2+1) = -1 would be applied.\nCofactor Matrix (same as minor):\nMatrix Elements &gt;&gt;&gt;\n           1            3            4       |\n           5            7            8       |\n          13           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.2.10] Non-square Matrix (Expect Error)\nTesting minor():\n[Error] Minor requires square matrix.\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nTesting cofactor():\n[Error] Minor requires square matrix.\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.3: Matrix Determinant Tests]\n\n[Test 5.3.1] 1x1 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: 7  (Expected: 7)\n\n[Test 5.3.2] 2x2 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           3            8       |\n           4            6       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: -14  (Expected: -14)\n\n[Test 5.3.3] 3x3 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            4            5       |\n           1            0            6       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant: 22  (Expected: 22)\n\n[Test 5.3.4] 4x4 Matrix Determinant\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3            4       |\n           5            6            7            8       |\n           9           10           11           12       |\n          13           14           15           16       |\n&lt;&lt;&lt; Matrix Elements\n\nNote: This matrix has linearly dependent rows (each row differs by constant 4),\n      so the determinant should be 0.\nDeterminant: 0  (Expected: 0)\n\n[Test 5.3.5] 5x5 Matrix Determinant (Tests Auto-select to LU Method)\nMatrix (5x5, tridiagonal):\nMatrix Elements &gt;&gt;&gt;\n           2            1            0            0            0       |\n           1            2            1            0            0       |\n           0            1            2            1            0       |\n           0            0            1            2            1       |\n           0            0            0            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant (auto-select, should use LU for n &gt; 4): 6\nNote: For n = 5 &gt; 4, auto-select should use LU decomposition (O(n\u00b3)).\n\n[Test 5.3.6] Non-square Matrix (Expect Error)\nMatrix (3x4, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0            0       |\n           0            0            0            0       |\n           0            0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Determinant requires a square matrix.\nDeterminant: 0  (Expected: 0 with error message)\n\n[Test 5.3.7] Comparison of Different Methods (5x5 Matrix)\nMatrix (5x5):\nMatrix Elements &gt;&gt;&gt;\n           2            2            3            4            5       |\n           2            5            6            8           10       |\n           3            6           10           12           15       |\n           4            8           12           17           20       |\n           5           10           15           20           26       |\n&lt;&lt;&lt; Matrix Elements\n\nDeterminant (auto-select): 56  (should use LU for n &gt; 4)\nDeterminant (Laplace):     56  (O(n!), slow for n=5)\nDeterminant (LU):          56  (O(n\u00b3), efficient)\nDeterminant (Gaussian):    56  (O(n\u00b3), efficient)\nNote: All methods should give the same result (within numerical precision).\n      Auto-select should use LU for n &gt; 4, avoiding slow Laplace expansion.\n\n[Test 5.3.8] Large Matrix (6x6) - Tests Efficient Methods\nMatrix (6x6, showing first 4x4 block):\n       1.5          2          3          4 ...\n         2        4.5          6          8 ...\n         3          6        9.5         12 ...\n         4          8         12       16.5 ...\n...\nDeterminant (auto-select, uses LU): 2.85938\nDeterminant (LU):                   2.85938\nDeterminant (Gaussian):             2.85938\nNote: For n &gt; 4, auto-select uses LU decomposition (O(n\u00b3) instead of O(n!)).\n\n[Test 5.3.9] Large Matrix (8x8) - Performance Comparison\nMatrix (8x8, showing first 4x4 block):\n         1          2          3          4 ...\n         2          4          6          8 ...\n         3          6          9         12 ...\n         4          8         12         16 ...\n...\n[Error] LU decomposition: Matrix is singular or near-singular.\nDeterminant (LU):       0\nDeterminant (Gaussian): 0\nNote: Both methods are O(n\u00b3) and should be much faster than Laplace expansion.\n\n[Group 5.4: Matrix Adjoint Tests]\n\n[Test 5.4.1] Adjoint of 1x1 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.2] Adjoint of 2x2 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n           4            2       |\n           3            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.3] Adjoint of 3x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            4            5       |\n           1            0            6       |\n&lt;&lt;&lt; Matrix Elements\n\nAdjoint Matrix:\nMatrix Elements &gt;&gt;&gt;\n          24           12           -2       |\n          -5            3            5       |\n          -4           -2            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.4.4] Adjoint of Non-Square Matrix (Expect Error)\nOriginal Matrix (2x3, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Adjoint requires a square matrix.\nAdjoint Matrix (should be empty due to error):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.5: Matrix Normalization Tests]\n\n[Test 5.5.1] Normalize a Standard 2x2 Matrix\nBefore normalization:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter normalization (Expected L2 norm = 1):\nMatrix Elements &gt;&gt;&gt;\n    0.424264     0.565685       |\n    0.424264     0.565685       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.5.2] Normalize a 2x2 Matrix with Stride=4 (Padding Test)\nBefore normalization:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |      0            0 \n           3            4       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\nAfter normalization:\nMatrix Elements &gt;&gt;&gt;\n    0.424264     0.565685       |      0            0 \n    0.424264     0.565685       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.5.3] Normalize a Zero Matrix (Expect Warning)\nMatrix Elements &gt;&gt;&gt;\n           0            0       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.6: Matrix Norm Calculation Tests]\n\n[Test 5.6.1] 2x2 Matrix Norm (Expect 5.0)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           3            4       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5\n\n[Test 5.6.2] Zero Matrix Norm (Expect 0.0)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 0\n\n[Test 5.6.3] Matrix with Negative Values\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n          -1           -2       |\n          -3           -4       |\n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5.47723  (Expect sqrt(30) \u2248 5.477)\n\n[Test 5.6.4] 2x2 Matrix with Stride=4 (Padding Test)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |      0            0 \n           3            4       |      0            0 \n&lt;&lt;&lt; Matrix Elements\n\nCalculated Norm: 5.47723  (Expect sqrt(30) \u2248 5.477)\n\n[Group 5.7: Matrix Inversion Tests]\n\n[Test 5.7.1] Inverse of 2x2 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           4            7       |\n           2            6       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse Matrix:\nMatrix Elements &gt;&gt;&gt;\n         0.6          0.7       |\n         0.2          0.4       |\n&lt;&lt;&lt; Matrix Elements\n\nExpected Approx:\n[ 0.6  -0.7 ]\n[ -0.2  0.4 ]\n\n[Test 5.7.2] Singular Matrix (Expect Error)\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nNote: This matrix is singular (determinant = 0), so inverse should fail.\n[Error] Matrix is singular, cannot compute inverse.\nInverse Matrix (Should be zero matrix):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.7.3] Inverse of 3x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           3            0            2       |\n           2            0           -2       |\n           0            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse Matrix:\nMatrix Elements &gt;&gt;&gt;\n         0.2         -0.2           -0       |\n         0.2          0.3           -1       |\n         0.2          0.3            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.7.4] Non-Square Matrix (Expect Error)\nOriginal Matrix (2x3, non-square):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Determinant requires a square matrix.\n[Error] Matrix is singular, cannot compute inverse.\nInverse Matrix (should be empty due to error):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.8: Matrix Utilities Tests]\n\n[Test 5.8.1] Generate Identity Matrix (eye)\n3x3 Identity Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n5x5 Identity Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0            0            0       |\n           0            1            0            0            0       |\n           0            0            1            0            0       |\n           0            0            0            1            0       |\n           0            0            0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.2] Generate Ones Matrix\n3x4 Ones Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n4x4 Ones Matrix (Square):\nMatrix Elements &gt;&gt;&gt;\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n           1            1            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.3] Augment Two Matrices Horizontally [A | B]\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\nAugmented Matrix [A | B]:\nMatrix Elements &gt;&gt;&gt;\n           1            2            5            6            7       |\n           3            4            8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.4] Augment with Row Mismatch (Expect Error)\n[Error] Cannot augment matrices: Row counts do not match (2 vs 3)\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9cf0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\n\n[Test 5.8.5] Vertically Stack Two Matrices [A; B]\nMatrix A (top):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (bottom):\nMatrix Elements &gt;&gt;&gt;\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nVertically Stacked Matrix [A; B]:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nExpected: 4x3 matrix with A on top, B on bottom\n\n[Test 5.8.6] Vertical Stack with Different Row Counts (Same Columns)\nMatrix A (1x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (3x3):\nMatrix Elements &gt;&gt;&gt;\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nVertically Stacked Matrix [A; B] (1x3 + 3x3 = 4x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n          10           11           12       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.8.7] VStack with Column Mismatch (Expect Error)\nMatrix A (2x2):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B (2x3, different columns):\nMatrix Elements &gt;&gt;&gt;\n           5            6            7       |\n           8            9           10       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Cannot vstack matrices: Column counts do not match (2 vs 3)\nResult (should be empty due to error):\nMatrix Info &gt;&gt;&gt;\nrows            1\ncols            1\nelements        1\npaddings        0\nstride          1\nmemory          1\ndata pointer    0x3fce9db0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-iv-6","title":"PHASE IV: \u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3 (\u7ec4 6)","text":"<p>\u76ee\u7684\uff1a\u6838\u5fc3\u5e94\u7528 - \u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4 Ax = b</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 5.9: Gaussian Elimination Tests]\n\n[Test 5.9.1] 3x3 Matrix (Simple Upper Triangular)\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           2            1           -1       |\n          -3           -1            2       |\n          -2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should be upper triangular):\nMatrix Elements &gt;&gt;&gt;\n           2            1           -1       |\n           0          0.5          0.5       |\n           0            0           -1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.2] 3x4 Augmented Matrix (Linear System Ax = b)\nOriginal Augmented Matrix [A | b]:\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1            8       |\n          -3           -1            2          -11       |\n          -2            1            2           -3       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Row Echelon Form):\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1            8       |\n           0            5           -1           13       |\n           0            0            1            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.3] Singular Matrix (No Unique Solution)\nOriginal Singular Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should show rows of zeros):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.9.4] Zero Matrix\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\nAfter Gaussian Elimination (Should be a zero matrix):\nMatrix Elements &gt;&gt;&gt;\n           0            0            0       |\n           0            0            0       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.10: Row Reduce from Gaussian (RREF) Tests]\n\n[Test 5.10.1] 3x4 Augmented Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2           -1           -4       |\n           2            3           -1          -11       |\n          -2            0           -3           22       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0            0           -8       |\n           0            1            0            1       |\n           0            0            1           -2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.10.2] 2x3 Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0           -1       |\n           0            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.10.3] Already Reduced Matrix\nOriginal Matrix:\nMatrix Elements &gt;&gt;&gt;\n           1            0            2       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nRREF Result:\nMatrix Elements &gt;&gt;&gt;\n           1            0            2       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.11: Gaussian Inverse Tests]\n\n[Test 5.11.1] 2x2 Matrix Inverse\nOriginal matrix (mat1):\nMatrix Elements &gt;&gt;&gt;\n           4            7       |\n           2            6       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (mat1):\nMatrix Elements &gt;&gt;&gt;\n         0.6         -0.7       |\n        -0.2          0.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.2] Identity Matrix Inverse\nOriginal matrix (Identity):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (Identity):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n           0            1            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.3] Singular Matrix (Expected: No Inverse)\nOriginal matrix (singular):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Matrix is singular, cannot compute inverse.\nInverse matrix (singular):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.4] 3x3 Matrix Inverse\nOriginal matrix (mat4):\nMatrix Elements &gt;&gt;&gt;\n           4            7            2       |\n           3            5            1       |\n           8            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\nInverse matrix (mat4):\nMatrix Elements &gt;&gt;&gt;\n    -1.85714      2.42857     0.142857       |\n    0.904762    -0.952381   -0.0952381       |\n     1.04762     -1.52381     0.047619       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.11.5] Non-square Matrix Inverse (Expected Error)\nOriginal matrix (non-square):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Inversion requires a square matrix.\nInverse matrix (non-square):\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.12: Dot Product Tests]\n\n[Test 5.12.1] Valid Dot Product (Same Length Vectors)\nVector A:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector B:\nMatrix Elements &gt;&gt;&gt;\n           4       |\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nDot product of vectorA and vectorB: 32\n\n[Test 5.12.2] Invalid Dot Product (Dimension Mismatch)\nVector A (3x1):\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector C (2x1, different size):\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Dot product requires matrices of the same size.\nDot product (dimension mismatch): 0\n\n[Test 5.12.3] Dot Product of Zero Vectors\nZero Vector A:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nZero Vector B:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nDot product of zero vectors: 0\n\n[Group 5.13: Solve Linear System Tests]\n\n[Test 5.13.1] Solving a Simple 2x2 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         1.8       |\n         1.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.2] Solving a 3x3 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            1       |\n           2            0            3       |\n           3            2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           8       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n          -1       |\n     3.33333       |\n     3.33333       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.3] Solving a System Where One Row is All Zeros (Expect Failure or Infinite Solutions)\nMatrix A (has zero row):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           0            0            0       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           0       |\n          15       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.4] Solving a System with Zero Determinant (Singular Matrix)\nMatrix A (singular, determinant = 0):\nMatrix Elements &gt;&gt;&gt;\n           2            4            1       |\n           1            2            3       |\n           3            6            2       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.5] Solving a System with Linearly Dependent Rows (Expect Failure or Infinite Solutions)\nMatrix A (all rows linearly dependent):\nMatrix Elements &gt;&gt;&gt;\n           1            1            1       |\n           2            2            2       |\n           3            3            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           6       |\n          12       |\n          18       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, matrix is singular.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.13.6] Solving a Larger 4x4 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           4            2            3            1       |\n           2            5            1            2       |\n           3            1            6            3       |\n           1            2            3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          12       |\n          14       |\n          16       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n     1.80645       |\n    0.258065       |\n   -0.516129       |\n     3.80645       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.14: Band Solve Tests]\n\n[Test 5.14.1] Simple 3x3 Band Matrix\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            0       |\n           1            3            2       |\n           0            1            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         2.5       |\n           0       |\n        1.75       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.2] 4x4 Band Matrix\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            0            0       |\n           1            3            2            0       |\n           0            1            4            2       |\n           0            0            1            5       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           8       |\n           9       |\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n     3.51429       |\n    0.971429       |\n     1.28571       |\n     1.94286       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.3] Incompatible Dimensions (Expect Error)\nMatrix A (3x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b (2x1, incompatible):\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Matrix dimensions are not compatible for solving.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.14.4] Singular Matrix (No Unique Solution)\nMatrix A (singular, linearly dependent rows):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           2            4            6       |\n           3            6            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          20       |\n          30       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Zero pivot detected in bandSolve. Cannot proceed.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n           0       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Group 5.15: Roots Tests]\n\n[Test 5.15.1] Solving a Simple 2x2 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         1.8       |\n         1.4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.2] Solving a 3x3 System Ax = b\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            1       |\n           2            0            3       |\n           3            2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           9       |\n           8       |\n           7       |\n&lt;&lt;&lt; Matrix Elements\n\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n          -1       |\n     3.33333       |\n     3.33333       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.3] Singular Matrix (No Unique Solution)\nMatrix A (singular, linearly dependent rows):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            4       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b:\nMatrix Elements &gt;&gt;&gt;\n           5       |\n           6       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Pivot is zero, system may have no solution.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 5.15.4] Incompatible Dimensions (Expect Error)\nMatrix A (3x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nVector b (2x1, incompatible):\nMatrix Elements &gt;&gt;&gt;\n          10       |\n          11       |\n&lt;&lt;&lt; Matrix Elements\n\n[Error] Cannot augment matrices: Row counts do not match (3 vs 2)\n[Error] Pivot is zero, system may have no solution.\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-v-7-8","title":"PHASE V: \u9ad8\u7ea7\u7ebf\u6027\u4ee3\u6570 (\u7ec4 7-8)","text":"<p>\u76ee\u7684\uff1a\u9ad8\u7ea7\u7ebf\u6027\u4ee3\u6570\u8fd0\u7b97\uff0c\u7528\u4e8e\u7a33\u5b9a\u548c\u9ad8\u6548\u6c42\u89e3</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 7: Advanced Linear Algebra - Matrix Decomposition Tests]\n\n[Test 7.1] is_positive_definite() - Basic Functionality\n\n[Test 7.1.1] Positive Definite 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           4            1            0       |\n           1            3            0       |\n           0            0            2       |\n&lt;&lt;&lt; Matrix Elements\n\nIs positive definite: True (Expected: True) [PASS]\n\n[Test 7.1.2] Non-Positive Definite Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           2            1       |\n&lt;&lt;&lt; Matrix Elements\n\nIs positive definite: False (Expected: False) [PASS]\n\n[Test 7.2] LU Decomposition\n\n[Test 7.2.1] 3x3 Matrix - LU Decomposition with Pivoting\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           2            1            1       |\n           4            3            3       |\n           2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nL matrix (lower triangular):\nMatrix Elements &gt;&gt;&gt;\n           1            0            0       |\n         0.5            1            0       |\n         0.5            1            1       |\n&lt;&lt;&lt; Matrix Elements\n\nU matrix (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n           4            3            3       |\n           0         -0.5         -0.5       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\nP matrix (permutation):\nMatrix Elements &gt;&gt;&gt;\n           0            1            0       |\n           1            0            0       |\n           0            0            1       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] P * A should equal L * U\nTotal difference: 0 [PASS]\n\n[Test 7.2.2] Solve Linear System using LU Decomposition\nSystem: A * x = b\nA:\nMatrix Elements &gt;&gt;&gt;\n           2            1            1       |\n           4            3            3       |\n           2            1            2       |\n&lt;&lt;&lt; Matrix Elements\n\nb:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           2       |\n           3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n         0.5       |\n          -2       |\n           2       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error: 0 [PASS]\n\n[Test 7.3] Cholesky Decomposition\n\n[Test 7.3.1] SPD Matrix - Cholesky Decomposition\nMatrix A (SPD):\nMatrix Elements &gt;&gt;&gt;\n           4            2            0       |\n           2            5            1       |\n           0            1            3       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nL matrix (lower triangular):\nMatrix Elements &gt;&gt;&gt;\n           2            0            0       |\n           1            2            0       |\n           0          0.5      1.65831       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] L * L^T should equal A\nTotal difference: 2.38419e-07 [PASS]\n\n[Test 7.3.2] Solve Linear System using Cholesky Decomposition\nSolution x:\nMatrix Elements &gt;&gt;&gt;\n    0.272727       |\n    0.454545       |\n    0.181818       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error: 0 [PASS]\n\n[Test 7.4] QR Decomposition\n\n[Test 7.4.1] General 3x3 Matrix - QR Decomposition\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nQ matrix (orthogonal):\nMatrix Elements &gt;&gt;&gt;\n    0.123091     0.904534     0.408248       |\n    0.492366     0.301511    -0.816497       |\n     0.86164    -0.301511     0.408248       |\n&lt;&lt;&lt; Matrix Elements\n\nR matrix (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n     8.12404      9.60114      11.0782       |\n           0     0.904534      1.80907       |\n           0            0            0       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Q * R should equal A\nTotal difference: 1.66893e-06 [PASS]\nQ orthogonality error: 2.83733e-07 [PASS]\n\n[Test 7.4.2] Least Squares Solution using QR Decomposition\nOverdetermined system: A * x \u2248 b\nA:\nMatrix Elements &gt;&gt;&gt;\n           1            1       |\n           1            2       |\n           1            3       |\n&lt;&lt;&lt; Matrix Elements\n\nb:\nMatrix Elements &gt;&gt;&gt;\n           2       |\n           3       |\n           4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nLeast squares solution x:\nMatrix Elements &gt;&gt;&gt;\n           1       |\n           1       |\n&lt;&lt;&lt; Matrix Elements\n\nResidual norm ||A*x - b||: 4.12953e-07\n\n[Test 7.5] Singular Value Decomposition (SVD)\n\n[Test 7.5.1] General 3x3 Matrix - SVD Decomposition\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nSingular values:\nMatrix Elements &gt;&gt;&gt;\n     1.06837       |\n     16.8481       |\n           0       |\n&lt;&lt;&lt; Matrix Elements\n\nNumerical rank: 2\nIterations: 7\nReconstruction error: 1.68085e-05 [PASS]\n\n[Test 7.5.2] Pseudo-inverse using SVD\nMatrix A (3x2):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n           5            6       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nPseudo-inverse A^+ (2x3):\nMatrix Elements &gt;&gt;&gt;\n    -1.33333    -0.333333     0.666666       |\n     1.08333     0.333333    -0.416666       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification error (A * A^+ * A \u2248 A): 5.72205e-06 [PASS]\n\n[Test 7.6] Matrix Decomposition Performance Tests\n\n[Test 7.6.1] LU Decomposition Performance\n[Performance] LU Decomposition (4x4 matrix): 110.00 us\n\n[Test 7.6.2] Cholesky Decomposition Performance\n[Performance] Cholesky Decomposition (4x4 SPD matrix): 62.00 us\n\n[Test 7.6.3] QR Decomposition Performance\n[Performance] QR Decomposition (4x4 matrix): 157.00 us\n\n[Test 7.6.4] SVD Decomposition Performance\n[Performance] SVD Decomposition (4x4 matrix): 288.00 us\n\n[Matrix Decomposition Tests Complete]\n\n[Group 8: Advanced Linear Algebra - Gram-Schmidt Orthogonalization Tests]\n\n[Test 8.1] Basic Orthogonalization - Linearly Independent Vectors\nInput vectors (each column is a vector):\nMatrix Elements &gt;&gt;&gt;\n        1.00         1.00         0.00       |\n        0.00         1.00         1.00       |\n        1.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q (each column is orthogonal):\nMatrix Elements &gt;&gt;&gt;\n        0.71         0.41        -0.58       |\n        0.00         0.82         0.58       |\n        0.71        -0.41         0.58       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R (upper triangular):\nMatrix Elements &gt;&gt;&gt;\n        1.41         0.71         0.71       |\n        0.00         1.22         0.41       |\n        0.00         0.00         1.15       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Q^T * Q should be identity\nOrthogonality error: 0.00 [PASS]\n\n[Verification] Each column of Q should be normalized\n  Column 0 norm: 1.00 (error: 0.00) [PASS]\n  Column 1 norm: 1.00 (error: 0.00) [PASS]\n  Column 2 norm: 1.00 (error: 0.00) [PASS]\n\n[Verification] Q * R should reconstruct original vectors\nReconstruction error: 0.00 [PASS]\n\n[Test 8.2] Orthogonalization - Near-Linear-Dependent Vectors\nInput vectors (third vector is nearly linear dependent):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         1.00       |\n        0.00         1.00         1.00       |\n        0.00         0.00         0.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         1.00       |\n        0.00         1.00         1.00       |\n        0.00         0.00         0.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Note] Third column norm: 1.00 (should be 0 if linearly dependent, or 1 if orthogonalized)\n\n[Test 8.3] Orthogonalization - 2D Vectors (2x2)\nInput vectors:\nMatrix Elements &gt;&gt;&gt;\n        3.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\nStatus: OK\nOrthogonalized vectors Q:\nMatrix Elements &gt;&gt;&gt;\n        0.95        -0.32       |\n        0.32         0.95       |\n&lt;&lt;&lt; Matrix Elements\n\nCoefficients R:\nMatrix Elements &gt;&gt;&gt;\n        3.16         1.58       |\n        0.00         1.58       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Verification] Dot product of Q columns: 0.00 (should be ~0 for orthogonal) [PASS]\n\n[Test 8.4] Error Handling - Invalid Input\nEmpty matrix test: FAIL (should return false)\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-vi-9","title":"PHASE VI: \u7cfb\u7edf\u8bc6\u522b\u5e94\u7528 (\u7ec4 9)","text":"<p>\u76ee\u7684\uff1a\u7279\u5f81\u503c\u5206\u89e3\uff0c\u7528\u4e8e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u548c\u6a21\u6001\u5206\u6790</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 9: System Identification - Eigenvalue and Eigenvector Decomposition Tests]\n\n[Test 9.1] is_symmetric() - Basic Functionality\n[Test 9.1.1] Symmetric 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           4            1            2       |\n           1            3            0       |\n           2            0            5       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: True (Expected: True)\n\n[Test 9.1.2] Non-Symmetric 3x3 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: False (Expected: False)\n\n[Test 9.1.3] Non-Square Matrix (2x3)\nIs symmetric: False (Expected: False)\n\n[Test 9.1.4] Symmetric Matrix with Small Numerical Errors\nMatrix with error 1e-05:\nMatrix Elements &gt;&gt;&gt;\n           1      2.00001       |\n           2            3       |\n&lt;&lt;&lt; Matrix Elements\n\nDifference: |A(0,1) - A(1,0)| = 1.001358e-05 (Expected: 0.000010)\nA(0,1) stored value: 2.00001001 (Expected: 2.00001001)\nIs symmetric (tolerance=1e-4): True (Expected: True, tolerance &gt; error) [PASS]\nIs symmetric (tolerance=1e-6): False (Expected: False, tolerance &lt; error) [PASS]\nDifference accuracy: |actual_diff - expected_diff| = 1.36e-08 [PASS - difference stored correctly]\n\n[Test 9.2] power_iteration() - Dominant Eigenvalue\n\n[Test 9.2.1] Simple 2x2 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        2.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\n  Expected dominant eigenvector (for \u03bb=3): approximately [0.707, 0.707] or [-0.707, -0.707] (normalized)\n  Expected dominant eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\n\n[Actual Results]\n  Dominant eigenvalue: 3.00 (Expected: 3.0, largest eigenvalue)\n  Iterations: 2\n  Status: OK\n  Dominant eigenvector:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n        0.71       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (3.0): 0.00 [PASS]\n\n[Test 9.2.2] 3x3 Stiffness Matrix (SHM Application)\nStiffness Matrix:\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00       |\n       -1.00         2.00        -1.00       |\n        0.00        -1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\n  Expected primary frequency: sqrt(3.414) \u2248 1.848 rad/s\n\n[Actual Results]\n  Dominant eigenvalue (primary frequency squared): 3.41\n  Primary frequency: 1.85 rad/s (Expected: ~1.848 rad/s)\n  Iterations: 8\n  Status: OK\n  Error from expected (3.41): 0.00 [PASS]\n\n[Test 9.2.3] Non-Square Matrix (Expect Error)\n[Error] Power iteration requires a square matrix.\nStatus: Error (Expected)\n\n[Test 9.2.4] inverse_power_iteration() - Smallest Eigenvalue (System Identification)\n\n[Test 9.2.4.1] Simple 2x2 Matrix - Smallest Eigenvalue\nMatrix (same as Test 9.2.1):\nMatrix Elements &gt;&gt;&gt;\n        2.00         1.00       |\n        1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 3.0 (largest), 1.0 (smallest)\n  Expected smallest eigenvalue: 1.0\n  Expected smallest eigenvector (for \u03bb=1): approximately [0.707, -0.707] or [-0.707, 0.707] (normalized)\n  Note: This is critical for system identification - smallest eigenvalue = fundamental frequency\n\n[Actual Results]\n  Smallest eigenvalue: 1.00 (Expected: 1.0, smallest eigenvalue)\n  Iterations: 6\n  Status: OK\n  Smallest eigenvector:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (1.0): 0.00 [PASS]\n\n[Comparison] Power vs Inverse Power Iteration:\n  Power iteration (\u03bb_max): 3.00\n  Inverse power iteration (\u03bb_min): 1.00\n  Ratio (\u03bb_max/\u03bb_min): 3.00 (Expected: ~3.0) [PASS]\n\n[Test 9.2.4.2] 3x3 Stiffness Matrix - Smallest Eigenvalue (SHM Application)\nStiffness Matrix (same as Test 9.2.2):\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00       |\n       -1.00         2.00        -1.00       |\n        0.00        -1.00         2.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414 (largest), 2.000, 0.586 (smallest)\n  Expected smallest eigenvalue: ~0.586 (fundamental frequency squared)\n  Expected fundamental frequency: sqrt(0.586) \u2248 0.765 rad/s\n  Note: Smallest eigenvalue is critical for system identification - represents fundamental mode\n\n[Actual Results]\n  Smallest eigenvalue (fundamental frequency squared): 0.59\n  Fundamental frequency: 0.77 rad/s (Expected: ~0.765 rad/s)\n  Iterations: 8\n  Status: OK\n  Smallest eigenvector (fundamental mode shape):\nMatrix Elements &gt;&gt;&gt;\n        0.50       |\n        0.71       |\n        0.50       |\n&lt;&lt;&lt; Matrix Elements\n\n  Error from expected (0.59): 0.00 [PASS]\n\n[Comparison] Power vs Inverse Power Iteration for SHM:\n  Power iteration (primary frequency\u00b2): 3.41 \u2192 frequency: 1.85 rad/s\n  Inverse power iteration (fundamental frequency\u00b2): 0.59 \u2192 frequency: 0.77 rad/s\n  Frequency ratio: 2.41 (Expected: ~2.4, ratio of highest to lowest mode)\n\n[Test 9.2.4.3] Non-Square Matrix (Expect Error)\n[Error] Inverse power iteration requires a square matrix.\nStatus: Error (Expected)\nError handling: [PASS]\n\n[Test 9.2.4.4] Near-Singular Matrix (Edge Case)\nMatrix (near-singular but invertible):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Results]\n  Status: OK\n  Smallest eigenvalue: 1.00\n  Iterations: 2\n  Note: Successfully handled near-singular matrix [PASS]\n\n[Test 9.3] eigendecompose_jacobi() - Symmetric Matrix Decomposition\n\n[Test 9.3.1] 2x2 Symmetric Matrix - Complete Decomposition\n[Expected Results]\n  Expected eigenvalues: 3.0, 1.0 (in any order)\n  Expected eigenvectors (for \u03bb=3): [0.707, 0.707] or [-0.707, -0.707] (normalized)\n  Expected eigenvectors (for \u03bb=1): [0.707, -0.707] or [-0.707, 0.707] (normalized)\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n        3.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors (each column is an eigenvector):\nMatrix Elements &gt;&gt;&gt;\n        0.71         0.71       |\n       -0.71         0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 2\nStatus: OK\nEigenvalue check (should be 3.0 and 1.0): [PASS]\n\n[Verification] Check A * v = lambda * v for first eigenvector:\nA * v:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nlambda * v:\nMatrix Elements &gt;&gt;&gt;\n        0.71       |\n       -0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nVerification (A*v = \u03bb*v): [PASS]\n\n[Test 9.3.2] 3x3 Stiffness Matrix (SHM Application)\n[Expected Results]\n  Expected eigenvalues (approximate): 3.414, 2.000, 0.586\n  Expected natural frequencies: 1.848, 1.414, 0.765 rad/s\n  Note: Eigenvalues may appear in any order\n\n[Actual Results]\nEigenvalues (natural frequencies squared):\nMatrix Elements &gt;&gt;&gt;\n        3.41       |\n        0.59       |\n        2.00       |\n&lt;&lt;&lt; Matrix Elements\n\nNatural frequencies (rad/s):\n  Mode 0: 1.85 rad/s (Expected: ~1.85 rad/s) [PASS]\n  Mode 1: 0.77 rad/s (Expected: ~0.76 rad/s) [PASS]\n  Mode 2: 1.41 rad/s (Expected: ~1.41 rad/s) [PASS]\nEigenvectors (mode shapes):\nMatrix Elements &gt;&gt;&gt;\n        0.50         0.50        -0.71       |\n       -0.71         0.71         0.00       |\n        0.50         0.50         0.71       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 9\nStatus: OK\n\n[Test 9.3.3] Diagonal Matrix (Eigenvalues on diagonal)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        5.00         0.00         0.00       |\n        0.00         3.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 5.0, 3.0, 1.0 (diagonal elements, may be in any order)\n  Expected eigenvectors: standard basis vectors [1,0,0], [0,1,0], [0,0,1] (or their negatives)\n  Expected iterations: 1 (diagonal matrix should converge immediately)\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        5.00       |\n        3.00       |\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 1 (Expected: 1)\nEigenvalue check (should be 5.0, 3.0, 1.0): [PASS]\n\n[Test 9.4] eigendecompose_qr() - General Matrix Decomposition\n\n[Test 9.4.1] General 2x2 Matrix\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        1.00         2.00       |\n        3.00         4.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: (5+\u221a33)/2 \u2248 5.372, (5-\u221a33)/2 \u2248 -0.372\n  Note: This is a non-symmetric matrix, eigenvalues are real but may have complex eigenvectors\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        5.37       |\n       -0.37       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        0.42         0.91       |\n        0.91        -0.42       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nEigenvalue 1: 5.37 (Expected: 5.37, Error: 0.00, Rel Error: 0.01%) [PASS]\nEigenvalue 2: -0.37 (Expected: -0.37, Error: 0.00, Rel Error: 0.07%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.4.2] Non-Symmetric 3x3 Matrix\nMatrix [1,2,3; 4,5,6; 7,8,9]:\nMatrix Elements &gt;&gt;&gt;\n        1.00         2.00         3.00       |\n        4.00         5.00         6.00       |\n        7.00         8.00         9.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\n  Note: This matrix is rank-deficient (determinant = 0), so one eigenvalue is 0\n  Note: QR algorithm may have numerical errors, especially for non-symmetric matrices\n  Acceptable range: largest eigenvalue ~15-18, smallest eigenvalue near 0\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n       16.12       |\n       -1.12       |\n        0.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        0.23         0.88         0.41       |\n        0.53         0.24        -0.82       |\n        0.82        -0.40         0.41       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nEigenvalue 0: 16.12 (Expected: 16.12, Error: 0.00, Rel Error: 0.02%) [PASS]\nEigenvalue 1: -1.12 (Expected: -1.12, Error: 0.00, Rel Error: 0.28%) [PASS]\nEigenvalue 2: 0.00 (Expected: 0.00, Error: 0.00, Rel Error: 0.00%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.5] eigendecompose() - Automatic Method Selection\n\n[Test 9.5.1] Symmetric Matrix (Auto-select: Jacobi)\nMatrix:\nMatrix Elements &gt;&gt;&gt;\n        4.00         1.00         2.00       |\n        1.00         3.00         0.00       |\n        2.00         0.00         5.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Method: Should automatically use Jacobi (symmetric matrix detected)\n  Expected eigenvalues (approximate): 6.67, 3.48, 1.85\n  Note: Eigenvalues may appear in any order\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n        1.85       |\n        3.48       |\n        6.67       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 8\nStatus: OK\nMethod used: Jacobi (auto-selected for symmetric matrix)\n\n[Test 9.5.2] Non-Symmetric Matrix (Auto-select: QR)\n[Expected Results]\n  Method: Should automatically use QR (non-symmetric matrix detected)\n  Expected eigenvalues (theoretical): 16.12, -1.12, 0.00\n  Note: One eigenvalue should be near 0 (rank-deficient matrix)\n  Note: QR algorithm may have numerical errors for non-symmetric matrices\n  Acceptable: largest ~15-18, smallest near 0, one near -1 to -3\n\n[Actual Results]\nEigenvalues:\nMatrix Elements &gt;&gt;&gt;\n       16.12       |\n       -1.12       |\n        0.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 6\nStatus: OK\nMethod used: QR (auto-selected for non-symmetric matrix)\nEigenvalue 0: 16.12 (Expected: 16.12, Error: 0.00, Rel Error: 0.02%) [PASS]\nEigenvalue 1: -1.12 (Expected: -1.12, Error: 0.00, Rel Error: 0.28%) [PASS]\nEigenvalue 2: 0.00 (Expected: 0.00, Error: 0.00, Rel Error: 0.00%) [PASS]\nOverall eigenvalue check: [PASS]\n\n[Test 9.6] SHM Application - Structural Dynamics Analysis\n\n[Test 9.6.1] 4-DOF Mass-Spring System\nStiffness Matrix K:\nMatrix Elements &gt;&gt;&gt;\n        2.00        -1.00         0.00         0.00       |\n       -1.00         2.00        -1.00         0.00       |\n        0.00        -1.00         2.00        -1.00       |\n        0.00         0.00        -1.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIs symmetric: Yes\n\n[Quick Analysis] Primary frequency using power_iteration():\n[Expected Results]\n  Expected primary eigenvalue: ~3.53 (largest eigenvalue)\n  Expected primary frequency: sqrt(3.53) \u2248 1.88 rad/s\n\n[Actual Results]\n  Primary eigenvalue: 3.53 (Expected: ~3.53)\n  Primary frequency: 1.88 rad/s (Expected: ~1.88 rad/s)\n  Iterations: 13\n  Error from expected: 0.00 [PASS]\n\n[Complete Analysis] Full modal analysis using eigendecompose_jacobi():\n[Expected Results]\n  Expected eigenvalues (approximate): 3.53, 2.35, 1.00, 0.12\n  Expected natural frequencies: 1.88, 1.53, 1.00, 0.35 rad/s\n  Note: These are approximate values for the 4-DOF system\n\n[Actual Results]\nAll eigenvalues (natural frequencies squared):\nMatrix Elements &gt;&gt;&gt;\n        3.53       |\n        1.00       |\n        2.35       |\n        0.12       |\n&lt;&lt;&lt; Matrix Elements\n\nNatural frequencies (rad/s):\n  Mode 0: 1.88 rad/s (Expected: ~1.88 rad/s) [PASS]\n  Mode 1: 1.00 rad/s (Expected: ~1.00 rad/s) [PASS]\n  Mode 2: 1.53 rad/s (Expected: ~1.53 rad/s) [PASS]\n  Mode 3: 0.35 rad/s (Expected: ~0.35 rad/s) [PASS]\nMode shapes (eigenvectors):\nMatrix Elements &gt;&gt;&gt;\n        0.43         0.58        -0.66         0.23       |\n       -0.66         0.58         0.23         0.43       |\n        0.58        -0.00         0.58         0.58       |\n       -0.23        -0.58        -0.43         0.66       |\n&lt;&lt;&lt; Matrix Elements\n\nTotal iterations: 17\n\n[Test 9.7] Edge Cases and Error Handling\n\n[Test 9.7.1] 1x1 Matrix\nMatrix: [5.0]\n[Expected Results]\n  Expected eigenvalue: 5.0 (the matrix element itself)\n  Expected eigenvector: [1.0] (normalized)\n\n[Actual Results]\nEigenvalue: 5.00 (Expected: 5.0)\nEigenvector:\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nError from expected: 0.00 [PASS]\n\n[Test 9.7.2] Zero Matrix\n[Error] Power iteration: computed vector norm too small.\nStatus: Error (Expected)\n\n[Test 9.7.3] Identity Matrix\nMatrix (3x3 Identity):\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Expected Results]\n  Expected eigenvalues: 1.0, 1.0, 1.0 (all eigenvalues are 1)\n  Expected eigenvectors: Any orthonormal basis (e.g., standard basis vectors)\n  Expected iterations: 1 (should converge immediately)\n\n[Actual Results]\nEigenvalues (should all be 1.0):\nMatrix Elements &gt;&gt;&gt;\n        1.00       |\n        1.00       |\n        1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nEigenvectors:\nMatrix Elements &gt;&gt;&gt;\n        1.00         0.00         0.00       |\n        0.00         1.00         0.00       |\n        0.00         0.00         1.00       |\n&lt;&lt;&lt; Matrix Elements\n\nIterations: 1 (Expected: 1)\nAll eigenvalues = 1.0: [PASS]\n\n[Test 9.8] Performance Test for SHM Applications\n\n[Test 9.8.1] Power Iteration Performance (Real-time SHM - Dominant Eigenvalue)\n[Performance] Power Iteration (3x3 matrix): 101.00 us\n\n[Test 9.8.2] Inverse Power Iteration Performance (System Identification - Smallest Eigenvalue)\n[Performance] Inverse Power Iteration (3x3 matrix): 495.00 us\n\n[Test 9.8.3] Jacobi Method Performance (Complete Eigendecomposition - Symmetric Matrices)\n[Performance] Jacobi Decomposition (3x3 symmetric matrix): 129.00 us\n\n[Test 9.8.4] QR Method Performance (Complete Eigendecomposition - General Matrices)\n[Performance] QR Decomposition (3x3 general matrix): 760.00 us\n\n[Eigenvalue Decomposition Tests Complete]\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-vii-10-11","title":"PHASE VII: \u8f85\u52a9\u51fd\u6570 (\u7ec4 10-11)","text":"<p>\u76ee\u7684\uff1a\u4fbf\u5229\u51fd\u6570\u548c I/O \u64cd\u4f5c</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 10: Auxiliary Functions - Stream Operators Tests]\n\n[Test 10.1] Stream Insertion Operator (&lt;&lt;) for Mat\nMatrix mat1:\n1 2 3\n4 5 6\n7 8 9\n\n\n[Test 10.2] Stream Insertion Operator (&lt;&lt;) for Mat::ROI\nROI created: ROI(pos_x=1, pos_y=2, width=3, height=4)\nExpected output:\n  row start: 2 (pos_y)\n  col start: 1 (pos_x)\n  row count: 4 (height)\n  col count: 3 (width)\n\nActual output:\nrow start 2\ncol start 1\nrow count 4\ncol count 3\n\n\n[Test 10.3] Stream Extraction Operator (&gt;&gt;) for Mat\nSimulated input: \"10 20 30 40\"\nMatrix mat2 after input:\n10 20\n30 40\n\nExpected: [10, 20; 30, 40]\n\n[Test 10.4] Stream Extraction Operator (&gt;&gt;) for Mat (2x3 matrix)\nSimulated input: \"1.5 2.5 3.5 4.5 5.5 6.5\"\nMatrix mat3 after input:\n1.5 2.5 3.5\n4.5 5.5 6.5\n\nExpected: [1.5, 2.5, 3.5; 4.5, 5.5, 6.5]\n\n[Group 11: Auxiliary Functions - Global Arithmetic Operators Tests]\n\n[Test 11.1] Matrix Addition (operator+)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA + matB:\n6 8\n10 12\n\n\n[Test 11.2] Matrix Addition with Constant (operator+)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 5.0\nmatA + 5.0f:\n6 7\n8 9\n\n\n[Test 11.3] Matrix Subtraction (operator-)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA - matB:\n-4 -4\n-4 -4\n\n\n[Test 11.4] Matrix Subtraction with Constant (operator-)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA - 2.0f:\n-1 0\n1 2\n\n\n[Test 11.5] Matrix Multiplication (operator*)\nMatrix C (2x3):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix D (3x2):\nMatrix Elements &gt;&gt;&gt;\n           7            8       |\n           9           10       |\n          11           12       |\n&lt;&lt;&lt; Matrix Elements\n\nmatC * matD:\n58 64\n139 154\n\n\n[Test 11.6] Matrix Multiplication with Constant (operator*)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA * 2.0f:\n2 4\n6 8\n\n\n[Test 11.7] Matrix Division (operator/)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nConstant: 2.0\nmatA / 2.0f:\n0.5 1\n1.5 2\n\n\n[Test 11.8] Matrix Division Element-wise (operator/)\nMatrix A:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix B:\nMatrix Elements &gt;&gt;&gt;\n           5            6       |\n           7            8       |\n&lt;&lt;&lt; Matrix Elements\n\nmatA / matB:\n0.2 0.333333\n0.428571 0.5\n\n\n[Test 11.9] Matrix Comparison (operator==)\nMatrix E:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix F:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nmatE == matF: True\n\nAfter modifying matF(0,0) = 5:\nMatrix E:\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\nMatrix F:\nMatrix Elements &gt;&gt;&gt;\n           5            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\noperator == Error: 0 0, m1.data=1, m2.data=5, diff=4\nmatE == matF after modification: False\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/MATRIX/tiny-matrix-test/#phase-viii-12-14","title":"PHASE VIII: \u8d28\u91cf\u4fdd\u8bc1 (\u7ec4 12-14)","text":"<p>\u76ee\u7684\uff1a\u786e\u4fdd\u9c81\u68d2\u6027\u3001\u6027\u80fd\u548c\u6b63\u786e\u6027</p> <pre><code>============ [tiny_matrix_test start] ============\n\n[Test Organization: Application-Oriented Logic]\n  Foundation \u2192 Basic Ops \u2192 Properties \u2192 Linear Systems \u2192 Decompositions \u2192 Applications \u2192 Quality\n\n\n[Group 12: Quality Assurance - Boundary Conditions and Error Handling Tests]\n\n[Test 12.1] Null Pointer Handling in print_matrix\n[Error] Cannot print matrix: data pointer is null.\n\n[Test 12.2] Null Pointer Handling in operator&lt;&lt;\n[Error] Cannot print matrix: data pointer is null.\n\n\n[Test 12.3] Invalid Block Parameters\n[Error] Invalid block parameters: negative start position or non-positive block size.\nblock(-1, 0, 2, 2): Error\n[Error] Block exceeds matrix boundaries.\nblock(2, 2, 2, 2) on 3x3 matrix: Error\n[Error] Invalid block parameters: negative start position or non-positive block size.\nblock(0, 0, 0, 2): Error\n\n[Test 12.4] Invalid swap_rows Parameters\nBefore invalid swap_rows:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: row index out of range\nAfter swap_rows(-1, 1):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: row index out of range\nAfter swap_rows(0, 5):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.5] Invalid swap_cols Parameters\nBefore invalid swap_cols:\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: column index out of range\nAfter swap_cols(-1, 1):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\nError: column index out of range\nAfter swap_cols(0, 5):\nMatrix Elements &gt;&gt;&gt;\n           1            2            3       |\n           4            5            6       |\n           7            8            9       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.6] Division by Zero\n[Error] Division by zero in operator/.\nmat3 / 0.0f: Error\n\n[Test 12.7] Matrix Division with Zero Elements\n[Error] Matrix division failed: Division by zero detected.\nmat4 /= divisor (with zero):\nMatrix Elements &gt;&gt;&gt;\n           1            2       |\n           3            4       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 12.8] Empty Matrix Operations\nEmpty matrix addition: Error\n\n[Group 13: Quality Assurance - Performance Benchmarks Tests]\n\n[Test 13.1] Matrix Addition Performance\n[Performance] 50x50 Matrix Addition (100 iterations): 15962.00 us total, 159.62 us avg\n\n[Test 13.2] Matrix Multiplication Performance\n[Performance] 30x30 Matrix Multiplication (100 iterations): 66056.00 us total, 660.56 us avg\n\n[Test 13.3] Matrix Transpose Performance\n[Performance] 50x30 Matrix Transpose (100 iterations): 17898.00 us total, 178.98 us avg\n\n[Test 13.4] Determinant Calculation Performance Comparison\n\n[Test 13.4.1] Small Matrix (4x4) - Laplace Expansion\n[Performance] 4x4 Determinant (Laplace, 10 iterations): 2883.00 us total, 288.30 us avg\n\n[Test 13.4.2] Large Matrix (8x8) - LU Decomposition\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Performance] 8x8 Determinant (LU, 10 iterations): 55564.00 us total, 5556.40 us avg\n\n[Test 13.4.3] Large Matrix (8x8) - Gaussian Elimination\n[Performance] 8x8 Determinant (Gaussian, 10 iterations): 349.00 us total, 34.90 us avg\n\n[Test 13.4.4] Large Matrix (8x8) - Auto-select Method\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Error] LU decomposition: Matrix is singular or near-singular.\n[Performance] 8x8 Determinant (auto-select, 10 iterations): 55582.00 us total, 5558.20 us avg\n\n[Note] Performance Summary:\n  - Laplace expansion (O(n!)): Suitable only for small matrices (n &lt;= 4)\n  - LU decomposition (O(n\u00b3)): Efficient for large matrices, auto-selected for n &gt; 4\n  - Gaussian elimination (O(n\u00b3)): Alternative efficient method for large matrices\n  - Auto-select: Automatically chooses the best method based on matrix size\n\n[Test 13.5] Matrix Copy with Padding Performance\n[Performance] 8x8 Copy ROI (with padding) (100 iterations): 2342.00 us total, 23.42 us avg\n\n[Test 13.6] Element Access Performance\n[Performance] Computing element access (warmup)...\n[Performance] 50x50 Element Access (all elements) (100 iterations): 9706.00 us total, 97.06 us avg\n\n[Group 14: Quality Assurance - Memory Layout Tests (Padding and Stride)]\n\n[Test 14.1] Contiguous Memory (no padding)\nMatrix 3x4 (stride=4, pad=0):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9af0\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |\n        4.00         5.00         6.00         7.00       |\n        8.00         9.00        10.00        11.00       |\n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.2] Padded Memory (stride &gt; col)\nMatrix 3x4 (stride=5, pad=1):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fc991e4\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |   0.00 \n        4.00         5.00         6.00         7.00       |   0.00 \n        8.00         9.00        10.00        11.00       |   0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.3] Addition with Padded Matrices\nResult of padded matrix addition:\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        1\nstride          5\nmemory          15\ndata pointer    0x3fce9c64\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n       11.00        22.00        33.00        44.00       |   0.00 \n       55.00        66.00        77.00        88.00       |   0.00 \n       99.00       110.00       121.00       132.00       |   0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.4] ROI Operations with Padded Matrices\nROI (1,1,2,2) from padded matrix:\nMatrix Info &gt;&gt;&gt;\nrows            2\ncols            2\nelements        4\npaddings        3\nstride          5\nmemory          10\ndata pointer    0x3fc991fc\ntemp pointer    0\next_buff        1   (External buffer or View)\nsub_matrix      1   (This is a Sub-Matrix View)\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        5.00         6.00       |   7.00         0.00         8.00 \n        9.00        10.00       |  11.00         0.00         0.00 \n&lt;&lt;&lt; Matrix Elements\n\n\n[Test 14.5] Copy Operations Preserve Stride\nCopied matrix (should have stride=4, no padding):\nMatrix Info &gt;&gt;&gt;\nrows            3\ncols            4\nelements        12\npaddings        0\nstride          4\nmemory          12\ndata pointer    0x3fce9d98\ntemp pointer    0\next_buff        0\nsub_matrix      0\n&lt;&lt;&lt; Matrix Info\nMatrix Elements &gt;&gt;&gt;\n        0.00         1.00         2.00         3.00       |\n        4.00         5.00         6.00         7.00       |\n        8.00         9.00        10.00        11.00       |\n&lt;&lt;&lt; Matrix Elements\n\n============ [tiny_matrix_test end] ============\n</code></pre>"},{"location":"zh/MATH/USAGE/usage/","title":"\u4f7f\u7528\u8bf4\u660e","text":"<p>\u4f7f\u7528\u8bf4\u660e</p> <p>\u8be5\u6587\u6863\u662f\u5bf9 <code>tiny_math</code> \u6a21\u5757\u7684\u4f7f\u7528\u8bf4\u660e\u3002</p>"},{"location":"zh/MATH/USAGE/usage/#tinymath","title":"\u6574\u4f53\u5f15\u5165TinyMath","text":"<p>Info</p> <p>\u9002\u7528\u4e8eC\u9879\u76ee\uff0c\u6216\u8005\u7ed3\u6784\u8f83\u4e3a\u7b80\u5355\u7684C++\u9879\u76ee\u3002</p> <pre><code>#include \"tiny_math.h\"\n</code></pre>"},{"location":"zh/MATH/USAGE/usage/#tinymath_1","title":"\u5206\u6a21\u5757\u5f15\u5165TinyMath","text":"<p>Info</p> <p>\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u63a7\u5236\u5f15\u5165\u6a21\u5757\u7684\u9879\u76ee\uff0c\u6216\u8005\u590d\u6742\u7684C++\u9879\u76ee\u3002</p> <pre><code>#include \"tiny_vec.h\" // \u5f15\u5165\u5411\u91cf\u6a21\u5757\n#include \"tiny_mat.h\" // \u5f15\u5165\u77e9\u9635\u6a21\u5757\n</code></pre> <pre><code>#include \"tiny_matrix.hpp\" // \u5f15\u5165\u9ad8\u7ea7\u77e9\u9635\u6a21\u5757\n</code></pre> <p>\u6ce8\u610f</p> <ul> <li> <p><code>tiny_vec.h</code> \u548c <code>tiny_mat.h</code> \u662f C \u8bed\u8a00\u7248\u672c\u7684\u5934\u6587\u4ef6\uff0c\u9002\u7528\u4e8e C \u8bed\u8a00\u7f16\u7a0b\u3002</p> </li> <li> <p><code>tiny_matrix.hpp</code> \u662f C++ \u8bed\u8a00\u7248\u672c\u7684\u5934\u6587\u4ef6\uff0c\u9002\u7528\u4e8e C++ \u8bed\u8a00\u7f16\u7a0b\u3002</p> </li> </ul> <p>\u7b80\u5355\u6765\u8bf4\uff0cC\u8bed\u8a00\u9879\u76ee\u53ea\u80fd\u7528 <code>tiny_vec.h</code> \u548c <code>tiny_mat.h</code>\uff0c\u800c C++ \u9879\u76ee\u53ef\u4ee5\u4f7f\u7528 <code>tiny_vec.h</code>\u3001<code>tiny_mat.h</code> \u548c <code>tiny_matrix.hpp</code>\u3002</p> <p>Tip</p> <p>\u5177\u4f53\u7684\u4f7f\u7528\u65b9\u6cd5\u8bf7\u53c2\u8003\u6d4b\u8bd5\u4ee3\u7801\u3002</p>"},{"location":"zh/MATH/VECTOR/api/","title":"\u5411\u91cf\u64cd\u4f5c","text":""},{"location":"zh/MATH/VECTOR/api/#_2","title":"\u76ee\u5f55","text":"<pre><code>// Addition\ntiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Subtraction\ntiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Multiplication\ntiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\ntiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n// Division\ntiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero);\ntiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero);\n// Square root\ntiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len);\ntiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len);\n// Dot product\ntiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len);\ntiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2);\n</code></pre>"},{"location":"zh/MATH/VECTOR/api/#_3","title":"\u52a0\u6cd5","text":""},{"location":"zh/MATH/VECTOR/api/#_4","title":"\u4e24\u4e2a\u5411\u91cf\u7684\u52a0\u6cd5","text":"<p><pre><code>tiny_error_t tiny_vec_add_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> \u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u9010\u5143\u7d20\u52a0\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>input2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>step1</code>\uff1a\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step2</code>\uff1a\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_5","title":"\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u52a0\u6cd5","text":"<p><pre><code>tiny_error_t tiny_vec_addc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> \u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u9010\u5143\u7d20\u52a0\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>C</code>\uff1a\u5e38\u6570\u503c\u3002</li> <li><code>step_in</code>\uff1a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_6","title":"\u51cf\u6cd5","text":""},{"location":"zh/MATH/VECTOR/api/#_7","title":"\u4e24\u4e2a\u5411\u91cf\u7684\u51cf\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_sub_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u9010\u5143\u7d20\u51cf\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>input2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>step1</code>\uff1a\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step2</code>\uff1a\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_8","title":"\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u51cf\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_subc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u9010\u5143\u7d20\u51cf\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>C</code>\uff1a\u5e38\u6570\u503c\u3002</li> <li><code>step_in</code>\uff1a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_9","title":"\u4e58\u6cd5","text":""},{"location":"zh/MATH/VECTOR/api/#_10","title":"\u4e24\u4e2a\u5411\u91cf\u7684\u4e58\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_mul_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u9010\u5143\u7d20\u4e58\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>input2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>step1</code>\uff1a\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step2</code>\uff1a\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_11","title":"\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u4e58\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_mulc_f32(const float *input, float *output, int len, float C, int step_in, int step_out);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u9010\u5143\u7d20\u4e58\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>C</code>\uff1a\u5e38\u6570\u503c\u3002</li> <li><code>step_in</code>\uff1a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_12","title":"\u9664\u6cd5","text":""},{"location":"zh/MATH/VECTOR/api/#_13","title":"\u4e24\u4e2a\u5411\u91cf\u7684\u9664\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_div_f32(const float *input1, const float *input2, float *output, int len, int step1, int step2, int step_out, bool allow_divide_by_zero);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u9010\u5143\u7d20\u9664\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>input2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>step1</code>\uff1a\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step2</code>\uff1a\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>allow_divide_by_zero</code>\uff1a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u5141\u8bb8\u9664\u4ee5\u96f6\u7684\u64cd\u4f5c\u3002</li> </ul>"},{"location":"zh/MATH/VECTOR/api/#_14","title":"\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u9664\u6cd5","text":"<pre><code>tiny_error_t tiny_vec_divc_f32(const float *input, float *output, int len, float C, int step_in, int step_out, bool allow_divide_by_zero);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u4e0e\u5e38\u6570\u7684\u9010\u5143\u7d20\u9664\u6cd5\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>C</code>\uff1a\u5e38\u6570\u503c\u3002</li> <li><code>step_in</code>\uff1a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step_out</code>\uff1a\u8f93\u51fa\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>allow_divide_by_zero</code>\uff1a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u5141\u8bb8\u9664\u4ee5\u96f6\u7684\u64cd\u4f5c\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_15","title":"\u5e73\u65b9\u6839","text":""},{"location":"zh/MATH/VECTOR/api/#_16","title":"\u5411\u91cf\u7684\u5e73\u65b9\u6839","text":"<pre><code>tiny_error_t tiny_vec_sqrt_f32(const float *input, float *output, int len);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u7684\u9010\u5143\u7d20\u5e73\u65b9\u6839\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_17","title":"\u5411\u91cf\u7684\u5e73\u65b9\u6839\uff08\u5feb\u901f\uff09","text":"<pre><code>tiny_error_t tiny_vec_sqrtf_f32(const float *input, float *output, int len);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u7684\u9010\u5143\u7d20\u5e73\u65b9\u6839\uff08\u5feb\u901f\u7248\u672c\uff09\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_18","title":"\u5411\u91cf\u7684\u5e73\u65b9\u6839\u5012\u6570","text":"<pre><code>tiny_error_t tiny_vec_inv_sqrt_f32(const float *input, float *output, int len);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u7684\u9010\u5143\u7d20\u5e73\u65b9\u6839\u5012\u6570\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_19","title":"\u5411\u91cf\u7684\u5e73\u65b9\u6839\u5012\u6570\uff08\u5feb\u901f\uff09","text":"<pre><code>tiny_error_t tiny_vec_inv_sqrtf_f32(const float *input, float *output, int len);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u5411\u91cf\u7684\u9010\u5143\u7d20\u5e73\u65b9\u6839\u5012\u6570\uff08\u5feb\u901f\u7248\u672c\uff09\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>input</code>\uff1a\u6307\u5411\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>output</code>\uff1a\u6307\u5411\u8f93\u51fa\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_20","title":"\u70b9\u79ef","text":""},{"location":"zh/MATH/VECTOR/api/#_21","title":"\u5411\u91cf\u7684\u70b9\u79ef","text":"<pre><code>tiny_error_t tiny_vec_dotprod_f32(const float *src1, const float *src2, float *dest, int len);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u70b9\u79ef\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>src1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>src2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>dest</code>\uff1a\u6307\u5411\u8f93\u51fa\u7ed3\u679c\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/api/#_22","title":"\u5411\u91cf\u7684\u70b9\u79ef\uff08\u5e26\u6b65\u957f\uff09","text":"<pre><code>tiny_error_t tiny_vec_dotprode_f32(const float *src1, const float *src2, float *dest, int len, int step1, int step2);\n</code></pre> <p>\u529f\u80fd\uff1a \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u70b9\u79ef\uff08\u5e26\u6b65\u957f\uff09\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>src1</code>\uff1a\u6307\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>src2</code>\uff1a\u6307\u5411\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6307\u9488\u3002</li> <li><code>dest</code>\uff1a\u6307\u5411\u8f93\u51fa\u7ed3\u679c\u7684\u6307\u9488\u3002</li> <li><code>len</code>\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u3002</li> <li><code>step1</code>\uff1a\u7b2c\u4e00\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> <li><code>step2</code>\uff1a\u7b2c\u4e8c\u4e2a\u8f93\u5165\u5411\u91cf\u7684\u6b65\u957f\u3002</li> </ul> <p>\u8fd4\u56de\u503c\uff1a \u8fd4\u56de <code>tiny_error_t</code> \u7c7b\u578b\u7684\u9519\u8bef\u7801\uff0c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u6210\u529f\u3002</p>"},{"location":"zh/MATH/VECTOR/code/","title":"\u4ee3\u7801","text":""},{"location":"zh/MATH/VECTOR/test/","title":"\u5411\u91cf\u64cd\u4f5c\u6d4b\u8bd5","text":"<p>\u5411\u91cf\u64cd\u4f5c\u6d4b\u8bd5</p> <p>\u8be5\u6d4b\u8bd5\u7528\u4e8e\u6d4b\u8bd5\u5411\u91cf\u76f8\u5173\u51fd\u6570\u7684\u6027\u80fd\u3002</p>"},{"location":"zh/MATH/VECTOR/test/#_2","title":"\u6d4b\u8bd5\u4ee3\u7801","text":""},{"location":"zh/MATH/VECTOR/test/#_3","title":"\u6d4b\u8bd5\u7ed3\u679c","text":"<p>\u57fa\u7840C\u8ba1\u7b97\u60c5\u51b5</p> <p></p> <p>ESP-DSP\u52a0\u901f\u60c5\u51b5</p> <p></p> <p>\u53ef\u4ee5\u53d1\u73b0\u5728\u5f00\u542fESP-DSP\u52a0\u901f\u540e\uff0c\u5411\u91cf\u8fd0\u7b97\u7684\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002</p>"},{"location":"zh/PREREQUISITE/prerequisite/","title":"\u524d\u7f6e\u6761\u4ef6","text":""},{"location":"zh/PREREQUISITE/prerequisite/#_2","title":"\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u8981\u6c42","text":"<p>ESP32S3 \u5f00\u53d1\u677f, \u63a8\u8350\u53c2\u8003\u4ee5\u4e0b\u9879\u76ee:</p> <ul> <li> <p> NexNode</p> <p>  \u4ee3\u7801 </p> <p>  \u6587\u6863 </p> </li> </ul> <p>\u6211\u4eec\u4ee5\u8be5\u9879\u76ee\u4e2d\u7684\u4ee3\u7801\u4e3a\u57fa\u7840\u8fdb\u884c\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002</p>"},{"location":"zh/PREREQUISITE/prerequisite/#_3","title":"\u4f9d\u8d56\u7ec4\u4ef6","text":"<p>\u4e3a\u4e86\u63d0\u5347\u6211\u4eec\u6846\u67b6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165ESP-DSP\u5e93\u548cESP-DL\u5e93\uff0c\u5b83\u4eec\u5206\u522b\u63d0\u4f9b\u4e86\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\u9ad8\u6548\u5b9e\u73b0\u3002</p> <p>Tip</p> <p>\u6ce8\u610f\u4ee5\u4e0a\u4e24\u4e2a\u5e93\u4f3c\u4e4e\u662f\u7531\u4e0d\u540c\u56e2\u961f\u5f00\u53d1\uff0c\u56e0\u6b64\u4ed6\u4eec\u7684\u5f88\u591a\u529f\u80fd\u6709\u91cd\u53e0\u3002</p> <pre><code>- espressif__esp-dsp\n- espressif__esp-dl\n   - espressif__dl_fft\n   - espressif__esp_new_jpeg\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u5728ESP-REGISTRY\u4e2d\u627e\u5230\u548c\u4e0b\u8f7d\u8fd9\u4e9b\u7ec4\u4ef6\u5230\u9879\u76ee\u4e2d\u3002\u5728\u672c\u9879\u76ee\u4e2d\u6211\u5c06\u4e0b\u8f7d\u7684\u7ec4\u4ef6\u53ca\u5176\u4f9d\u8d56\u7ec4\u4ef6\u79fb\u52a8\u5230\u4e86<code>middleware</code>\u6587\u4ef6\u5939\u4e0b\uff0c\u5e76\u79fb\u9664\u4e86\u914d\u7f6e\u6587\u4ef6\uff0c\u4ece\u800c\u907f\u514d\u7248\u672c\u9501\u5b9a\u548c\u7f51\u7edc\u4f9d\u8d56\u3002</p>"},{"location":"zh/TOOLBOX/toolbox/","title":"\u5de5\u5177\u7bb1","text":"<p>tiny_toolbox</p> <p>\u5de5\u5177\u7bb1tiny_toolbox\u5b9a\u4f4d\u662f\u7528\u4e8e \u5e73\u53f0\u9002\u914d\u4e0e\u4f18\u5316 \u5e76\u63d0\u4f9b \u5404\u79cd\u5b9e\u7528\u5de5\u5177 \u7684\u5e93\uff0c\u670d\u52a1\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u4e0e\u5e94\u7528\u5f00\u53d1\u3002\u6ce8\u610f\uff0c\u4e4b\u6240\u4ee5\u5c06\u9002\u914d\u548c\u5de5\u5177\u653e\u5728\u4e00\u4e2a\u5e93\u91cc\u9762\uff0c\u662f\u56e0\u4e3a\u5f88\u591a\u5de5\u5177\u5e95\u5c42\u5229\u7528\u7684\u662f\u5e73\u53f0\u63d0\u4f9b\u7684\u529f\u80fd\uff0c\u6240\u4ee5\u5c06\u5e73\u53f0\u9002\u914d\u548c\u5404\u7c7b\u5de5\u5177\u653e\u5728\u540c\u4e00\u4e2a\u5e93\u91cc\u9762\uff0c\u4fbf\u4e8e\u4f7f\u7528\u548c\u7ef4\u62a4\u3002</p> <p>Warning</p> <p>\u76ee\u524d\u5f00\u53d1\u4ee5ESP32\u4e3a\u57fa\u7840\uff0c\u5411STM32\u7b49\u5e73\u53f0\u7684\u8fc1\u79fb\u9700\u8981\u5bf9\u9002\u914d\u5c42\u8fdb\u884c\u4e00\u5b9a\u7684\u4fee\u6539\u3002</p>"},{"location":"zh/TOOLBOX/toolbox/#_2","title":"\u67b6\u6784\u4e0e\u529f\u80fd\u76ee\u5f55","text":"<pre><code>    tiny_toolbox\n    \u251c\u2500\u2500 CMakeLists.txt\n    \u251c\u2500\u2500 tiny_toolbox.h // serves as a directory, integrating all submodules\n    \u251c\u2500\u2500 time\n    \u2502   \u251c\u2500\u2500 tiny_time.h // submodule for time management - header file\n    \u2502   \u251c\u2500\u2500 tiny_time.c // submodule for time management - source file\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"zh/TOOLBOX/toolbox/#_3","title":"\u65f6\u95f4","text":"<ul> <li>\u83b7\u53d6\u8fd0\u884c\u65f6\u95f4\uff1a <code>tiny_get_running_time()</code></li> <li>SNTP\u5bf9\u65f6\uff1a <code>sync_time_with_timezone(\"CST-8\")</code></li> <li>\u83b7\u53d6\u4e16\u754c\u65f6\u95f4\uff1a <code>tiny_get_current_datetime(1)</code></li> </ul> <p>\u5f85\u5f00\u53d1:</p> <ul> <li>\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u672c\u5730\u5bf9\u65f6-\u5fae\u79d2\u7ea7\u522b</li> </ul>"},{"location":"zh/TOOLBOX/toolbox/#_4","title":"\u4ee3\u7801","text":"<p>Tip</p> <p>tiny_toolbox.h \u53ea\u662f\u4f5c\u4e3a\u4e00\u4e2a\u76ee\u5f55\uff0c\u96c6\u6210\u4e86\u6240\u6709\u7684\u5b50\u6a21\u5757\uff0c\u5177\u4f53\u7684\u529f\u80fd\u5728\u5404\u4e2a\u5b50\u6a21\u5757\u4e2d\u5b9e\u73b0\u3002tiny_toolbox.c \u53ea\u662f\u5f62\u5f0f\u4e0a\u7684\u6e90\u6587\u4ef6\uff0c\u6ca1\u6709\u5177\u4f53\u7684\u529f\u80fd\u3002</p>"},{"location":"zh/TOOLBOX/TIME/code/#_1","title":"\u7ed3\u679c","text":"<pre><code>I (25) boot: ESP-IDF v6.0-dev-1833-g758939caec 2nd stage bootloader\nI (25) boot: compile time Nov  4 2025 23:13:16\nI (25) boot: Multicore bootloader\nI (27) boot: chip revision: v0.2\nI (30) boot: efuse block revision: v1.3\nI (33) qio_mode: Enabling default flash chip QIO\nI (38) boot.esp32s3: Boot SPI Speed : 80MHz\nI (41) boot.esp32s3: SPI Mode       : QIO\nI (45) boot.esp32s3: SPI Flash Size : 16MB\nI (49) boot: Enabling RNG early entropy source...\nI (54) boot: Partition Table:\nI (56) boot: ## Label            Usage          Type ST Offset   Length\nI (62) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (69) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (75) boot:  2 factory          factory app      00 00 00010000 001f0000\nI (82) boot:  3 vfs              Unknown data     01 81 00200000 00a00000\nI (89) boot:  4 storage          Unknown data     01 82 00c00000 00400000\nI (95) boot: End of partition table\nI (98) esp_image: segment 0: paddr=00010020 vaddr=3c0b0020 size=1df80h (122752) map\nI (124) esp_image: segment 1: paddr=0002dfa8 vaddr=3fc99300 size=02070h (  8304) load\nI (126) esp_image: segment 2: paddr=00030020 vaddr=42000020 size=a26fch (665340) map\nI (227) esp_image: segment 3: paddr=000d2724 vaddr=3fc9b370 size=030e0h ( 12512) load\nI (229) esp_image: segment 4: paddr=000d580c vaddr=40374000 size=152ech ( 86764) load\nI (247) esp_image: segment 5: paddr=000eab00 vaddr=50000000 size=00020h (    32) load\nI (256) boot: Loaded app from partition at offset 0x10000\nI (256) boot: Disabling RNG early entropy source...\nI (266) octal_psram: vendor id    : 0x0d (AP)\nI (267) octal_psram: dev id       : 0x02 (generation 3)\nI (267) octal_psram: density      : 0x03 (64 Mbit)\nI (269) octal_psram: good-die     : 0x01 (Pass)\nI (273) octal_psram: Latency      : 0x01 (Fixed)\nI (277) octal_psram: VCC          : 0x01 (3V)\nI (281) octal_psram: SRF          : 0x01 (Fast Refresh)\nI (286) octal_psram: BurstType    : 0x01 (Hybrid Wrap)\nI (291) octal_psram: BurstLen     : 0x01 (32 Byte)\nI (296) octal_psram: Readlatency  : 0x02 (10 cycles@Fixed)\nI (301) octal_psram: DriveStrength: 0x00 (1/1)\nI (306) MSPI Timing: PSRAM timing tuning index: 5\nI (310) esp_psram: Found 8MB PSRAM device\nI (313) esp_psram: Speed: 80MHz\nI (316) cpu_start: Multicore app\nI (752) esp_psram: SPI SRAM memory test OK\nI (760) cpu_start: GPIO 44 and 43 are used as console UART I/O pins\nI (761) cpu_start: Pro cpu start user code\nI (761) cpu_start: cpu freq: 240000000 Hz\nI (762) app_init: Application information:\nI (766) app_init: Project name:     AIoTNode\nI (770) app_init: App version:      0a79117-dirty\nI (775) app_init: Compile time:     Nov  4 2025 23:13:38\nI (780) app_init: ELF file SHA256:  a5e0090b4...\nI (784) app_init: ESP-IDF:          v6.0-dev-1833-g758939caec\nI (789) efuse_init: Min chip rev:     v0.0\nI (793) efuse_init: Max chip rev:     v0.99 \nI (797) efuse_init: Chip rev:         v0.2\nI (801) heap_init: Initializing. RAM available for dynamic allocation:\nI (807) heap_init: At 3FCA2918 len 00046DF8 (283 KiB): RAM\nI (812) heap_init: At 3FCE9710 len 00005724 (21 KiB): RAM\nI (818) heap_init: At 3FCF0000 len 00008000 (32 KiB): DRAM\nI (823) heap_init: At 600FE000 len 00001FE8 (7 KiB): RTCRAM\nI (828) esp_psram: Adding pool of 8192K of PSRAM memory to heap allocator\nI (835) spi_flash: detected chip: boya\nI (838) spi_flash: flash io: qio\nI (841) sleep_gpio: Configure to isolate all GPIO pins in sleep state\nI (847) sleep_gpio: Enable automatic switching of GPIO sleep configuration\nI (854) main_task: Started on CPU0\nI (878) esp_psram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (878) main_task: Calling app_main()\nI (883) tiny_time_test: ========================================\nI (884) tiny_time_test:   tiny_time Module Test Program\nI (889) tiny_time_test: ========================================\nI (895) tiny_time_test: Initializing WiFi...\nI (900) pp: pp rom version: e7ae62f\nI (902) net80211: net80211 rom version: e7ae62f\nI (907) wifi:wifi driver task: 3fcaf644, prio:23, stack:6656, core=0\nI (915) wifi:wifi firmware version: 14da9b7\nI (916) wifi:wifi certification version: v7.0\nI (920) wifi:config NVS flash: enabled\nI (924) wifi:config nano formatting: disabled\nI (928) wifi:Init data frame dynamic rx buffer num: 32\nI (933) wifi:Init static rx mgmt buffer num: 5\nI (937) wifi:Init management short buffer num: 32\nI (941) wifi:Init dynamic tx buffer num: 32\nI (945) wifi:Init static tx FG buffer num: 2\nI (949) wifi:Init static rx buffer size: 1600\nI (953) wifi:Init static rx buffer num: 10\nI (957) wifi:Init dynamic rx buffer num: 32\nI (961) wifi_init: rx ba win: 6\nI (964) wifi_init: accept mbox: 6\nI (967) wifi_init: tcpip mbox: 32\nI (970) wifi_init: udp mbox: 6\nI (973) wifi_init: tcp mbox: 6\nI (975) wifi_init: tcp tx win: 5760\nI (979) wifi_init: tcp rx win: 5760\nI (982) wifi_init: tcp mss: 1440\nI (985) wifi_init: WiFi IRAM OP enabled\nI (988) wifi_init: WiFi RX IRAM OP enabled\nI (992) NODE-WIFI: Setting WiFi configuration SSID NTUSECURE...\nI (999) phy_init: phy_version 701,f4f1da3a,Mar  3 2025,15:50:10\nI (1037) wifi:mode : sta (cc:ba:97:09:a7:50)\nI (1038) wifi:enable tsf\nI (1039) tiny_time_test: WiFi initialized successfully\nI (1040) tiny_time_test: Waiting for WiFi connection...\nI (1107) wifi:new:&lt;1,0&gt;, old:&lt;1,0&gt;, ap:&lt;255,255&gt;, sta:&lt;1,0&gt;, prof:1, snd_ch_cfg:0x0\nI (1108) wifi:state: init -&gt; auth (0xb0)\nI (1111) wifi:state: auth -&gt; assoc (0x0)\nI (1115) wifi:state: assoc -&gt; run (0x10)\nI (1430) wifi:connected with NTUSECURE, aid = 2, channel 1, BW20, bssid = a8:9d:21:3c:12:b1\nI (1430) wifi:security: WPA2-ENT, phy: bgn, rssi: -66\nI (1432) wifi:pm start, type: 1\n\nI (1435) wifi:dp: 1, bi: 104448, li: 2, scale listen interval from 307200 us to 208896 us\nI (1443) wifi:set rx beacon pti, rx_bcn_pti: 0, bcn_timeout: 25000, mt_pti: 0, mt_time: 10000\nI (1459) wifi:&lt;ba-add&gt;idx:0 (ifx:0, a8:9d:21:3c:12:b1), tid:0, ssn:1200, winSize:64\nI (1488) wifi:AP's beacon interval = 104448 us, DTIM period = 1\nI (2467) esp_netif_handlers: sta ip: 10.91.180.236, mask: 255.255.0.0, gw: 10.91.255.254\nI (2467) tiny_time_test: WiFi connected!\nI (2467) tiny_time_test: \n--- Test 1: Get Running Time ---\nI (2473) tiny_time_test: Running time: 1644833 microseconds\nI (2478) tiny_time_test: Running time: 1.645 seconds\nI (2483) tiny_time_test: \n--- Test 2: Sync Time with Timezone ---\nI (2489) tiny_time_test: Syncing time with timezone CST-8...\nI (2494) NTP_SYNC: Initializing SNTP\nI (2498) NTP_SYNC: Waiting for system time to be set... (1/15)\nI (4503) NTP_SYNC: Waiting for system time to be set... (2/15)\nI (4715) NTP_SYNC: Time synchronized!\nI (6503) NTP_SYNC: System time is set.\nI (6503) NTP_SYNC: Current time: Tue Nov 04 23:15:34 2025\nI (6503) tiny_time_test: Waiting for time synchronization...\nI (11506) tiny_time_test: \n--- Test 3: Get Current DateTime ---\nI (11506) TIME: Current Time: 2025-11-04 23:15:39.003179\nI (11506) tiny_time_test: \n--- Test 4: Measure Time Elapsed ---\nI (11511) tiny_time_test: Time elapsed: 9038406 microseconds\nI (11517) tiny_time_test: Time elapsed: 9.038 seconds\nI (11521) tiny_time_test: \n========================================\nI (11527) tiny_time_test:   Initial Tests Completed\nI (11532) tiny_time_test: ========================================\n\nI (11538) tiny_time_test: \n========================================\nI (11544) tiny_time_test:   Timer Precision Test\nI (11548) tiny_time_test: ========================================\nI (11554) tiny_time_test: Recording 15 timestamps at 2-second intervals...\nI (11561) tiny_time_test: No printing during recording to avoid timing overhead.\n\nI (11568) tiny_time_test: Timer started. Waiting for 15 timestamps...\nI (41674) tiny_time_test: \n========================================\nI (41674) tiny_time_test:   Timer Precision Test Results\nI (41674) tiny_time_test: ========================================\nI (41680) tiny_time_test: Expected interval: 2000000 microseconds (2.000000 seconds)\n\nI (41687) tiny_time_test: Timestamp # 1: 12740383 microseconds (12.740383 seconds) [baseline]\nI (41696) tiny_time_test: Timestamp # 2: 14740381 microseconds (14.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41708) tiny_time_test: Timestamp # 3: 16740383 microseconds (16.740383 seconds) | Interval: 2000002 us (2.000002 s) | Error: 2 us (0.002 ms)\nI (41721) tiny_time_test: Timestamp # 4: 18740383 microseconds (18.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41733) tiny_time_test: Timestamp # 5: 20740383 microseconds (20.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41746) tiny_time_test: Timestamp # 6: 22740383 microseconds (22.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41759) tiny_time_test: Timestamp # 7: 24740382 microseconds (24.740382 seconds) | Interval: 1999999 us (1.999999 s) | Error: -1 us (-0.001 ms)\nI (41771) tiny_time_test: Timestamp # 8: 26740383 microseconds (26.740383 seconds) | Interval: 2000001 us (2.000001 s) | Error: 1 us (0.001 ms)\nI (41784) tiny_time_test: Timestamp # 9: 28740383 microseconds (28.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41797) tiny_time_test: Timestamp #10: 30740383 microseconds (30.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41809) tiny_time_test: Timestamp #11: 32740383 microseconds (32.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41822) tiny_time_test: Timestamp #12: 34740381 microseconds (34.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41834) tiny_time_test: Timestamp #13: 36740383 microseconds (36.740383 seconds) | Interval: 2000002 us (2.000002 s) | Error: 2 us (0.002 ms)\nI (41847) tiny_time_test: Timestamp #14: 38740383 microseconds (38.740383 seconds) | Interval: 2000000 us (2.000000 s) | Error: 0 us (0.000 ms)\nI (41860) tiny_time_test: Timestamp #15: 40740381 microseconds (40.740381 seconds) | Interval: 1999998 us (1.999998 s) | Error: -2 us (-0.002 ms)\nI (41872) tiny_time_test: \n--- Statistics ---\nI (41877) tiny_time_test: Total time: 27999998 microseconds (27.999998 seconds)\nI (41884) tiny_time_test: Expected total: 28000000 microseconds (28.000000 seconds)\nI (41891) tiny_time_test: Total error: -2 microseconds (-0.002 milliseconds)\nI (41898) tiny_time_test: Average interval: 2.000000 seconds (1999999.857 microseconds)\nI (41906) tiny_time_test: \n========================================\nI (41912) tiny_time_test:   Test Complete\nI (41915) tiny_time_test: ========================================\n</code></pre>"},{"location":"zh/TOOLBOX/TIME/log/","title":"LOG","text":"<p>2025-04-10</p> <ul> <li>\u83b7\u53d6\u8fd0\u884c\u65f6\u95f4\uff1a <code>tiny_get_running_time()</code></li> <li>SNTP\u5bf9\u65f6\uff1a <code>sync_time_with_timezone(\"CST-8\")</code></li> <li>\u83b7\u53d6\u4e16\u754c\u65f6\u95f4\uff1a <code>tiny_get_current_datetime(1)</code></li> </ul> <p>\u5f85\u5f00\u53d1:</p> <ul> <li>\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u672c\u5730\u5bf9\u65f6-\u5fae\u79d2\u7ea7\u522b</li> </ul>"},{"location":"zh/TOOLBOX/TIME/notes/","title":"\u65f6\u95f4","text":"<p>\u65f6\u95f4</p> <p>\u65f6\u95f4\u76f8\u5173\u7684\u529f\u80fd\u5bf9\u4e8eMCU\u6765\u8bf4\u975e\u5e38\u91cd\u8981\uff0c\u672c\u8282\u63d0\u4f9b\u4e00\u7cfb\u5217\u65f6\u95f4\u76f8\u5173\u7684\u5b9a\u4e49\u548c\u51fd\u6570\uff0c\u4f9b\u5f00\u53d1\u8005\u4f7f\u7528\u3002</p> <p>MCU\u4e2d\u7684\u65f6\u95f4\u53ef\u4ee5\u5206\u4ee5\u4e0b\u51e0\u79cd\u7c7b\u578b\uff1a</p> <ul> <li> <p>\u8fd0\u884c\u65f6\u95f4\uff1a \u6307\u7684\u662fMCU\u4ece\u4e0a\u7535\u5230\u73b0\u5728\u7684\u65f6\u95f4\u3002</p> </li> <li> <p>\u4e16\u754c\u65f6\u95f4\uff1a \u6307\u7684\u662fMCU\u6240\u5728\u7684\u65f6\u533a\u7684\u65f6\u95f4\u3002\u4e16\u754c\u65f6\u95f4\u53ef\u4ee5\u901a\u8fc7\u6807\u51c6\u7684\u5e74\u6708\u65e5\u65f6\u5206\u79d2\u6765\u8868\u793a\uff0c\u4e5f\u53ef\u4ee5\u8868\u793a\u4e3aUNIX\u65f6\u95f4\u6233\u3002</p> </li> </ul>"},{"location":"zh/TOOLBOX/TIME/notes/#_2","title":"\u8fd0\u884c\u65f6\u95f4","text":"<p>ESP\u6709\u81ea\u5df1\u7684\u83b7\u53d6\u8fd0\u884c\u65f6\u95f4\u7684\u51fd\u6570<code>esp_timer_get_time</code>\uff0c\u4f9d\u8d56\u4e8e<code>esp_timer</code>\u5e93\u3002\u8be5\u51fd\u6570\u8fd4\u56de\u4ece\u4e0a\u7535\u5230\u73b0\u5728\u7684\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u5fae\u79d2\u3002</p> <p>\u4e3a\u4e86\u65b9\u4fbf\u4f7f\u7528\uff0cTinyToolbox\u91cd\u65b0\u5b9a\u4e49\u4e86\u6570\u636e\u7c7b\u578b<code>TinyTimeMark_t</code>\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51fd\u6570<code>tiny_get_running_time</code>\u6765\u83b7\u53d6\u8fd0\u884c\u65f6\u95f4\u3002\u8be5\u51fd\u6570\u8fd4\u56de\u7684\u65f6\u95f4\u5355\u4f4d\u4e3aint64_t\uff0c\u5176\u957f\u5ea6\u8db3\u591f\u4ee5\u907f\u514d\u6ea2\u51fa\u3002</p> <pre><code>typedef int64_t TinyTimeMark_t;\n</code></pre> <pre><code>/**\n * @brief Get the running time in microseconds\n * @return TinyTimeMark_t\n */\nTinyTimeMark_t tiny_get_running_time(void) { return esp_timer_get_time(); }\n</code></pre> <p>\u4f7f\u7528\u53c2\u8003\uff1a</p> <pre><code>void app_main(void)\n{\n    // Get running time\n    TinyTimeMark_t running_time = tiny_get_running_time();\n    ESP_LOGI(TAG_TIME, \"Running Time: %lld us\", running_time);\n}\n</code></pre>"},{"location":"zh/TOOLBOX/TIME/notes/#_3","title":"\u4e16\u754c\u65f6\u95f4","text":"<p>Warning</p> <p>\u6ce8\u610f\uff0c\u83b7\u53d6\u4e16\u754c\u65f6\u95f4\u9700\u8981\u5efa\u7acb\u5728\u5df2\u7ecf\u8054\u7f51\u7684\u57fa\u7840\u4e0a\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u83b7\u53d6\u4e16\u754c\u65f6\u95f4\u7684\u51fd\u6570\u9700\u8981\u5728\u8054\u7f51\u6210\u529f\u540e\u8c03\u7528\u3002</p>"},{"location":"zh/TOOLBOX/TIME/notes/#ntp","title":"NTP\u5bf9\u65f6","text":"<p>NTP\u5bf9\u65f6</p> <p>NTP\uff08Network Time Protocol\uff09\u662f\u7f51\u7edc\u65f6\u95f4\u534f\u8bae\u7684\u7f29\u5199\uff0c\u662f\u4e00\u79cd\u7528\u4e8e\u5728\u8ba1\u7b97\u673a\u7f51\u7edc\u4e2d\u540c\u6b65\u65f6\u95f4\u7684\u534f\u8bae\u3002\u5b83\u53ef\u4ee5\u901a\u8fc7\u4e92\u8054\u7f51\u6216\u5c40\u57df\u7f51\u83b7\u53d6\u51c6\u786e\u7684\u65f6\u95f4\u4fe1\u606f\u3002 NTP\u534f\u8bae\u4f7f\u7528UDP\u534f\u8bae\u8fdb\u884c\u901a\u4fe1\uff0c\u9ed8\u8ba4\u4f7f\u7528123\u7aef\u53e3\u3002NTP\u670d\u52a1\u5668\u4f1a\u5b9a\u671f\u5411\u5ba2\u6237\u7aef\u53d1\u9001\u65f6\u95f4\u4fe1\u606f\uff0c\u5ba2\u6237\u7aef\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u6765\u6821\u6b63\u81ea\u5df1\u7684\u7cfb\u7edf\u65f6\u95f4\u3002</p> <pre><code>   Client                      Server\n     |-------------------&gt;      |     T1\uff1a\u8bf7\u6c42\u53d1\u51fa\n     |                          |\n     |         &lt;--------------- |     T2/T3\uff1a\u670d\u52a1\u5668\u6536\u5230 &amp; \u56de\u590d\n     |                          |\n     |-------------------&gt;      |     T4\uff1a\u5ba2\u6237\u7aef\u6536\u5230\u54cd\u5e94\n</code></pre> <p>NTP\u5bf9\u65f6\u539f\u7406</p> <p>NTP\u5bf9\u65f6\u662f\u57fa\u4e8e\u56db\u4e2a\u65f6\u95f4\u6233\uff1a1. \u5ba2\u6237\u7aef\u53d1\u9001\u8bf7\u6c42\u65f6\u7684\u65f6\u95f4\u6233T1 2. \u670d\u52a1\u5668\u63a5\u6536\u5230\u8bf7\u6c42\u65f6\u7684\u65f6\u95f4\u6233T2 3. \u670d\u52a1\u5668\u53d1\u9001\u54cd\u5e94\u65f6\u7684\u65f6\u95f4\u6233T3 4. \u5ba2\u6237\u7aef\u63a5\u6536\u5230\u54cd\u5e94\u65f6\u7684\u65f6\u95f4\u6233T4\u3002\u6839\u636e\u8fd9\u56db\u4e2a\u65f6\u95f4\u6233\uff0c\u53ef\u4ee5\u8ba1\u7b97 \u7f51\u7edc\u5ef6\u8fdf Delay = (T4 - T1) - (T3 - T2)\uff0c\u4ee5\u53ca \u65f6\u95f4\u504f\u79fb Offset = ((T2 - T1) + (T3 - T4)) / 2\u3002</p> <p>ESP32 SNTP\u5bf9\u65f6</p> <p>ESP32\u4e2d\u4f7f\u7528\u7684\u662fSNTP\uff0c\u4e5f\u5c31\u662fSimple Network Time Protocol\u3002SNTP\u662fNTP\u7684\u7b80\u5316\u7248\uff0c\u9002\u7528\u4e8e\u5bf9\u65f6\u95f4\u7cbe\u5ea6\u8981\u6c42\u4e0d\u9ad8\u7684\u573a\u666f\u3002ESP32\u4e2d\u5bf9\u65f6\u4f9d\u8d56\u4e8e<code>esp_sntp</code>\u5e93\u3002SNTP\u7684\u5de5\u4f5c\u539f\u7406\u4e0eNTP\u7c7b\u4f3c\uff0c\u4f46SNTP\u7684\u5b9e\u73b0\u76f8\u5bf9\u7b80\u5355\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u4f7f\u7528\u3002\u5176\u7cbe\u5ea6\u901a\u5e38\u5728ms\u7ea7\u522b\uff0c\u9002\u7528\u4e8e\u5927\u591a\u6570\u5e94\u7528\u573a\u666f\u3002</p> <p>\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u56de\u8c03\u51fd\u6570\uff0c\u7528\u4e8e\u63a5\u6536\u5bf9\u65f6\u901a\u77e5\uff1a</p> <pre><code>/* WORLD CURRENT TIME - SNTP */\n/**\n * @brief Callback function for time synchronization notification\n * @param tv Pointer to the timeval structure containing the synchronized time\n * @return None\n */\nstatic void time_sync_notification_cb(struct timeval *tv)\n{\n    ESP_LOGI(TAG_SNTP, \"Time synchronized!\");\n}\n</code></pre> <p>\u63a5\u4e0b\u6765\u662fSNTP\u7684\u521d\u59cb\u5316\u51fd\u6570\uff0c\u4e5f\u662f\u5bf9\u65f6\u7684\u6838\u5fc3\u51fd\u6570\uff0c\u901a\u5e38\u5728\u7cfb\u7edf\u521d\u59cb\u5316\u65f6\uff0c\u5b8c\u6210\u8054\u7f51\u540e\u8c03\u7528\u3002\u6ce8\u610f\u5176\u4e2d\u7684\u5bf9\u65f6\u670d\u52a1\u5668\u5730\u5740\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8fdb\u884c\u4fee\u6539\u3002\u5bf9\u65f6\u5b8c\u6210\u540e\uff0cESP32\u4f1a\u5728\u5e95\u5c42\u5bf9\u672c\u673a\u65f6\u95f4\u8fdb\u884c\u8bbe\u7f6e\u3002</p> <pre><code>/**\n * @brief Initialize SNTP\n * @note This function can be called multiple times if needed\n * @return None\n */\nstatic void initialize_sntp(void)\n{\n    ESP_LOGI(TAG_SNTP, \"Initializing SNTP\");\n    esp_sntp_setoperatingmode(SNTP_OPMODE_POLL);\n    esp_sntp_setservername(0, \"pool.ntp.org\"); // NTP server // pool.ntp.org // ntp.aliyun.com\n    esp_sntp_set_time_sync_notification_cb(time_sync_notification_cb);\n    esp_sntp_init();\n}\n</code></pre> <p>\u518d\u63a5\u4e0b\u6765\u662f\u5bf9\u4ee5\u4e0a\u51fd\u6570\u7684\u8fdb\u4e00\u6b65\u5c01\u88c5\uff0c\u5305\u542b\u4e86\u65f6\u533a\u8bbe\u7f6e\u3002\u6ce8\u610f\u4ee5\u4e0b\u51fd\u6570\u4e2d\u5305\u62ec\u4e86\u5bf9RTC\u7684\u8bbe\u7f6e<code>rtc_set_time</code>\uff0c\u4f9d\u8d56\u4e8edriver\u5c42\u7684RTC\u9a71\u52a8\u3002\u6b64\u5904\u4f7f\u7528\u7684\u662f\u6211\u81ea\u5b9a\u4e49\u7684rtc\u9a71\u52a8\uff0c\u82e5\u6ca1\u6709\u76f8\u5173\u529f\u80fd\u53ef\u4ee5\u76f4\u63a5\u6ce8\u91ca\u6389\u3002</p> <pre><code>/**\n * @brief Obtain the current time with timezone\n * @param timezone_str Timezone string (e.g., \"CST-8\" or \"GMT+8\")\n * @note The timezone string format should be compatible with POSIX TZ format\n * (e.g., \"CST-8\", \"GMT+8\")\n * @note To use this function, in application, after internet connection, call\n * sync_time_with_timezone(\"CST-8\")\n * @return None\n */\nvoid sync_time_with_timezone(const char *timezone_str)\n{\n    // Validate input parameter\n    if (timezone_str == NULL)\n    {\n        ESP_LOGE(TAG_SNTP, \"timezone_str is NULL\");\n        return;\n    }\n\n    // Set system timezone\n    if (setenv(\"TZ\", timezone_str, 1) != 0)\n    {\n        ESP_LOGE(TAG_SNTP, \"Failed to set timezone environment variable\");\n        return;\n    }\n    tzset();\n\n    // Initialize SNTP and start time sync\n    initialize_sntp();\n\n    // Wait for system time to be set\n    time_t now = 0;\n    struct tm timeinfo = {0};\n    int retry = 0;\n    const int retry_count = 15;\n\n    while (timeinfo.tm_year &lt; MIN_VALID_YEAR_OFFSET &amp;&amp; ++retry &lt; retry_count)\n    {\n        ESP_LOGI(TAG_SNTP, \"Waiting for system time to be set... (%d/%d)\", retry,\n                 retry_count);\n        vTaskDelay(2000 / portTICK_PERIOD_MS);\n        time(&amp;now);\n        if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n        {\n            ESP_LOGW(TAG_SNTP, \"Failed to convert time to local time\");\n            continue;\n        }\n    }\n\n    if (timeinfo.tm_year &gt;= MIN_VALID_YEAR_OFFSET)\n    {\n        rtc_set_time(timeinfo.tm_year + 1900, timeinfo.tm_mon + 1, timeinfo.tm_mday,\n                     timeinfo.tm_hour, timeinfo.tm_min,\n                     timeinfo.tm_sec); // defined in esp_rtc.c\n        ESP_LOGI(TAG_SNTP, \"System time is set.\");\n    }\n    else\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to sync time.\");\n        return;\n    }\n\n    // Log current local time (using thread-safe formatting)\n    char time_str[64];\n    if (strftime(time_str, sizeof(time_str), \"%a %b %d %H:%M:%S %Y\", &amp;timeinfo) ==\n        0)\n    {\n        ESP_LOGW(TAG_SNTP, \"Failed to format time string\");\n    }\n    else\n    {\n        ESP_LOGI(TAG_SNTP, \"Current time: %s\", time_str);\n    }\n\n    // vTaskDelay(10000 / portTICK_PERIOD_MS); // Wait for 10 second\n    // rtc_get_time(); // uncomment to check the RTC time\n    // ESP_LOGI(TAG_SNTP, \"Current RTC time: %04d-%02d-%02d %02d:%02d:%02d\",\n    //          calendar.year, calendar.month, calendar.date,\n    //          calendar.hour, calendar.min, calendar.sec); // uncomment to check\n    //          the RTC time\n}\n</code></pre>"},{"location":"zh/TOOLBOX/TIME/notes/#_4","title":"\u4e16\u754c\u65f6\u95f4\u83b7\u53d6","text":"<p>\u4e3a\u4e86\u65b9\u4fbf\u4e16\u754c\u65f6\u95f4\u7684\u83b7\u53d6\uff0c\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6570\u636e\u7ed3\u6784<code>DateTime_t</code>\uff0c\u7528\u4e8e\u5b58\u50a8\u5e74\u6708\u65e5\u65f6\u5206\u79d2\u7b49\u4fe1\u606f\u3002\u7136\u540e\u5b9a\u4e49\u4e86\u4e00\u4e2a\u51fd\u6570<code>tiny_get_current_datetime</code>\uff0c\u7528\u4e8e\u83b7\u53d6\u5f53\u524d\u7684\u4e16\u754c\u65f6\u95f4\u3002\u8be5\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a<code>DateTime_t</code>\u7ed3\u6784\u4f53\uff0c\u5305\u542b\u4e86\u5f53\u524d\u7684\u5e74\u6708\u65e5\u65f6\u5206\u79d2\u7b49\u4fe1\u606f\u3002\u5728\u4f7f\u7528\u65f6\uff0c\u4f20\u5165\u4e00\u4e2a\u5e03\u5c14\u503c<code>print_flag</code>\uff0c\u7528\u4e8e\u63a7\u5236\u662f\u5426\u6253\u5370\u5f53\u524d\u65f6\u95f4\u3002</p> <pre><code>/**\n * @brief Structure to hold date and time\n */\ntypedef struct TinyDateTime_t\n{\n    int year;\n    int month;\n    int day;\n    int hour;\n    int minute;\n    int second;\n    long microsecond;\n} TinyDateTime_t; \n</code></pre> <pre><code>/* WORLD CURRENT TIME - GET TIME */\n/**\n * @name tiny_get_current_datetime\n * @brief Get the current time as a TinyDateTime_t struct\n * @param print_flag Flag to indicate whether to print the time\n * @return TinyDateTime_t structure containing the current date and time\n */\nTinyDateTime_t tiny_get_current_datetime(bool print_flag)\n{\n    TinyDateTime_t result = {0}; // Initialize to zero\n    struct timeval tv;\n\n    // Get current time (seconds + microseconds)\n    if (gettimeofday(&amp;tv, NULL) != 0)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to get time of day\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    time_t now = tv.tv_sec;\n    struct tm timeinfo;\n    if (localtime_r(&amp;now, &amp;timeinfo) == NULL)\n    {\n        ESP_LOGE(TAG_TIME, \"Failed to convert time to local time\");\n        return result; // Return zero-initialized structure on error\n    }\n\n    result.year = timeinfo.tm_year + 1900;\n    result.month = timeinfo.tm_mon + 1;\n    result.day = timeinfo.tm_mday;\n    result.hour = timeinfo.tm_hour;\n    result.minute = timeinfo.tm_min;\n    result.second = timeinfo.tm_sec;\n    result.microsecond = (int32_t)tv.tv_usec; // Explicit cast for portability\n\n    if (print_flag)\n    {\n        ESP_LOGI(TAG_TIME, \"Current Time: %04d-%02d-%02d %02d:%02d:%02d.%06d\",\n                 result.year, result.month, result.day, result.hour, result.minute,\n                 result.second, result.microsecond);\n    }\n\n    return result;\n}\n</code></pre> <p>\u4f7f\u7528\u53c2\u8003\uff1a</p> <pre><code>void app_main(void)\n{\n    // Initialize SNTP and sync time\n    sync_time_with_timezone(\"CST-8\");\n\n    // Get current time\n    TinyDateTime_t current_time = tiny_get_current_datetime(true);\n\n    // Print current time\n    ESP_LOGI(TAG_TIME, \"Current Time: %04d-%02d-%02d %02d:%02d:%02d.%06ld\",\n             current_time.year, current_time.month, current_time.day,\n             current_time.hour, current_time.minute, current_time.second, current_time.microsecond);\n}\n</code></pre> <p>\u4f7f\u7528\u6548\u679c\uff1a</p> <p></p> <p>Danger</p> <p>SNTP\u540c\u6b65\u5230RTC\u4e2d\u7684\u7cbe\u5ea6\u4e3a\u79d2\u7ea7\u522b\uff0c\u56e0\u6b64\u5728\u83b7\u53d6\u4e16\u754c\u65f6\u95f4\u65f6\uff0c\u5fae\u79d2\u90e8\u5206\u53ef\u80fd\u5e76\u4e0d\u51c6\u786e\uff0c\u4ec5\u4f9b\u53c2\u8003\u3002</p>"}]}